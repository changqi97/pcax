{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "# Core dependencies\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# pcax\n",
    "import pcax as px\n",
    "import pcax.predictive_coding as pxc\n",
    "import pcax.nn as pxnn\n",
    "import pcax.utils as pxu\n",
    "\n",
    "\n",
    "class BasicBlock(pxc.EnergyModule):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None, act_fn=Callable[[jax.Array], jax.Array]) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = pxnn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = pxnn.BatchNorm(out_channels)\n",
    "        self.act_fn = px.static(act_fn)\n",
    "        self.conv2 = pxnn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = pxnn.BatchNorm(out_channels)\n",
    "\n",
    "    def __call__(self, x: jax.Array) -> jax.Array:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.act_fn(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        out += identity\n",
    "        out = self.act_fn(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(pxc.EnergyModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        block,\n",
    "        layers,\n",
    "        num_classes=1000,\n",
    "        act_fn=Callable[[jax.Array], jax.Array]\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = 64\n",
    "        self.act_fn = px.static(act_fn)\n",
    "        self.conv1 = pxnn.Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
    "        self.bn1 = pxnn.BatchNorm(64)\n",
    "        self.maxpool = pxnn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "\n",
    "        self.avgpool = pxnn.AvgPool2d((1, 1))\n",
    "        self.fc = pxnn.Linear(512, num_classes)\n",
    "\n",
    "        self.vodes = [\n",
    "            pxc.Vode((64, 56, 56)), \n",
    "            pxc.Vode((64, 56, 56)), \n",
    "            pxc.Vode((128, 28, 28)),\n",
    "            pxc.Vode((256, 14, 14)), \n",
    "            pxc.Vode((512, 7, 7)), \n",
    "            pxc.Vode((512, 1, 1)), \n",
    "            pxc.Vode((num_classes,), energy_fn=pxc.ce_energy)\n",
    "        ]\n",
    "\n",
    "        self.vodes[-1].h.frozen = True\n",
    "\n",
    "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels:\n",
    "            downsample = pxnn.Layer(\n",
    "                pxnn.Conv2d(self.in_channels, out_channels, kernel_size=1, stride=stride),\n",
    "                pxnn.BatchNorm(out_channels)\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample, self.act_fn))\n",
    "        self.in_channels = out_channels\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(out_channels, out_channels, act_fn=self.act_fn))\n",
    "\n",
    "    def __call__(self, x: jax.Array, y: jax.Array):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.vodes[0](x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.vodes[1](x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.vodes[2](x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.vodes[3](x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.vodes[4](x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.flatten(1)\n",
    "        x = self.vodes[5](x)\n",
    "        x = self.fc(x)\n",
    "        x = self.vodes[6](x)\n",
    "\n",
    "        if y is not None:\n",
    "            self.vodes[-1].set(\"h\", y)\n",
    "\n",
    "        return self.vodes[-1].get(\"u\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# This is a simple collate function that stacks numpy arrays used to interface\n",
    "# the PyTorch dataloader with JAX. In the future we hope to provide custom dataloaders\n",
    "# that are independent of PyTorch.\n",
    "\n",
    "def numpy_collate(batch):\n",
    "    if isinstance(batch[0], np.ndarray):\n",
    "        return np.stack(batch)\n",
    "    elif isinstance(batch[0], (tuple, list)):\n",
    "        transposed = zip(*batch)\n",
    "        return [numpy_collate(samples) for samples in transposed]\n",
    "    else:\n",
    "        return np.array(batch)\n",
    "\n",
    "\n",
    "# The dataloader assumes cuda is being used, as such it sets 'pin_memory = True' and\n",
    "# 'prefetch_factor = 2'. Note that the batch size should be constant during training, so\n",
    "# we set 'drop_last = True' to avoid having to deal with variable batch sizes. \n",
    "class TorchDataloader(torch.utils.data.DataLoader):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=None,\n",
    "        sampler=None,\n",
    "        batch_sampler=None,\n",
    "        num_workers=1,\n",
    "        pin_memory=True,\n",
    "        timeout=0,\n",
    "        worker_init_fn=None,\n",
    "        persistent_workers=True,\n",
    "        prefetch_factor=2,\n",
    "    ):\n",
    "        super(self.__class__, self).__init__(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            sampler=sampler,\n",
    "            batch_sampler=batch_sampler,\n",
    "            num_workers=num_workers,\n",
    "            collate_fn=numpy_collate,\n",
    "            pin_memory=pin_memory,\n",
    "            drop_last=True if batch_sampler is None else None,\n",
    "            timeout=timeout,\n",
    "            worker_init_fn=worker_init_fn,\n",
    "            persistent_workers=persistent_workers,\n",
    "            prefetch_factor=prefetch_factor,\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "def resnet18(num_classes=1000, act_fn=jax.nn.leaky_relu):\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes, act_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "pcax.nn._layer.Layer.__init__() got multiple values for keyword argument 'key'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m TorchDataloader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Define the model and optimizer\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mresnet18\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optax\u001b[38;5;241m.\u001b[39madam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Initialize model parameters and optimizer state\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 55\u001b[0m, in \u001b[0;36mresnet18\u001b[0;34m(num_classes, act_fn)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresnet18\u001b[39m(num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, act_fn\u001b[38;5;241m=\u001b[39mjax\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mleaky_relu):\n\u001b[0;32m---> 55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mResNet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBasicBlock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mact_fn\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 52\u001b[0m, in \u001b[0;36mResNet.__init__\u001b[0;34m(self, block, layers, num_classes, act_fn)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_channels \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn \u001b[38;5;241m=\u001b[39m px\u001b[38;5;241m.\u001b[39mstatic(act_fn)\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1 \u001b[38;5;241m=\u001b[39m \u001b[43mpxnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1 \u001b[38;5;241m=\u001b[39m pxnn\u001b[38;5;241m.\u001b[39mBatchNorm(\u001b[38;5;241m64\u001b[39m)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpool \u001b[38;5;241m=\u001b[39m pxnn\u001b[38;5;241m.\u001b[39mMaxPool2d(kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/NeuroMesh/pcax/pcax/nn/_layer.py:137\u001b[0m, in \u001b[0;36mConv2d.__init__\u001b[0;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, groups, use_bias, padding_mode, dtype, rkg, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    123\u001b[0m     in_channels: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    136\u001b[0m ):\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mout_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrkg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/NeuroMesh/pcax/pcax/nn/_layer.py:102\u001b[0m, in \u001b[0;36mConv.__init__\u001b[0;34m(self, num_spatial_dims, in_channels, out_channels, kernel_size, stride, padding, dilation, groups, use_bias, padding_mode, dtype, rkg, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     87\u001b[0m     num_spatial_dims: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    101\u001b[0m ):\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    103\u001b[0m         eqx\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mConv,\n\u001b[1;32m    104\u001b[0m         num_spatial_dims,\n\u001b[1;32m    105\u001b[0m         in_channels,\n\u001b[1;32m    106\u001b[0m         out_channels,\n\u001b[1;32m    107\u001b[0m         kernel_size,\n\u001b[1;32m    108\u001b[0m         stride,\n\u001b[1;32m    109\u001b[0m         padding,\n\u001b[1;32m    110\u001b[0m         dilation,\n\u001b[1;32m    111\u001b[0m         groups,\n\u001b[1;32m    112\u001b[0m         use_bias,\n\u001b[1;32m    113\u001b[0m         padding_mode,\n\u001b[1;32m    114\u001b[0m         dtype,\n\u001b[1;32m    115\u001b[0m         key\u001b[38;5;241m=\u001b[39mrkg(),\n\u001b[1;32m    116\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    117\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: pcax.nn._layer.Layer.__init__() got multiple values for keyword argument 'key'"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Define a simple synthetic dataset\n",
    "class RandomDataset(Dataset):\n",
    "    def __init__(self, num_samples, num_classes):\n",
    "        self.num_samples = num_samples\n",
    "        self.num_classes = num_classes\n",
    "        self.data = np.random.rand(num_samples, 3, 64, 64).astype(np.float32)\n",
    "        self.labels = np.random.randint(0, num_classes, size=num_samples).astype(np.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# Generate synthetic data\n",
    "num_samples = 100\n",
    "num_classes = 10\n",
    "dataset = RandomDataset(num_samples, num_classes)\n",
    "dataloader = TorchDataloader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Define the model and optimizer\n",
    "model = resnet18(num_classes=num_classes)\n",
    "optimizer = optax.adam(learning_rate=0.001)\n",
    "\n",
    "# Initialize model parameters and optimizer state\n",
    "params = model.init(jax.random.PRNGKey(0), jnp.ones((1, 3, 64, 64)), jnp.ones((1, num_classes)))\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "# Define the loss function\n",
    "def loss_fn(params, x, y):\n",
    "    logits = model.apply(params, x, y)\n",
    "    one_hot = jax.nn.one_hot(y, num_classes)\n",
    "    loss = optax.softmax_cross_entropy(logits, one_hot).mean()\n",
    "    return loss\n",
    "\n",
    "# Define the training step\n",
    "@jax.jit\n",
    "def train_step(params, opt_state, x, y):\n",
    "    grads = jax.grad(loss_fn)(params, x, y)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for x_batch, y_batch in dataloader:\n",
    "        x_batch = jnp.array(x_batch)\n",
    "        y_batch = jnp.array(y_batch)\n",
    "        params, opt_state = train_step(params, opt_state, x_batch, y_batch)\n",
    "    print(f\"Epoch {epoch + 1} completed.\")\n",
    "\n",
    "print(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.7%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Loss: 2.377 | Acc: 10.156% (13/128)\n",
      "Train: Loss: 2.529 | Acc: 9.375% (24/256)\n",
      "Train: Loss: 3.116 | Acc: 12.760% (49/384)\n",
      "Train: Loss: 3.306 | Acc: 13.281% (68/512)\n",
      "Train: Loss: 3.547 | Acc: 14.219% (91/640)\n",
      "Train: Loss: 3.485 | Acc: 13.281% (102/768)\n",
      "Train: Loss: 3.429 | Acc: 12.500% (112/896)\n",
      "Train: Loss: 3.377 | Acc: 12.109% (124/1024)\n",
      "Train: Loss: 3.268 | Acc: 12.240% (141/1152)\n",
      "Train: Loss: 3.206 | Acc: 12.266% (157/1280)\n",
      "Train: Loss: 3.143 | Acc: 12.500% (176/1408)\n",
      "Train: Loss: 3.112 | Acc: 12.891% (198/1536)\n",
      "Train: Loss: 3.119 | Acc: 13.281% (221/1664)\n",
      "Train: Loss: 3.093 | Acc: 13.672% (245/1792)\n",
      "Train: Loss: 3.075 | Acc: 14.010% (269/1920)\n",
      "Train: Loss: 3.042 | Acc: 13.965% (286/2048)\n",
      "Train: Loss: 3.075 | Acc: 13.603% (296/2176)\n",
      "Train: Loss: 3.029 | Acc: 13.976% (322/2304)\n",
      "Train: Loss: 3.008 | Acc: 14.186% (345/2432)\n",
      "Train: Loss: 2.986 | Acc: 14.492% (371/2560)\n",
      "Train: Loss: 2.958 | Acc: 14.546% (391/2688)\n",
      "Train: Loss: 2.951 | Acc: 14.631% (412/2816)\n",
      "Train: Loss: 2.931 | Acc: 14.980% (441/2944)\n",
      "Train: Loss: 2.922 | Acc: 15.072% (463/3072)\n",
      "Train: Loss: 2.897 | Acc: 15.156% (485/3200)\n",
      "Train: Loss: 2.870 | Acc: 15.054% (501/3328)\n",
      "Train: Loss: 2.841 | Acc: 15.336% (530/3456)\n",
      "Train: Loss: 2.824 | Acc: 15.458% (554/3584)\n",
      "Train: Loss: 2.803 | Acc: 15.733% (584/3712)\n",
      "Train: Loss: 2.784 | Acc: 15.651% (601/3840)\n",
      "Train: Loss: 2.766 | Acc: 15.902% (631/3968)\n",
      "Train: Loss: 2.749 | Acc: 16.040% (657/4096)\n",
      "Train: Loss: 2.732 | Acc: 16.170% (683/4224)\n",
      "Train: Loss: 2.717 | Acc: 16.360% (712/4352)\n",
      "Train: Loss: 2.698 | Acc: 16.496% (739/4480)\n",
      "Train: Loss: 2.680 | Acc: 16.775% (773/4608)\n",
      "Train: Loss: 2.662 | Acc: 17.082% (809/4736)\n",
      "Train: Loss: 2.646 | Acc: 17.126% (833/4864)\n",
      "Train: Loss: 2.629 | Acc: 17.348% (866/4992)\n",
      "Train: Loss: 2.615 | Acc: 17.441% (893/5120)\n",
      "Train: Loss: 2.602 | Acc: 17.530% (920/5248)\n",
      "Train: Loss: 2.586 | Acc: 17.690% (951/5376)\n",
      "Train: Loss: 2.571 | Acc: 17.878% (984/5504)\n",
      "Train: Loss: 2.555 | Acc: 18.093% (1019/5632)\n",
      "Train: Loss: 2.544 | Acc: 18.229% (1050/5760)\n",
      "Train: Loss: 2.535 | Acc: 18.614% (1096/5888)\n",
      "Train: Loss: 2.525 | Acc: 18.717% (1126/6016)\n",
      "Train: Loss: 2.514 | Acc: 18.945% (1164/6144)\n",
      "Train: Loss: 2.507 | Acc: 19.180% (1203/6272)\n",
      "Train: Loss: 2.499 | Acc: 19.328% (1237/6400)\n",
      "Train: Loss: 2.488 | Acc: 19.485% (1272/6528)\n",
      "Train: Loss: 2.478 | Acc: 19.681% (1310/6656)\n",
      "Train: Loss: 2.466 | Acc: 19.826% (1345/6784)\n",
      "Train: Loss: 2.456 | Acc: 19.936% (1378/6912)\n",
      "Train: Loss: 2.445 | Acc: 20.156% (1419/7040)\n",
      "Train: Loss: 2.437 | Acc: 20.299% (1455/7168)\n",
      "Train: Loss: 2.428 | Acc: 20.477% (1494/7296)\n",
      "Train: Loss: 2.419 | Acc: 20.622% (1531/7424)\n",
      "Train: Loss: 2.410 | Acc: 20.882% (1577/7552)\n",
      "Train: Loss: 2.403 | Acc: 21.003% (1613/7680)\n",
      "Train: Loss: 2.393 | Acc: 21.209% (1656/7808)\n",
      "Train: Loss: 2.385 | Acc: 21.220% (1684/7936)\n",
      "Train: Loss: 2.379 | Acc: 21.305% (1718/8064)\n",
      "Train: Loss: 2.369 | Acc: 21.497% (1761/8192)\n",
      "Train: Loss: 2.362 | Acc: 21.623% (1799/8320)\n",
      "Train: Loss: 2.352 | Acc: 21.816% (1843/8448)\n",
      "Train: Loss: 2.346 | Acc: 21.910% (1879/8576)\n",
      "Train: Loss: 2.338 | Acc: 22.024% (1917/8704)\n",
      "Train: Loss: 2.330 | Acc: 22.215% (1962/8832)\n",
      "Train: Loss: 2.326 | Acc: 22.310% (1999/8960)\n",
      "Train: Loss: 2.321 | Acc: 22.370% (2033/9088)\n",
      "Train: Loss: 2.316 | Acc: 22.493% (2073/9216)\n",
      "Train: Loss: 2.309 | Acc: 22.742% (2125/9344)\n",
      "Train: Loss: 2.300 | Acc: 22.889% (2168/9472)\n",
      "Train: Loss: 2.294 | Acc: 22.958% (2204/9600)\n",
      "Train: Loss: 2.287 | Acc: 23.160% (2253/9728)\n",
      "Train: Loss: 2.280 | Acc: 23.356% (2302/9856)\n",
      "Train: Loss: 2.275 | Acc: 23.397% (2336/9984)\n",
      "Train: Loss: 2.269 | Acc: 23.447% (2371/10112)\n",
      "Train: Loss: 2.262 | Acc: 23.604% (2417/10240)\n",
      "Train: Loss: 2.256 | Acc: 23.708% (2458/10368)\n",
      "Train: Loss: 2.249 | Acc: 23.895% (2508/10496)\n",
      "Train: Loss: 2.244 | Acc: 23.946% (2544/10624)\n",
      "Train: Loss: 2.239 | Acc: 24.005% (2581/10752)\n",
      "Train: Loss: 2.234 | Acc: 24.062% (2618/10880)\n",
      "Train: Loss: 2.227 | Acc: 24.210% (2665/11008)\n",
      "Train: Loss: 2.222 | Acc: 24.309% (2707/11136)\n",
      "Train: Loss: 2.216 | Acc: 24.379% (2746/11264)\n",
      "Train: Loss: 2.212 | Acc: 24.482% (2789/11392)\n",
      "Train: Loss: 2.208 | Acc: 24.583% (2832/11520)\n",
      "Train: Loss: 2.204 | Acc: 24.631% (2869/11648)\n",
      "Train: Loss: 2.199 | Acc: 24.728% (2912/11776)\n",
      "Train: Loss: 2.195 | Acc: 24.815% (2954/11904)\n",
      "Train: Loss: 2.190 | Acc: 24.925% (2999/12032)\n",
      "Train: Loss: 2.186 | Acc: 24.984% (3038/12160)\n",
      "Train: Loss: 2.182 | Acc: 25.057% (3079/12288)\n",
      "Train: Loss: 2.177 | Acc: 25.242% (3134/12416)\n",
      "Train: Loss: 2.173 | Acc: 25.319% (3176/12544)\n",
      "Train: Loss: 2.169 | Acc: 25.402% (3219/12672)\n",
      "Train: Loss: 2.166 | Acc: 25.500% (3264/12800)\n",
      "Train: Loss: 2.161 | Acc: 25.588% (3308/12928)\n",
      "Train: Loss: 2.158 | Acc: 25.643% (3348/13056)\n",
      "Train: Loss: 2.154 | Acc: 25.736% (3393/13184)\n",
      "Train: Loss: 2.150 | Acc: 25.879% (3445/13312)\n",
      "Train: Loss: 2.146 | Acc: 25.997% (3494/13440)\n",
      "Train: Loss: 2.143 | Acc: 26.024% (3531/13568)\n",
      "Train: Loss: 2.139 | Acc: 26.117% (3577/13696)\n",
      "Train: Loss: 2.134 | Acc: 26.230% (3626/13824)\n",
      "Train: Loss: 2.131 | Acc: 26.304% (3670/13952)\n",
      "Train: Loss: 2.127 | Acc: 26.371% (3713/14080)\n",
      "Train: Loss: 2.125 | Acc: 26.387% (3749/14208)\n",
      "Train: Loss: 2.121 | Acc: 26.465% (3794/14336)\n",
      "Train: Loss: 2.118 | Acc: 26.521% (3836/14464)\n",
      "Train: Loss: 2.115 | Acc: 26.624% (3885/14592)\n",
      "Train: Loss: 2.111 | Acc: 26.726% (3934/14720)\n",
      "Train: Loss: 2.108 | Acc: 26.798% (3979/14848)\n",
      "Train: Loss: 2.105 | Acc: 26.856% (4022/14976)\n",
      "Train: Loss: 2.103 | Acc: 26.874% (4059/15104)\n",
      "Train: Loss: 2.099 | Acc: 26.956% (4106/15232)\n",
      "Train: Loss: 2.096 | Acc: 27.038% (4153/15360)\n",
      "Train: Loss: 2.093 | Acc: 27.079% (4194/15488)\n",
      "Train: Loss: 2.090 | Acc: 27.145% (4239/15616)\n",
      "Train: Loss: 2.086 | Acc: 27.268% (4293/15744)\n",
      "Train: Loss: 2.083 | Acc: 27.325% (4337/15872)\n",
      "Train: Loss: 2.081 | Acc: 27.344% (4375/16000)\n",
      "Train: Loss: 2.079 | Acc: 27.350% (4411/16128)\n",
      "Train: Loss: 2.076 | Acc: 27.405% (4455/16256)\n",
      "Train: Loss: 2.073 | Acc: 27.496% (4505/16384)\n",
      "Train: Loss: 2.070 | Acc: 27.568% (4552/16512)\n",
      "Train: Loss: 2.067 | Acc: 27.632% (4598/16640)\n",
      "Train: Loss: 2.064 | Acc: 27.731% (4650/16768)\n",
      "Train: Loss: 2.061 | Acc: 27.829% (4702/16896)\n",
      "Train: Loss: 2.060 | Acc: 27.849% (4741/17024)\n",
      "Train: Loss: 2.056 | Acc: 27.979% (4799/17152)\n",
      "Train: Loss: 2.053 | Acc: 28.027% (4843/17280)\n",
      "Train: Loss: 2.051 | Acc: 28.073% (4887/17408)\n",
      "Train: Loss: 2.048 | Acc: 28.154% (4937/17536)\n",
      "Train: Loss: 2.047 | Acc: 28.210% (4983/17664)\n",
      "Train: Loss: 2.044 | Acc: 28.310% (5037/17792)\n",
      "Train: Loss: 2.042 | Acc: 28.371% (5084/17920)\n",
      "Train: Loss: 2.040 | Acc: 28.446% (5134/18048)\n",
      "Train: Loss: 2.037 | Acc: 28.549% (5189/18176)\n",
      "Train: Loss: 2.035 | Acc: 28.622% (5239/18304)\n",
      "Train: Loss: 2.032 | Acc: 28.684% (5287/18432)\n",
      "Train: Loss: 2.030 | Acc: 28.712% (5329/18560)\n",
      "Train: Loss: 2.028 | Acc: 28.746% (5372/18688)\n",
      "Train: Loss: 2.026 | Acc: 28.800% (5419/18816)\n",
      "Train: Loss: 2.023 | Acc: 28.901% (5475/18944)\n",
      "Train: Loss: 2.021 | Acc: 28.964% (5524/19072)\n",
      "Train: Loss: 2.019 | Acc: 29.000% (5568/19200)\n",
      "Train: Loss: 2.017 | Acc: 29.082% (5621/19328)\n",
      "Train: Loss: 2.015 | Acc: 29.158% (5673/19456)\n",
      "Train: Loss: 2.013 | Acc: 29.182% (5715/19584)\n",
      "Train: Loss: 2.011 | Acc: 29.256% (5767/19712)\n",
      "Train: Loss: 2.007 | Acc: 29.385% (5830/19840)\n",
      "Train: Loss: 2.005 | Acc: 29.417% (5874/19968)\n",
      "Train: Loss: 2.004 | Acc: 29.404% (5909/20096)\n",
      "Train: Loss: 2.002 | Acc: 29.505% (5967/20224)\n",
      "Train: Loss: 2.000 | Acc: 29.574% (6019/20352)\n",
      "Train: Loss: 1.997 | Acc: 29.629% (6068/20480)\n",
      "Train: Loss: 1.995 | Acc: 29.712% (6123/20608)\n",
      "Train: Loss: 1.993 | Acc: 29.741% (6167/20736)\n",
      "Train: Loss: 1.991 | Acc: 29.788% (6215/20864)\n",
      "Train: Loss: 1.990 | Acc: 29.778% (6251/20992)\n",
      "Train: Loss: 1.987 | Acc: 29.848% (6304/21120)\n",
      "Train: Loss: 1.985 | Acc: 29.928% (6359/21248)\n",
      "Train: Loss: 1.984 | Acc: 29.968% (6406/21376)\n",
      "Train: Loss: 1.982 | Acc: 29.985% (6448/21504)\n",
      "Train: Loss: 1.981 | Acc: 30.034% (6497/21632)\n",
      "Train: Loss: 1.978 | Acc: 30.069% (6543/21760)\n",
      "Train: Loss: 1.977 | Acc: 30.135% (6596/21888)\n",
      "Train: Loss: 1.974 | Acc: 30.210% (6651/22016)\n",
      "Train: Loss: 1.970 | Acc: 30.297% (6709/22144)\n",
      "Train: Loss: 1.968 | Acc: 30.379% (6766/22272)\n",
      "Train: Loss: 1.966 | Acc: 30.406% (6811/22400)\n",
      "Train: Loss: 1.964 | Acc: 30.478% (6866/22528)\n",
      "Train: Loss: 1.961 | Acc: 30.588% (6930/22656)\n",
      "Train: Loss: 1.959 | Acc: 30.649% (6983/22784)\n",
      "Train: Loss: 1.957 | Acc: 30.696% (7033/22912)\n",
      "Train: Loss: 1.955 | Acc: 30.747% (7084/23040)\n",
      "Train: Loss: 1.953 | Acc: 30.831% (7143/23168)\n",
      "Train: Loss: 1.951 | Acc: 30.855% (7188/23296)\n",
      "Train: Loss: 1.951 | Acc: 30.883% (7234/23424)\n",
      "Train: Loss: 1.949 | Acc: 30.902% (7278/23552)\n",
      "Train: Loss: 1.947 | Acc: 30.959% (7331/23680)\n",
      "Train: Loss: 1.945 | Acc: 31.023% (7386/23808)\n",
      "Train: Loss: 1.942 | Acc: 31.091% (7442/23936)\n",
      "Train: Loss: 1.940 | Acc: 31.167% (7500/24064)\n",
      "Train: Loss: 1.939 | Acc: 31.200% (7548/24192)\n",
      "Train: Loss: 1.937 | Acc: 31.299% (7612/24320)\n",
      "Train: Loss: 1.934 | Acc: 31.389% (7674/24448)\n",
      "Train: Loss: 1.932 | Acc: 31.462% (7732/24576)\n",
      "Train: Loss: 1.931 | Acc: 31.501% (7782/24704)\n",
      "Train: Loss: 1.929 | Acc: 31.548% (7834/24832)\n",
      "Train: Loss: 1.927 | Acc: 31.583% (7883/24960)\n",
      "Train: Loss: 1.926 | Acc: 31.617% (7932/25088)\n",
      "Train: Loss: 1.924 | Acc: 31.690% (7991/25216)\n",
      "Train: Loss: 1.922 | Acc: 31.767% (8051/25344)\n",
      "Train: Loss: 1.921 | Acc: 31.815% (8104/25472)\n",
      "Train: Loss: 1.919 | Acc: 31.824% (8147/25600)\n",
      "Train: Loss: 1.917 | Acc: 31.868% (8199/25728)\n",
      "Train: Loss: 1.915 | Acc: 31.907% (8250/25856)\n",
      "Train: Loss: 1.913 | Acc: 31.954% (8303/25984)\n",
      "Train: Loss: 1.911 | Acc: 32.031% (8364/26112)\n",
      "Train: Loss: 1.910 | Acc: 32.077% (8417/26240)\n",
      "Train: Loss: 1.908 | Acc: 32.126% (8471/26368)\n",
      "Train: Loss: 1.906 | Acc: 32.178% (8526/26496)\n",
      "Train: Loss: 1.905 | Acc: 32.230% (8581/26624)\n",
      "Train: Loss: 1.901 | Acc: 32.342% (8652/26752)\n",
      "Train: Loss: 1.900 | Acc: 32.377% (8703/26880)\n",
      "Train: Loss: 1.899 | Acc: 32.405% (8752/27008)\n",
      "Train: Loss: 1.897 | Acc: 32.440% (8803/27136)\n",
      "Train: Loss: 1.895 | Acc: 32.482% (8856/27264)\n",
      "Train: Loss: 1.894 | Acc: 32.528% (8910/27392)\n",
      "Train: Loss: 1.893 | Acc: 32.547% (8957/27520)\n",
      "Train: Loss: 1.891 | Acc: 32.610% (9016/27648)\n",
      "Train: Loss: 1.889 | Acc: 32.629% (9063/27776)\n",
      "Train: Loss: 1.888 | Acc: 32.666% (9115/27904)\n",
      "Train: Loss: 1.887 | Acc: 32.709% (9169/28032)\n",
      "Train: Loss: 1.885 | Acc: 32.752% (9223/28160)\n",
      "Train: Loss: 1.884 | Acc: 32.791% (9276/28288)\n",
      "Train: Loss: 1.883 | Acc: 32.834% (9330/28416)\n",
      "Train: Loss: 1.881 | Acc: 32.897% (9390/28544)\n",
      "Train: Loss: 1.880 | Acc: 32.942% (9445/28672)\n",
      "Train: Loss: 1.878 | Acc: 32.983% (9499/28800)\n",
      "Train: Loss: 1.876 | Acc: 33.041% (9558/28928)\n",
      "Train: Loss: 1.875 | Acc: 33.071% (9609/29056)\n",
      "Train: Loss: 1.874 | Acc: 33.097% (9659/29184)\n",
      "Train: Loss: 1.872 | Acc: 33.171% (9723/29312)\n",
      "Train: Loss: 1.870 | Acc: 33.213% (9778/29440)\n",
      "Train: Loss: 1.869 | Acc: 33.262% (9835/29568)\n",
      "Train: Loss: 1.867 | Acc: 33.328% (9897/29696)\n",
      "Train: Loss: 1.865 | Acc: 33.379% (9955/29824)\n",
      "Train: Loss: 1.863 | Acc: 33.423% (10011/29952)\n",
      "Train: Loss: 1.862 | Acc: 33.484% (10072/30080)\n",
      "Train: Loss: 1.860 | Acc: 33.544% (10133/30208)\n",
      "Train: Loss: 1.859 | Acc: 33.614% (10197/30336)\n",
      "Train: Loss: 1.856 | Acc: 33.679% (10260/30464)\n",
      "Train: Loss: 1.855 | Acc: 33.734% (10320/30592)\n",
      "Train: Loss: 1.853 | Acc: 33.802% (10384/30720)\n",
      "Train: Loss: 1.851 | Acc: 33.853% (10443/30848)\n",
      "Train: Loss: 1.850 | Acc: 33.923% (10508/30976)\n",
      "Train: Loss: 1.849 | Acc: 33.925% (10552/31104)\n",
      "Train: Loss: 1.848 | Acc: 33.975% (10611/31232)\n",
      "Train: Loss: 1.846 | Acc: 34.021% (10669/31360)\n",
      "Train: Loss: 1.845 | Acc: 34.045% (10720/31488)\n",
      "Train: Loss: 1.844 | Acc: 34.087% (10777/31616)\n",
      "Train: Loss: 1.842 | Acc: 34.136% (10836/31744)\n",
      "Train: Loss: 1.840 | Acc: 34.165% (10889/31872)\n",
      "Train: Loss: 1.839 | Acc: 34.209% (10947/32000)\n",
      "Train: Loss: 1.838 | Acc: 34.216% (10993/32128)\n",
      "Train: Loss: 1.836 | Acc: 34.276% (11056/32256)\n",
      "Train: Loss: 1.835 | Acc: 34.338% (11120/32384)\n",
      "Train: Loss: 1.834 | Acc: 34.384% (11179/32512)\n",
      "Train: Loss: 1.832 | Acc: 34.439% (11241/32640)\n",
      "Train: Loss: 1.831 | Acc: 34.470% (11295/32768)\n",
      "Train: Loss: 1.829 | Acc: 34.521% (11356/32896)\n",
      "Train: Loss: 1.828 | Acc: 34.572% (11417/33024)\n",
      "Train: Loss: 1.826 | Acc: 34.607% (11473/33152)\n",
      "Train: Loss: 1.825 | Acc: 34.651% (11532/33280)\n",
      "Train: Loss: 1.824 | Acc: 34.674% (11584/33408)\n",
      "Train: Loss: 1.822 | Acc: 34.748% (11653/33536)\n",
      "Train: Loss: 1.821 | Acc: 34.794% (11713/33664)\n",
      "Train: Loss: 1.820 | Acc: 34.801% (11760/33792)\n",
      "Train: Loss: 1.819 | Acc: 34.847% (11820/33920)\n",
      "Train: Loss: 1.818 | Acc: 34.895% (11881/34048)\n",
      "Train: Loss: 1.816 | Acc: 34.931% (11938/34176)\n",
      "Train: Loss: 1.814 | Acc: 34.978% (11999/34304)\n",
      "Train: Loss: 1.813 | Acc: 35.043% (12066/34432)\n",
      "Train: Loss: 1.811 | Acc: 35.093% (12128/34560)\n",
      "Train: Loss: 1.810 | Acc: 35.130% (12186/34688)\n",
      "Train: Loss: 1.809 | Acc: 35.153% (12239/34816)\n",
      "Train: Loss: 1.808 | Acc: 35.188% (12296/34944)\n",
      "Train: Loss: 1.807 | Acc: 35.199% (12345/35072)\n",
      "Train: Loss: 1.806 | Acc: 35.233% (12402/35200)\n",
      "Train: Loss: 1.805 | Acc: 35.278% (12463/35328)\n",
      "Train: Loss: 1.803 | Acc: 35.317% (12522/35456)\n",
      "Train: Loss: 1.802 | Acc: 35.359% (12582/35584)\n",
      "Train: Loss: 1.800 | Acc: 35.425% (12651/35712)\n",
      "Train: Loss: 1.799 | Acc: 35.444% (12703/35840)\n",
      "Train: Loss: 1.798 | Acc: 35.479% (12761/35968)\n",
      "Train: Loss: 1.797 | Acc: 35.503% (12815/36096)\n",
      "Train: Loss: 1.796 | Acc: 35.545% (12876/36224)\n",
      "Train: Loss: 1.795 | Acc: 35.580% (12934/36352)\n",
      "Train: Loss: 1.794 | Acc: 35.622% (12995/36480)\n",
      "Train: Loss: 1.792 | Acc: 35.664% (13056/36608)\n",
      "Train: Loss: 1.791 | Acc: 35.701% (13115/36736)\n",
      "Train: Loss: 1.789 | Acc: 35.742% (13176/36864)\n",
      "Train: Loss: 1.788 | Acc: 35.773% (13233/36992)\n",
      "Train: Loss: 1.787 | Acc: 35.797% (13288/37120)\n",
      "Train: Loss: 1.786 | Acc: 35.825% (13344/37248)\n",
      "Train: Loss: 1.785 | Acc: 35.847% (13398/37376)\n",
      "Train: Loss: 1.784 | Acc: 35.876% (13455/37504)\n",
      "Train: Loss: 1.783 | Acc: 35.924% (13519/37632)\n",
      "Train: Loss: 1.782 | Acc: 35.959% (13578/37760)\n",
      "Train: Loss: 1.781 | Acc: 35.998% (13639/37888)\n",
      "Train: Loss: 1.780 | Acc: 36.016% (13692/38016)\n",
      "Train: Loss: 1.779 | Acc: 36.045% (13749/38144)\n",
      "Train: Loss: 1.778 | Acc: 36.073% (13806/38272)\n",
      "Train: Loss: 1.778 | Acc: 36.083% (13856/38400)\n",
      "Train: Loss: 1.777 | Acc: 36.109% (13912/38528)\n",
      "Train: Loss: 1.776 | Acc: 36.147% (13973/38656)\n",
      "Train: Loss: 1.775 | Acc: 36.164% (14026/38784)\n",
      "Train: Loss: 1.774 | Acc: 36.205% (14088/38912)\n",
      "Train: Loss: 1.773 | Acc: 36.219% (14140/39040)\n",
      "Train: Loss: 1.771 | Acc: 36.264% (14204/39168)\n",
      "Train: Loss: 1.770 | Acc: 36.296% (14263/39296)\n",
      "Train: Loss: 1.770 | Acc: 36.351% (14331/39424)\n",
      "Train: Loss: 1.768 | Acc: 36.405% (14399/39552)\n",
      "Train: Loss: 1.767 | Acc: 36.449% (14463/39680)\n",
      "Train: Loss: 1.766 | Acc: 36.478% (14521/39808)\n",
      "Train: Loss: 1.765 | Acc: 36.498% (14576/39936)\n",
      "Train: Loss: 1.764 | Acc: 36.522% (14632/40064)\n",
      "Train: Loss: 1.763 | Acc: 36.555% (14692/40192)\n",
      "Train: Loss: 1.762 | Acc: 36.565% (14743/40320)\n",
      "Train: Loss: 1.761 | Acc: 36.598% (14803/40448)\n",
      "Train: Loss: 1.760 | Acc: 36.647% (14870/40576)\n",
      "Train: Loss: 1.760 | Acc: 36.660% (14922/40704)\n",
      "Train: Loss: 1.759 | Acc: 36.682% (14978/40832)\n",
      "Train: Loss: 1.758 | Acc: 36.707% (15035/40960)\n",
      "Train: Loss: 1.756 | Acc: 36.758% (15103/41088)\n",
      "Train: Loss: 1.756 | Acc: 36.792% (15164/41216)\n",
      "Train: Loss: 1.754 | Acc: 36.820% (15223/41344)\n",
      "Train: Loss: 1.753 | Acc: 36.861% (15287/41472)\n",
      "Train: Loss: 1.753 | Acc: 36.868% (15337/41600)\n",
      "Train: Loss: 1.752 | Acc: 36.887% (15392/41728)\n",
      "Train: Loss: 1.751 | Acc: 36.905% (15447/41856)\n",
      "Train: Loss: 1.750 | Acc: 36.940% (15509/41984)\n",
      "Train: Loss: 1.749 | Acc: 36.992% (15578/42112)\n",
      "Train: Loss: 1.749 | Acc: 37.010% (15633/42240)\n",
      "Train: Loss: 1.748 | Acc: 37.059% (15701/42368)\n",
      "Train: Loss: 1.747 | Acc: 37.079% (15757/42496)\n",
      "Train: Loss: 1.746 | Acc: 37.113% (15819/42624)\n",
      "Train: Loss: 1.745 | Acc: 37.151% (15883/42752)\n",
      "Train: Loss: 1.744 | Acc: 37.167% (15937/42880)\n",
      "Train: Loss: 1.743 | Acc: 37.207% (16002/43008)\n",
      "Train: Loss: 1.742 | Acc: 37.264% (16074/43136)\n",
      "Train: Loss: 1.741 | Acc: 37.310% (16142/43264)\n",
      "Train: Loss: 1.740 | Acc: 37.350% (16207/43392)\n",
      "Train: Loss: 1.739 | Acc: 37.387% (16271/43520)\n",
      "Train: Loss: 1.738 | Acc: 37.420% (16333/43648)\n",
      "Train: Loss: 1.737 | Acc: 37.436% (16388/43776)\n",
      "Train: Loss: 1.736 | Acc: 37.477% (16454/43904)\n",
      "Train: Loss: 1.735 | Acc: 37.505% (16514/44032)\n",
      "Train: Loss: 1.734 | Acc: 37.536% (16576/44160)\n",
      "Train: Loss: 1.733 | Acc: 37.547% (16629/44288)\n",
      "Train: Loss: 1.732 | Acc: 37.601% (16701/44416)\n",
      "Train: Loss: 1.731 | Acc: 37.621% (16758/44544)\n",
      "Train: Loss: 1.730 | Acc: 37.652% (16820/44672)\n",
      "Train: Loss: 1.729 | Acc: 37.676% (16879/44800)\n",
      "Train: Loss: 1.729 | Acc: 37.687% (16932/44928)\n",
      "Train: Loss: 1.728 | Acc: 37.711% (16991/45056)\n",
      "Train: Loss: 1.727 | Acc: 37.748% (17056/45184)\n",
      "Train: Loss: 1.726 | Acc: 37.774% (17116/45312)\n",
      "Train: Loss: 1.725 | Acc: 37.819% (17185/45440)\n",
      "Train: Loss: 1.724 | Acc: 37.853% (17249/45568)\n",
      "Train: Loss: 1.723 | Acc: 37.896% (17317/45696)\n",
      "Train: Loss: 1.722 | Acc: 37.930% (17381/45824)\n",
      "Train: Loss: 1.721 | Acc: 37.990% (17457/45952)\n",
      "Train: Loss: 1.720 | Acc: 38.016% (17518/46080)\n",
      "Train: Loss: 1.719 | Acc: 38.052% (17583/46208)\n",
      "Train: Loss: 1.718 | Acc: 38.085% (17647/46336)\n",
      "Train: Loss: 1.717 | Acc: 38.116% (17710/46464)\n",
      "Train: Loss: 1.716 | Acc: 38.148% (17774/46592)\n",
      "Train: Loss: 1.715 | Acc: 38.179% (17837/46720)\n",
      "Train: Loss: 1.714 | Acc: 38.198% (17895/46848)\n",
      "Train: Loss: 1.713 | Acc: 38.232% (17960/46976)\n",
      "Train: Loss: 1.712 | Acc: 38.269% (18026/47104)\n",
      "Train: Loss: 1.711 | Acc: 38.302% (18091/47232)\n",
      "Train: Loss: 1.710 | Acc: 38.304% (18141/47360)\n",
      "Train: Loss: 1.709 | Acc: 38.336% (18205/47488)\n",
      "Train: Loss: 1.708 | Acc: 38.369% (18270/47616)\n",
      "Train: Loss: 1.707 | Acc: 38.422% (18344/47744)\n",
      "Train: Loss: 1.706 | Acc: 38.448% (18406/47872)\n",
      "Train: Loss: 1.705 | Acc: 38.485% (18473/48000)\n",
      "Train: Loss: 1.704 | Acc: 38.518% (18538/48128)\n",
      "Train: Loss: 1.703 | Acc: 38.559% (18607/48256)\n",
      "Train: Loss: 1.702 | Acc: 38.579% (18666/48384)\n",
      "Train: Loss: 1.702 | Acc: 38.593% (18722/48512)\n",
      "Train: Loss: 1.701 | Acc: 38.610% (18780/48640)\n",
      "Train: Loss: 1.700 | Acc: 38.638% (18843/48768)\n",
      "Train: Loss: 1.699 | Acc: 38.658% (18902/48896)\n",
      "Train: Loss: 1.698 | Acc: 38.697% (18971/49024)\n",
      "Train: Loss: 1.697 | Acc: 38.723% (19033/49152)\n",
      "Train: Loss: 1.696 | Acc: 38.760% (19101/49280)\n",
      "Train: Loss: 1.695 | Acc: 38.801% (19171/49408)\n",
      "Train: Loss: 1.694 | Acc: 38.814% (19227/49536)\n",
      "Train: Loss: 1.693 | Acc: 38.853% (19296/49664)\n",
      "Train: Loss: 1.692 | Acc: 38.888% (19363/49792)\n",
      "Train: Loss: 1.691 | Acc: 38.902% (19420/49920)\n",
      "Train: Loss: 1.690 | Acc: 38.924% (19462/50000)\n",
      "Test: Loss: 1.172 | Acc: 59.000% (59/100)\n",
      "Test: Loss: 1.279 | Acc: 54.000% (108/200)\n",
      "Test: Loss: 1.277 | Acc: 53.667% (161/300)\n",
      "Test: Loss: 1.317 | Acc: 51.750% (207/400)\n",
      "Test: Loss: 1.330 | Acc: 51.600% (258/500)\n",
      "Test: Loss: 1.306 | Acc: 52.500% (315/600)\n",
      "Test: Loss: 1.299 | Acc: 52.714% (369/700)\n",
      "Test: Loss: 1.310 | Acc: 52.750% (422/800)\n",
      "Test: Loss: 1.318 | Acc: 52.444% (472/900)\n",
      "Test: Loss: 1.309 | Acc: 53.000% (530/1000)\n",
      "Test: Loss: 1.301 | Acc: 53.273% (586/1100)\n",
      "Test: Loss: 1.294 | Acc: 53.333% (640/1200)\n",
      "Test: Loss: 1.298 | Acc: 53.462% (695/1300)\n",
      "Test: Loss: 1.303 | Acc: 53.143% (744/1400)\n",
      "Test: Loss: 1.294 | Acc: 53.400% (801/1500)\n",
      "Test: Loss: 1.300 | Acc: 53.188% (851/1600)\n",
      "Test: Loss: 1.299 | Acc: 53.294% (906/1700)\n",
      "Test: Loss: 1.302 | Acc: 53.056% (955/1800)\n",
      "Test: Loss: 1.301 | Acc: 53.316% (1013/1900)\n",
      "Test: Loss: 1.308 | Acc: 53.300% (1066/2000)\n",
      "Test: Loss: 1.307 | Acc: 53.524% (1124/2100)\n",
      "Test: Loss: 1.314 | Acc: 53.273% (1172/2200)\n",
      "Test: Loss: 1.318 | Acc: 53.174% (1223/2300)\n",
      "Test: Loss: 1.321 | Acc: 53.125% (1275/2400)\n",
      "Test: Loss: 1.320 | Acc: 53.160% (1329/2500)\n",
      "Test: Loss: 1.334 | Acc: 52.692% (1370/2600)\n",
      "Test: Loss: 1.331 | Acc: 52.852% (1427/2700)\n",
      "Test: Loss: 1.332 | Acc: 52.821% (1479/2800)\n",
      "Test: Loss: 1.334 | Acc: 52.862% (1533/2900)\n",
      "Test: Loss: 1.334 | Acc: 52.833% (1585/3000)\n",
      "Test: Loss: 1.334 | Acc: 52.806% (1637/3100)\n",
      "Test: Loss: 1.332 | Acc: 52.875% (1692/3200)\n",
      "Test: Loss: 1.331 | Acc: 52.727% (1740/3300)\n",
      "Test: Loss: 1.333 | Acc: 52.676% (1791/3400)\n",
      "Test: Loss: 1.334 | Acc: 52.514% (1838/3500)\n",
      "Test: Loss: 1.333 | Acc: 52.472% (1889/3600)\n",
      "Test: Loss: 1.335 | Acc: 52.324% (1936/3700)\n",
      "Test: Loss: 1.335 | Acc: 52.211% (1984/3800)\n",
      "Test: Loss: 1.332 | Acc: 52.103% (2032/3900)\n",
      "Test: Loss: 1.332 | Acc: 51.975% (2079/4000)\n",
      "Test: Loss: 1.338 | Acc: 51.829% (2125/4100)\n",
      "Test: Loss: 1.339 | Acc: 51.857% (2178/4200)\n",
      "Test: Loss: 1.338 | Acc: 51.930% (2233/4300)\n",
      "Test: Loss: 1.336 | Acc: 51.977% (2287/4400)\n",
      "Test: Loss: 1.338 | Acc: 51.844% (2333/4500)\n",
      "Test: Loss: 1.339 | Acc: 51.696% (2378/4600)\n",
      "Test: Loss: 1.337 | Acc: 51.638% (2427/4700)\n",
      "Test: Loss: 1.335 | Acc: 51.688% (2481/4800)\n",
      "Test: Loss: 1.333 | Acc: 51.673% (2532/4900)\n",
      "Test: Loss: 1.333 | Acc: 51.700% (2585/5000)\n",
      "Test: Loss: 1.333 | Acc: 51.627% (2633/5100)\n",
      "Test: Loss: 1.333 | Acc: 51.558% (2681/5200)\n",
      "Test: Loss: 1.334 | Acc: 51.453% (2727/5300)\n",
      "Test: Loss: 1.340 | Acc: 51.241% (2767/5400)\n",
      "Test: Loss: 1.342 | Acc: 51.309% (2822/5500)\n",
      "Test: Loss: 1.344 | Acc: 51.196% (2867/5600)\n",
      "Test: Loss: 1.342 | Acc: 51.351% (2927/5700)\n",
      "Test: Loss: 1.338 | Acc: 51.431% (2983/5800)\n",
      "Test: Loss: 1.339 | Acc: 51.322% (3028/5900)\n",
      "Test: Loss: 1.338 | Acc: 51.367% (3082/6000)\n",
      "Test: Loss: 1.339 | Acc: 51.279% (3128/6100)\n",
      "Test: Loss: 1.339 | Acc: 51.210% (3175/6200)\n",
      "Test: Loss: 1.339 | Acc: 51.222% (3227/6300)\n",
      "Test: Loss: 1.339 | Acc: 51.297% (3283/6400)\n",
      "Test: Loss: 1.340 | Acc: 51.277% (3333/6500)\n",
      "Test: Loss: 1.340 | Acc: 51.258% (3383/6600)\n",
      "Test: Loss: 1.339 | Acc: 51.284% (3436/6700)\n",
      "Test: Loss: 1.339 | Acc: 51.324% (3490/6800)\n",
      "Test: Loss: 1.339 | Acc: 51.333% (3542/6900)\n",
      "Test: Loss: 1.341 | Acc: 51.200% (3584/7000)\n",
      "Test: Loss: 1.341 | Acc: 51.155% (3632/7100)\n",
      "Test: Loss: 1.341 | Acc: 51.194% (3686/7200)\n",
      "Test: Loss: 1.341 | Acc: 51.123% (3732/7300)\n",
      "Test: Loss: 1.338 | Acc: 51.149% (3785/7400)\n",
      "Test: Loss: 1.339 | Acc: 51.120% (3834/7500)\n",
      "Test: Loss: 1.339 | Acc: 51.158% (3888/7600)\n",
      "Test: Loss: 1.339 | Acc: 51.156% (3939/7700)\n",
      "Test: Loss: 1.338 | Acc: 51.167% (3991/7800)\n",
      "Test: Loss: 1.338 | Acc: 51.165% (4042/7900)\n",
      "Test: Loss: 1.340 | Acc: 51.062% (4085/8000)\n",
      "Test: Loss: 1.340 | Acc: 50.988% (4130/8100)\n",
      "Test: Loss: 1.341 | Acc: 50.915% (4175/8200)\n",
      "Test: Loss: 1.342 | Acc: 50.855% (4221/8300)\n",
      "Test: Loss: 1.343 | Acc: 50.786% (4266/8400)\n",
      "Test: Loss: 1.345 | Acc: 50.718% (4311/8500)\n",
      "Test: Loss: 1.343 | Acc: 50.837% (4372/8600)\n",
      "Test: Loss: 1.345 | Acc: 50.736% (4414/8700)\n",
      "Test: Loss: 1.345 | Acc: 50.750% (4466/8800)\n",
      "Test: Loss: 1.344 | Acc: 50.798% (4521/8900)\n",
      "Test: Loss: 1.345 | Acc: 50.856% (4577/9000)\n",
      "Test: Loss: 1.345 | Acc: 50.813% (4624/9100)\n",
      "Test: Loss: 1.346 | Acc: 50.772% (4671/9200)\n",
      "Test: Loss: 1.345 | Acc: 50.882% (4732/9300)\n",
      "Test: Loss: 1.345 | Acc: 50.830% (4778/9400)\n",
      "Test: Loss: 1.345 | Acc: 50.811% (4827/9500)\n",
      "Test: Loss: 1.344 | Acc: 50.823% (4879/9600)\n",
      "Test: Loss: 1.343 | Acc: 50.887% (4936/9700)\n",
      "Test: Loss: 1.344 | Acc: 50.827% (4981/9800)\n",
      "Test: Loss: 1.344 | Acc: 50.788% (5028/9900)\n",
      "Test: Loss: 1.343 | Acc: 50.760% (5076/10000)\n",
      "Train: Loss: 1.296 | Acc: 55.469% (71/128)\n",
      "Train: Loss: 1.351 | Acc: 48.828% (125/256)\n",
      "Train: Loss: 1.312 | Acc: 50.781% (195/384)\n",
      "Train: Loss: 1.322 | Acc: 49.609% (254/512)\n",
      "Train: Loss: 1.347 | Acc: 49.375% (316/640)\n",
      "Train: Loss: 1.327 | Acc: 50.391% (387/768)\n",
      "Train: Loss: 1.349 | Acc: 48.996% (439/896)\n",
      "Train: Loss: 1.335 | Acc: 49.902% (511/1024)\n",
      "Train: Loss: 1.343 | Acc: 49.740% (573/1152)\n",
      "Train: Loss: 1.360 | Acc: 49.609% (635/1280)\n",
      "Train: Loss: 1.355 | Acc: 49.929% (703/1408)\n",
      "Train: Loss: 1.376 | Acc: 48.763% (749/1536)\n",
      "Train: Loss: 1.375 | Acc: 48.678% (810/1664)\n",
      "Train: Loss: 1.380 | Acc: 48.549% (870/1792)\n",
      "Train: Loss: 1.384 | Acc: 48.281% (927/1920)\n",
      "Train: Loss: 1.381 | Acc: 48.535% (994/2048)\n",
      "Train: Loss: 1.385 | Acc: 48.667% (1059/2176)\n",
      "Train: Loss: 1.374 | Acc: 49.349% (1137/2304)\n",
      "Train: Loss: 1.379 | Acc: 49.301% (1199/2432)\n",
      "Train: Loss: 1.387 | Acc: 48.945% (1253/2560)\n",
      "Train: Loss: 1.388 | Acc: 48.847% (1313/2688)\n",
      "Train: Loss: 1.384 | Acc: 49.148% (1384/2816)\n",
      "Train: Loss: 1.377 | Acc: 49.423% (1455/2944)\n",
      "Train: Loss: 1.372 | Acc: 49.609% (1524/3072)\n",
      "Train: Loss: 1.367 | Acc: 49.969% (1599/3200)\n",
      "Train: Loss: 1.368 | Acc: 49.820% (1658/3328)\n",
      "Train: Loss: 1.366 | Acc: 49.769% (1720/3456)\n",
      "Train: Loss: 1.359 | Acc: 50.056% (1794/3584)\n",
      "Train: Loss: 1.353 | Acc: 50.404% (1871/3712)\n",
      "Train: Loss: 1.356 | Acc: 50.417% (1936/3840)\n",
      "Train: Loss: 1.355 | Acc: 50.302% (1996/3968)\n",
      "Train: Loss: 1.350 | Acc: 50.391% (2064/4096)\n",
      "Train: Loss: 1.348 | Acc: 50.331% (2126/4224)\n",
      "Train: Loss: 1.353 | Acc: 50.230% (2186/4352)\n",
      "Train: Loss: 1.355 | Acc: 50.045% (2242/4480)\n",
      "Train: Loss: 1.358 | Acc: 50.065% (2307/4608)\n",
      "Train: Loss: 1.362 | Acc: 49.894% (2363/4736)\n",
      "Train: Loss: 1.356 | Acc: 50.164% (2440/4864)\n",
      "Train: Loss: 1.356 | Acc: 50.200% (2506/4992)\n",
      "Train: Loss: 1.354 | Acc: 50.254% (2573/5120)\n",
      "Train: Loss: 1.352 | Acc: 50.362% (2643/5248)\n",
      "Train: Loss: 1.350 | Acc: 50.558% (2718/5376)\n",
      "Train: Loss: 1.348 | Acc: 50.727% (2792/5504)\n",
      "Train: Loss: 1.346 | Acc: 50.906% (2867/5632)\n",
      "Train: Loss: 1.341 | Acc: 50.920% (2933/5760)\n",
      "Train: Loss: 1.341 | Acc: 50.968% (3001/5888)\n",
      "Train: Loss: 1.339 | Acc: 51.047% (3071/6016)\n",
      "Train: Loss: 1.338 | Acc: 51.058% (3137/6144)\n",
      "Train: Loss: 1.336 | Acc: 51.052% (3202/6272)\n",
      "Train: Loss: 1.335 | Acc: 51.109% (3271/6400)\n",
      "Train: Loss: 1.332 | Acc: 51.103% (3336/6528)\n",
      "Train: Loss: 1.330 | Acc: 51.187% (3407/6656)\n",
      "Train: Loss: 1.327 | Acc: 51.282% (3479/6784)\n",
      "Train: Loss: 1.326 | Acc: 51.317% (3547/6912)\n",
      "Train: Loss: 1.323 | Acc: 51.477% (3624/7040)\n",
      "Train: Loss: 1.321 | Acc: 51.646% (3702/7168)\n",
      "Train: Loss: 1.320 | Acc: 51.686% (3771/7296)\n",
      "Train: Loss: 1.318 | Acc: 51.711% (3839/7424)\n",
      "Train: Loss: 1.319 | Acc: 51.642% (3900/7552)\n",
      "Train: Loss: 1.318 | Acc: 51.680% (3969/7680)\n",
      "Train: Loss: 1.319 | Acc: 51.691% (4036/7808)\n",
      "Train: Loss: 1.318 | Acc: 51.714% (4104/7936)\n",
      "Train: Loss: 1.314 | Acc: 51.860% (4182/8064)\n",
      "Train: Loss: 1.313 | Acc: 51.941% (4255/8192)\n",
      "Train: Loss: 1.312 | Acc: 51.971% (4324/8320)\n",
      "Train: Loss: 1.311 | Acc: 52.143% (4405/8448)\n",
      "Train: Loss: 1.313 | Acc: 52.122% (4470/8576)\n",
      "Train: Loss: 1.313 | Acc: 52.102% (4535/8704)\n",
      "Train: Loss: 1.311 | Acc: 52.219% (4612/8832)\n",
      "Train: Loss: 1.311 | Acc: 52.243% (4681/8960)\n",
      "Train: Loss: 1.308 | Acc: 52.377% (4760/9088)\n",
      "Train: Loss: 1.307 | Acc: 52.365% (4826/9216)\n",
      "Train: Loss: 1.307 | Acc: 52.408% (4897/9344)\n",
      "Train: Loss: 1.306 | Acc: 52.470% (4970/9472)\n",
      "Train: Loss: 1.305 | Acc: 52.479% (5038/9600)\n",
      "Train: Loss: 1.304 | Acc: 52.467% (5104/9728)\n",
      "Train: Loss: 1.303 | Acc: 52.537% (5178/9856)\n",
      "Train: Loss: 1.303 | Acc: 52.544% (5246/9984)\n",
      "Train: Loss: 1.302 | Acc: 52.512% (5310/10112)\n",
      "Train: Loss: 1.302 | Acc: 52.520% (5378/10240)\n",
      "Train: Loss: 1.300 | Acc: 52.546% (5448/10368)\n",
      "Train: Loss: 1.299 | Acc: 52.630% (5524/10496)\n",
      "Train: Loss: 1.300 | Acc: 52.645% (5593/10624)\n",
      "Train: Loss: 1.298 | Acc: 52.688% (5665/10752)\n",
      "Train: Loss: 1.296 | Acc: 52.684% (5732/10880)\n",
      "Train: Loss: 1.295 | Acc: 52.762% (5808/11008)\n",
      "Train: Loss: 1.296 | Acc: 52.721% (5871/11136)\n",
      "Train: Loss: 1.297 | Acc: 52.725% (5939/11264)\n",
      "Train: Loss: 1.296 | Acc: 52.748% (6009/11392)\n",
      "Train: Loss: 1.293 | Acc: 52.882% (6092/11520)\n",
      "Train: Loss: 1.293 | Acc: 52.859% (6157/11648)\n",
      "Train: Loss: 1.293 | Acc: 52.819% (6220/11776)\n",
      "Train: Loss: 1.294 | Acc: 52.839% (6290/11904)\n",
      "Train: Loss: 1.292 | Acc: 52.876% (6362/12032)\n",
      "Train: Loss: 1.291 | Acc: 52.862% (6428/12160)\n",
      "Train: Loss: 1.294 | Acc: 52.759% (6483/12288)\n",
      "Train: Loss: 1.292 | Acc: 52.843% (6561/12416)\n",
      "Train: Loss: 1.295 | Acc: 52.758% (6618/12544)\n",
      "Train: Loss: 1.293 | Acc: 52.849% (6697/12672)\n",
      "Train: Loss: 1.293 | Acc: 52.836% (6763/12800)\n",
      "Train: Loss: 1.292 | Acc: 52.893% (6838/12928)\n",
      "Train: Loss: 1.291 | Acc: 52.918% (6909/13056)\n",
      "Train: Loss: 1.291 | Acc: 52.935% (6979/13184)\n",
      "Train: Loss: 1.290 | Acc: 52.990% (7054/13312)\n",
      "Train: Loss: 1.289 | Acc: 53.051% (7130/13440)\n",
      "Train: Loss: 1.287 | Acc: 53.118% (7207/13568)\n",
      "Train: Loss: 1.287 | Acc: 53.096% (7272/13696)\n",
      "Train: Loss: 1.287 | Acc: 53.176% (7351/13824)\n",
      "Train: Loss: 1.286 | Acc: 53.168% (7418/13952)\n",
      "Train: Loss: 1.285 | Acc: 53.182% (7488/14080)\n",
      "Train: Loss: 1.286 | Acc: 53.181% (7556/14208)\n",
      "Train: Loss: 1.285 | Acc: 53.244% (7633/14336)\n",
      "Train: Loss: 1.284 | Acc: 53.291% (7708/14464)\n",
      "Train: Loss: 1.283 | Acc: 53.289% (7776/14592)\n",
      "Train: Loss: 1.284 | Acc: 53.254% (7839/14720)\n",
      "Train: Loss: 1.283 | Acc: 53.287% (7912/14848)\n",
      "Train: Loss: 1.283 | Acc: 53.305% (7983/14976)\n",
      "Train: Loss: 1.282 | Acc: 53.304% (8051/15104)\n",
      "Train: Loss: 1.281 | Acc: 53.329% (8123/15232)\n",
      "Train: Loss: 1.279 | Acc: 53.385% (8200/15360)\n",
      "Train: Loss: 1.278 | Acc: 53.428% (8275/15488)\n",
      "Train: Loss: 1.278 | Acc: 53.452% (8347/15616)\n",
      "Train: Loss: 1.279 | Acc: 53.430% (8412/15744)\n",
      "Train: Loss: 1.280 | Acc: 53.396% (8475/15872)\n",
      "Train: Loss: 1.278 | Acc: 53.425% (8548/16000)\n",
      "Train: Loss: 1.279 | Acc: 53.441% (8619/16128)\n",
      "Train: Loss: 1.277 | Acc: 53.500% (8697/16256)\n",
      "Train: Loss: 1.277 | Acc: 53.522% (8769/16384)\n",
      "Train: Loss: 1.277 | Acc: 53.537% (8840/16512)\n",
      "Train: Loss: 1.275 | Acc: 53.624% (8923/16640)\n",
      "Train: Loss: 1.275 | Acc: 53.596% (8987/16768)\n",
      "Train: Loss: 1.274 | Acc: 53.658% (9066/16896)\n",
      "Train: Loss: 1.274 | Acc: 53.660% (9135/17024)\n",
      "Train: Loss: 1.273 | Acc: 53.661% (9204/17152)\n",
      "Train: Loss: 1.271 | Acc: 53.738% (9286/17280)\n",
      "Train: Loss: 1.270 | Acc: 53.791% (9364/17408)\n",
      "Train: Loss: 1.270 | Acc: 53.809% (9436/17536)\n",
      "Train: Loss: 1.270 | Acc: 53.816% (9506/17664)\n",
      "Train: Loss: 1.270 | Acc: 53.850% (9581/17792)\n",
      "Train: Loss: 1.269 | Acc: 53.878% (9655/17920)\n",
      "Train: Loss: 1.268 | Acc: 53.917% (9731/18048)\n",
      "Train: Loss: 1.268 | Acc: 53.923% (9801/18176)\n",
      "Train: Loss: 1.267 | Acc: 53.934% (9872/18304)\n",
      "Train: Loss: 1.266 | Acc: 53.961% (9946/18432)\n",
      "Train: Loss: 1.265 | Acc: 53.949% (10013/18560)\n",
      "Train: Loss: 1.264 | Acc: 53.965% (10085/18688)\n",
      "Train: Loss: 1.264 | Acc: 54.007% (10162/18816)\n",
      "Train: Loss: 1.263 | Acc: 53.991% (10228/18944)\n",
      "Train: Loss: 1.263 | Acc: 54.022% (10303/19072)\n",
      "Train: Loss: 1.263 | Acc: 54.026% (10373/19200)\n",
      "Train: Loss: 1.262 | Acc: 54.061% (10449/19328)\n",
      "Train: Loss: 1.261 | Acc: 54.117% (10529/19456)\n",
      "Train: Loss: 1.261 | Acc: 54.110% (10597/19584)\n",
      "Train: Loss: 1.260 | Acc: 54.119% (10668/19712)\n",
      "Train: Loss: 1.259 | Acc: 54.148% (10743/19840)\n",
      "Train: Loss: 1.258 | Acc: 54.217% (10826/19968)\n",
      "Train: Loss: 1.257 | Acc: 54.274% (10907/20096)\n",
      "Train: Loss: 1.256 | Acc: 54.302% (10982/20224)\n",
      "Train: Loss: 1.255 | Acc: 54.314% (11054/20352)\n",
      "Train: Loss: 1.255 | Acc: 54.282% (11117/20480)\n",
      "Train: Loss: 1.254 | Acc: 54.299% (11190/20608)\n",
      "Train: Loss: 1.254 | Acc: 54.311% (11262/20736)\n",
      "Train: Loss: 1.253 | Acc: 54.347% (11339/20864)\n",
      "Train: Loss: 1.254 | Acc: 54.306% (11400/20992)\n",
      "Train: Loss: 1.254 | Acc: 54.309% (11470/21120)\n",
      "Train: Loss: 1.256 | Acc: 54.278% (11533/21248)\n",
      "Train: Loss: 1.254 | Acc: 54.351% (11618/21376)\n",
      "Train: Loss: 1.255 | Acc: 54.334% (11684/21504)\n",
      "Train: Loss: 1.255 | Acc: 54.327% (11752/21632)\n",
      "Train: Loss: 1.254 | Acc: 54.352% (11827/21760)\n",
      "Train: Loss: 1.254 | Acc: 54.363% (11899/21888)\n",
      "Train: Loss: 1.254 | Acc: 54.406% (11978/22016)\n",
      "Train: Loss: 1.253 | Acc: 54.430% (12053/22144)\n",
      "Train: Loss: 1.253 | Acc: 54.414% (12119/22272)\n",
      "Train: Loss: 1.253 | Acc: 54.433% (12193/22400)\n",
      "Train: Loss: 1.253 | Acc: 54.421% (12260/22528)\n",
      "Train: Loss: 1.252 | Acc: 54.454% (12337/22656)\n",
      "Train: Loss: 1.252 | Acc: 54.459% (12408/22784)\n",
      "Train: Loss: 1.251 | Acc: 54.474% (12481/22912)\n",
      "Train: Loss: 1.251 | Acc: 54.505% (12558/23040)\n",
      "Train: Loss: 1.251 | Acc: 54.502% (12627/23168)\n",
      "Train: Loss: 1.250 | Acc: 54.550% (12708/23296)\n",
      "Train: Loss: 1.250 | Acc: 54.572% (12783/23424)\n",
      "Train: Loss: 1.249 | Acc: 54.611% (12862/23552)\n",
      "Train: Loss: 1.248 | Acc: 54.658% (12943/23680)\n",
      "Train: Loss: 1.248 | Acc: 54.641% (13009/23808)\n",
      "Train: Loss: 1.247 | Acc: 54.658% (13083/23936)\n",
      "Train: Loss: 1.247 | Acc: 54.658% (13153/24064)\n",
      "Train: Loss: 1.246 | Acc: 54.700% (13233/24192)\n",
      "Train: Loss: 1.246 | Acc: 54.729% (13310/24320)\n",
      "Train: Loss: 1.245 | Acc: 54.741% (13383/24448)\n",
      "Train: Loss: 1.245 | Acc: 54.761% (13458/24576)\n",
      "Train: Loss: 1.245 | Acc: 54.793% (13536/24704)\n",
      "Train: Loss: 1.244 | Acc: 54.853% (13621/24832)\n",
      "Train: Loss: 1.244 | Acc: 54.856% (13692/24960)\n",
      "Train: Loss: 1.244 | Acc: 54.867% (13765/25088)\n",
      "Train: Loss: 1.245 | Acc: 54.862% (13834/25216)\n",
      "Train: Loss: 1.244 | Acc: 54.865% (13905/25344)\n",
      "Train: Loss: 1.243 | Acc: 54.899% (13984/25472)\n",
      "Train: Loss: 1.243 | Acc: 54.926% (14061/25600)\n",
      "Train: Loss: 1.243 | Acc: 54.917% (14129/25728)\n",
      "Train: Loss: 1.243 | Acc: 54.935% (14204/25856)\n",
      "Train: Loss: 1.242 | Acc: 54.961% (14281/25984)\n",
      "Train: Loss: 1.242 | Acc: 55.009% (14364/26112)\n",
      "Train: Loss: 1.241 | Acc: 55.000% (14432/26240)\n",
      "Train: Loss: 1.241 | Acc: 55.040% (14513/26368)\n",
      "Train: Loss: 1.241 | Acc: 55.027% (14580/26496)\n",
      "Train: Loss: 1.239 | Acc: 55.063% (14660/26624)\n",
      "Train: Loss: 1.239 | Acc: 55.084% (14736/26752)\n",
      "Train: Loss: 1.238 | Acc: 55.138% (14821/26880)\n",
      "Train: Loss: 1.239 | Acc: 55.117% (14886/27008)\n",
      "Train: Loss: 1.239 | Acc: 55.119% (14957/27136)\n",
      "Train: Loss: 1.238 | Acc: 55.175% (15043/27264)\n",
      "Train: Loss: 1.238 | Acc: 55.180% (15115/27392)\n",
      "Train: Loss: 1.237 | Acc: 55.178% (15185/27520)\n",
      "Train: Loss: 1.238 | Acc: 55.172% (15254/27648)\n",
      "Train: Loss: 1.238 | Acc: 55.170% (15324/27776)\n",
      "Train: Loss: 1.238 | Acc: 55.168% (15394/27904)\n",
      "Train: Loss: 1.238 | Acc: 55.158% (15462/28032)\n",
      "Train: Loss: 1.237 | Acc: 55.156% (15532/28160)\n",
      "Train: Loss: 1.238 | Acc: 55.122% (15593/28288)\n",
      "Train: Loss: 1.237 | Acc: 55.131% (15666/28416)\n",
      "Train: Loss: 1.237 | Acc: 55.139% (15739/28544)\n",
      "Train: Loss: 1.237 | Acc: 55.158% (15815/28672)\n",
      "Train: Loss: 1.236 | Acc: 55.181% (15892/28800)\n",
      "Train: Loss: 1.236 | Acc: 55.199% (15968/28928)\n",
      "Train: Loss: 1.236 | Acc: 55.183% (16034/29056)\n",
      "Train: Loss: 1.236 | Acc: 55.188% (16106/29184)\n",
      "Train: Loss: 1.236 | Acc: 55.206% (16182/29312)\n",
      "Train: Loss: 1.235 | Acc: 55.265% (16270/29440)\n",
      "Train: Loss: 1.234 | Acc: 55.300% (16351/29568)\n",
      "Train: Loss: 1.234 | Acc: 55.300% (16422/29696)\n",
      "Train: Loss: 1.234 | Acc: 55.294% (16491/29824)\n",
      "Train: Loss: 1.233 | Acc: 55.335% (16574/29952)\n",
      "Train: Loss: 1.233 | Acc: 55.372% (16656/30080)\n",
      "Train: Loss: 1.232 | Acc: 55.403% (16736/30208)\n",
      "Train: Loss: 1.231 | Acc: 55.419% (16812/30336)\n",
      "Train: Loss: 1.231 | Acc: 55.426% (16885/30464)\n",
      "Train: Loss: 1.230 | Acc: 55.449% (16963/30592)\n",
      "Train: Loss: 1.230 | Acc: 55.459% (17037/30720)\n",
      "Train: Loss: 1.231 | Acc: 55.443% (17103/30848)\n",
      "Train: Loss: 1.230 | Acc: 55.488% (17188/30976)\n",
      "Train: Loss: 1.230 | Acc: 55.498% (17262/31104)\n",
      "Train: Loss: 1.229 | Acc: 55.523% (17341/31232)\n",
      "Train: Loss: 1.229 | Acc: 55.533% (17415/31360)\n",
      "Train: Loss: 1.229 | Acc: 55.545% (17490/31488)\n",
      "Train: Loss: 1.229 | Acc: 55.538% (17559/31616)\n",
      "Train: Loss: 1.228 | Acc: 55.566% (17639/31744)\n",
      "Train: Loss: 1.227 | Acc: 55.594% (17719/31872)\n",
      "Train: Loss: 1.226 | Acc: 55.653% (17809/32000)\n",
      "Train: Loss: 1.226 | Acc: 55.668% (17885/32128)\n",
      "Train: Loss: 1.227 | Acc: 55.652% (17951/32256)\n",
      "Train: Loss: 1.226 | Acc: 55.663% (18026/32384)\n",
      "Train: Loss: 1.226 | Acc: 55.672% (18100/32512)\n",
      "Train: Loss: 1.226 | Acc: 55.692% (18178/32640)\n",
      "Train: Loss: 1.225 | Acc: 55.728% (18261/32768)\n",
      "Train: Loss: 1.224 | Acc: 55.754% (18341/32896)\n",
      "Train: Loss: 1.224 | Acc: 55.772% (18418/33024)\n",
      "Train: Loss: 1.224 | Acc: 55.773% (18490/33152)\n",
      "Train: Loss: 1.223 | Acc: 55.802% (18571/33280)\n",
      "Train: Loss: 1.223 | Acc: 55.813% (18646/33408)\n",
      "Train: Loss: 1.222 | Acc: 55.841% (18727/33536)\n",
      "Train: Loss: 1.221 | Acc: 55.885% (18813/33664)\n",
      "Train: Loss: 1.221 | Acc: 55.883% (18884/33792)\n",
      "Train: Loss: 1.221 | Acc: 55.884% (18956/33920)\n",
      "Train: Loss: 1.221 | Acc: 55.889% (19029/34048)\n",
      "Train: Loss: 1.221 | Acc: 55.899% (19104/34176)\n",
      "Train: Loss: 1.221 | Acc: 55.906% (19178/34304)\n",
      "Train: Loss: 1.220 | Acc: 55.916% (19253/34432)\n",
      "Train: Loss: 1.220 | Acc: 55.955% (19338/34560)\n",
      "Train: Loss: 1.219 | Acc: 55.950% (19408/34688)\n",
      "Train: Loss: 1.219 | Acc: 55.957% (19482/34816)\n",
      "Train: Loss: 1.219 | Acc: 55.944% (19549/34944)\n",
      "Train: Loss: 1.219 | Acc: 55.942% (19620/35072)\n",
      "Train: Loss: 1.218 | Acc: 55.963% (19699/35200)\n",
      "Train: Loss: 1.218 | Acc: 55.970% (19773/35328)\n",
      "Train: Loss: 1.218 | Acc: 55.959% (19841/35456)\n",
      "Train: Loss: 1.218 | Acc: 55.966% (19915/35584)\n",
      "Train: Loss: 1.217 | Acc: 55.995% (19997/35712)\n",
      "Train: Loss: 1.217 | Acc: 55.991% (20067/35840)\n",
      "Train: Loss: 1.217 | Acc: 55.978% (20134/35968)\n",
      "Train: Loss: 1.216 | Acc: 56.034% (20226/36096)\n",
      "Train: Loss: 1.215 | Acc: 56.054% (20305/36224)\n",
      "Train: Loss: 1.215 | Acc: 56.074% (20384/36352)\n",
      "Train: Loss: 1.214 | Acc: 56.080% (20458/36480)\n",
      "Train: Loss: 1.214 | Acc: 56.083% (20531/36608)\n",
      "Train: Loss: 1.214 | Acc: 56.100% (20609/36736)\n",
      "Train: Loss: 1.213 | Acc: 56.125% (20690/36864)\n",
      "Train: Loss: 1.212 | Acc: 56.158% (20774/36992)\n",
      "Train: Loss: 1.212 | Acc: 56.191% (20858/37120)\n",
      "Train: Loss: 1.211 | Acc: 56.234% (20946/37248)\n",
      "Train: Loss: 1.210 | Acc: 56.250% (21024/37376)\n",
      "Train: Loss: 1.209 | Acc: 56.282% (21108/37504)\n",
      "Train: Loss: 1.209 | Acc: 56.306% (21189/37632)\n",
      "Train: Loss: 1.208 | Acc: 56.332% (21271/37760)\n",
      "Train: Loss: 1.208 | Acc: 56.350% (21350/37888)\n",
      "Train: Loss: 1.207 | Acc: 56.368% (21429/38016)\n",
      "Train: Loss: 1.207 | Acc: 56.371% (21502/38144)\n",
      "Train: Loss: 1.207 | Acc: 56.388% (21581/38272)\n",
      "Train: Loss: 1.206 | Acc: 56.424% (21667/38400)\n",
      "Train: Loss: 1.205 | Acc: 56.455% (21751/38528)\n",
      "Train: Loss: 1.204 | Acc: 56.472% (21830/38656)\n",
      "Train: Loss: 1.203 | Acc: 56.505% (21915/38784)\n",
      "Train: Loss: 1.203 | Acc: 56.533% (21998/38912)\n",
      "Train: Loss: 1.202 | Acc: 56.555% (22079/39040)\n",
      "Train: Loss: 1.202 | Acc: 56.572% (22158/39168)\n",
      "Train: Loss: 1.202 | Acc: 56.578% (22233/39296)\n",
      "Train: Loss: 1.202 | Acc: 56.572% (22303/39424)\n",
      "Train: Loss: 1.201 | Acc: 56.601% (22387/39552)\n",
      "Train: Loss: 1.201 | Acc: 56.615% (22465/39680)\n",
      "Train: Loss: 1.201 | Acc: 56.632% (22544/39808)\n",
      "Train: Loss: 1.200 | Acc: 56.646% (22622/39936)\n",
      "Train: Loss: 1.200 | Acc: 56.677% (22707/40064)\n",
      "Train: Loss: 1.199 | Acc: 56.690% (22785/40192)\n",
      "Train: Loss: 1.199 | Acc: 56.709% (22865/40320)\n",
      "Train: Loss: 1.198 | Acc: 56.744% (22952/40448)\n",
      "Train: Loss: 1.197 | Acc: 56.760% (23031/40576)\n",
      "Train: Loss: 1.196 | Acc: 56.783% (23113/40704)\n",
      "Train: Loss: 1.196 | Acc: 56.803% (23194/40832)\n",
      "Train: Loss: 1.195 | Acc: 56.829% (23277/40960)\n",
      "Train: Loss: 1.195 | Acc: 56.844% (23356/41088)\n",
      "Train: Loss: 1.194 | Acc: 56.854% (23433/41216)\n",
      "Train: Loss: 1.194 | Acc: 56.852% (23505/41344)\n",
      "Train: Loss: 1.194 | Acc: 56.877% (23588/41472)\n",
      "Train: Loss: 1.194 | Acc: 56.894% (23668/41600)\n",
      "Train: Loss: 1.193 | Acc: 56.899% (23743/41728)\n",
      "Train: Loss: 1.193 | Acc: 56.936% (23831/41856)\n",
      "Train: Loss: 1.193 | Acc: 56.943% (23907/41984)\n",
      "Train: Loss: 1.193 | Acc: 56.946% (23981/42112)\n",
      "Train: Loss: 1.192 | Acc: 56.984% (24070/42240)\n",
      "Train: Loss: 1.192 | Acc: 56.984% (24143/42368)\n",
      "Train: Loss: 1.192 | Acc: 57.001% (24223/42496)\n",
      "Train: Loss: 1.192 | Acc: 57.003% (24297/42624)\n",
      "Train: Loss: 1.191 | Acc: 57.015% (24375/42752)\n",
      "Train: Loss: 1.191 | Acc: 57.043% (24460/42880)\n",
      "Train: Loss: 1.190 | Acc: 57.080% (24549/43008)\n",
      "Train: Loss: 1.190 | Acc: 57.085% (24624/43136)\n",
      "Train: Loss: 1.190 | Acc: 57.082% (24696/43264)\n",
      "Train: Loss: 1.190 | Acc: 57.098% (24776/43392)\n",
      "Train: Loss: 1.189 | Acc: 57.119% (24858/43520)\n",
      "Train: Loss: 1.189 | Acc: 57.139% (24940/43648)\n",
      "Train: Loss: 1.188 | Acc: 57.171% (25027/43776)\n",
      "Train: Loss: 1.188 | Acc: 57.195% (25111/43904)\n",
      "Train: Loss: 1.187 | Acc: 57.222% (25196/44032)\n",
      "Train: Loss: 1.186 | Acc: 57.235% (25275/44160)\n",
      "Train: Loss: 1.186 | Acc: 57.243% (25352/44288)\n",
      "Train: Loss: 1.185 | Acc: 57.261% (25433/44416)\n",
      "Train: Loss: 1.186 | Acc: 57.254% (25503/44544)\n",
      "Train: Loss: 1.185 | Acc: 57.271% (25584/44672)\n",
      "Train: Loss: 1.184 | Acc: 57.306% (25673/44800)\n",
      "Train: Loss: 1.184 | Acc: 57.318% (25752/44928)\n",
      "Train: Loss: 1.184 | Acc: 57.326% (25829/45056)\n",
      "Train: Loss: 1.183 | Acc: 57.339% (25908/45184)\n",
      "Train: Loss: 1.183 | Acc: 57.351% (25987/45312)\n",
      "Train: Loss: 1.183 | Acc: 57.366% (26067/45440)\n",
      "Train: Loss: 1.183 | Acc: 57.369% (26142/45568)\n",
      "Train: Loss: 1.182 | Acc: 57.392% (26226/45696)\n",
      "Train: Loss: 1.182 | Acc: 57.407% (26306/45824)\n",
      "Train: Loss: 1.182 | Acc: 57.397% (26375/45952)\n",
      "Train: Loss: 1.181 | Acc: 57.418% (26458/46080)\n",
      "Train: Loss: 1.181 | Acc: 57.436% (26540/46208)\n",
      "Train: Loss: 1.181 | Acc: 57.454% (26622/46336)\n",
      "Train: Loss: 1.181 | Acc: 57.466% (26701/46464)\n",
      "Train: Loss: 1.180 | Acc: 57.497% (26789/46592)\n",
      "Train: Loss: 1.180 | Acc: 57.494% (26861/46720)\n",
      "Train: Loss: 1.180 | Acc: 57.503% (26939/46848)\n",
      "Train: Loss: 1.180 | Acc: 57.527% (27024/46976)\n",
      "Train: Loss: 1.179 | Acc: 57.549% (27108/47104)\n",
      "Train: Loss: 1.179 | Acc: 57.590% (27201/47232)\n",
      "Train: Loss: 1.178 | Acc: 57.606% (27282/47360)\n",
      "Train: Loss: 1.177 | Acc: 57.640% (27372/47488)\n",
      "Train: Loss: 1.177 | Acc: 57.668% (27459/47616)\n",
      "Train: Loss: 1.176 | Acc: 57.676% (27537/47744)\n",
      "Train: Loss: 1.176 | Acc: 57.702% (27623/47872)\n",
      "Train: Loss: 1.175 | Acc: 57.748% (27719/48000)\n",
      "Train: Loss: 1.174 | Acc: 57.771% (27804/48128)\n",
      "Train: Loss: 1.174 | Acc: 57.779% (27882/48256)\n",
      "Train: Loss: 1.173 | Acc: 57.796% (27964/48384)\n",
      "Train: Loss: 1.173 | Acc: 57.812% (28046/48512)\n",
      "Train: Loss: 1.173 | Acc: 57.825% (28126/48640)\n",
      "Train: Loss: 1.172 | Acc: 57.856% (28215/48768)\n",
      "Train: Loss: 1.172 | Acc: 57.860% (28291/48896)\n",
      "Train: Loss: 1.171 | Acc: 57.872% (28371/49024)\n",
      "Train: Loss: 1.171 | Acc: 57.882% (28450/49152)\n",
      "Train: Loss: 1.170 | Acc: 57.906% (28536/49280)\n",
      "Train: Loss: 1.170 | Acc: 57.926% (28620/49408)\n",
      "Train: Loss: 1.169 | Acc: 57.954% (28708/49536)\n",
      "Train: Loss: 1.169 | Acc: 57.955% (28783/49664)\n",
      "Train: Loss: 1.168 | Acc: 57.967% (28863/49792)\n",
      "Train: Loss: 1.168 | Acc: 57.979% (28943/49920)\n",
      "Train: Loss: 1.168 | Acc: 57.998% (28999/50000)\n",
      "Test: Loss: 0.948 | Acc: 66.000% (66/100)\n",
      "Test: Loss: 1.111 | Acc: 62.500% (125/200)\n",
      "Test: Loss: 1.102 | Acc: 63.667% (191/300)\n",
      "Test: Loss: 1.117 | Acc: 62.000% (248/400)\n",
      "Test: Loss: 1.135 | Acc: 61.600% (308/500)\n",
      "Test: Loss: 1.094 | Acc: 62.500% (375/600)\n",
      "Test: Loss: 1.085 | Acc: 62.857% (440/700)\n",
      "Test: Loss: 1.092 | Acc: 62.625% (501/800)\n",
      "Test: Loss: 1.109 | Acc: 61.556% (554/900)\n",
      "Test: Loss: 1.103 | Acc: 62.000% (620/1000)\n",
      "Test: Loss: 1.097 | Acc: 62.000% (682/1100)\n",
      "Test: Loss: 1.094 | Acc: 62.083% (745/1200)\n",
      "Test: Loss: 1.090 | Acc: 61.769% (803/1300)\n",
      "Test: Loss: 1.088 | Acc: 61.857% (866/1400)\n",
      "Test: Loss: 1.081 | Acc: 61.733% (926/1500)\n",
      "Test: Loss: 1.079 | Acc: 61.875% (990/1600)\n",
      "Test: Loss: 1.070 | Acc: 62.235% (1058/1700)\n",
      "Test: Loss: 1.073 | Acc: 61.722% (1111/1800)\n",
      "Test: Loss: 1.068 | Acc: 62.000% (1178/1900)\n",
      "Test: Loss: 1.074 | Acc: 61.650% (1233/2000)\n",
      "Test: Loss: 1.080 | Acc: 61.571% (1293/2100)\n",
      "Test: Loss: 1.085 | Acc: 61.455% (1352/2200)\n",
      "Test: Loss: 1.091 | Acc: 61.217% (1408/2300)\n",
      "Test: Loss: 1.094 | Acc: 61.167% (1468/2400)\n",
      "Test: Loss: 1.092 | Acc: 61.200% (1530/2500)\n",
      "Test: Loss: 1.109 | Acc: 60.885% (1583/2600)\n",
      "Test: Loss: 1.109 | Acc: 60.926% (1645/2700)\n",
      "Test: Loss: 1.107 | Acc: 61.036% (1709/2800)\n",
      "Test: Loss: 1.105 | Acc: 61.069% (1771/2900)\n",
      "Test: Loss: 1.103 | Acc: 61.167% (1835/3000)\n",
      "Test: Loss: 1.103 | Acc: 61.097% (1894/3100)\n",
      "Test: Loss: 1.104 | Acc: 61.031% (1953/3200)\n",
      "Test: Loss: 1.103 | Acc: 61.152% (2018/3300)\n",
      "Test: Loss: 1.108 | Acc: 60.882% (2070/3400)\n",
      "Test: Loss: 1.113 | Acc: 60.829% (2129/3500)\n",
      "Test: Loss: 1.117 | Acc: 60.806% (2189/3600)\n",
      "Test: Loss: 1.120 | Acc: 60.973% (2256/3700)\n",
      "Test: Loss: 1.121 | Acc: 60.947% (2316/3800)\n",
      "Test: Loss: 1.116 | Acc: 61.103% (2383/3900)\n",
      "Test: Loss: 1.115 | Acc: 61.100% (2444/4000)\n",
      "Test: Loss: 1.114 | Acc: 61.098% (2505/4100)\n",
      "Test: Loss: 1.115 | Acc: 61.024% (2563/4200)\n",
      "Test: Loss: 1.112 | Acc: 61.163% (2630/4300)\n",
      "Test: Loss: 1.108 | Acc: 61.341% (2699/4400)\n",
      "Test: Loss: 1.111 | Acc: 61.244% (2756/4500)\n",
      "Test: Loss: 1.113 | Acc: 61.087% (2810/4600)\n",
      "Test: Loss: 1.111 | Acc: 61.043% (2869/4700)\n",
      "Test: Loss: 1.109 | Acc: 61.188% (2937/4800)\n",
      "Test: Loss: 1.106 | Acc: 61.204% (2999/4900)\n",
      "Test: Loss: 1.104 | Acc: 61.280% (3064/5000)\n",
      "Test: Loss: 1.102 | Acc: 61.333% (3128/5100)\n",
      "Test: Loss: 1.102 | Acc: 61.327% (3189/5200)\n",
      "Test: Loss: 1.100 | Acc: 61.283% (3248/5300)\n",
      "Test: Loss: 1.103 | Acc: 61.204% (3305/5400)\n",
      "Test: Loss: 1.102 | Acc: 61.309% (3372/5500)\n",
      "Test: Loss: 1.103 | Acc: 61.268% (3431/5600)\n",
      "Test: Loss: 1.102 | Acc: 61.368% (3498/5700)\n",
      "Test: Loss: 1.099 | Acc: 61.379% (3560/5800)\n",
      "Test: Loss: 1.100 | Acc: 61.271% (3615/5900)\n",
      "Test: Loss: 1.098 | Acc: 61.317% (3679/6000)\n",
      "Test: Loss: 1.100 | Acc: 61.180% (3732/6100)\n",
      "Test: Loss: 1.101 | Acc: 61.161% (3792/6200)\n",
      "Test: Loss: 1.101 | Acc: 61.238% (3858/6300)\n",
      "Test: Loss: 1.101 | Acc: 61.188% (3916/6400)\n",
      "Test: Loss: 1.102 | Acc: 61.108% (3972/6500)\n",
      "Test: Loss: 1.101 | Acc: 61.136% (4035/6600)\n",
      "Test: Loss: 1.101 | Acc: 61.104% (4094/6700)\n",
      "Test: Loss: 1.103 | Acc: 60.985% (4147/6800)\n",
      "Test: Loss: 1.103 | Acc: 60.884% (4201/6900)\n",
      "Test: Loss: 1.106 | Acc: 60.700% (4249/7000)\n",
      "Test: Loss: 1.106 | Acc: 60.634% (4305/7100)\n",
      "Test: Loss: 1.106 | Acc: 60.708% (4371/7200)\n",
      "Test: Loss: 1.105 | Acc: 60.712% (4432/7300)\n",
      "Test: Loss: 1.103 | Acc: 60.811% (4500/7400)\n",
      "Test: Loss: 1.104 | Acc: 60.867% (4565/7500)\n",
      "Test: Loss: 1.105 | Acc: 60.842% (4624/7600)\n",
      "Test: Loss: 1.107 | Acc: 60.753% (4678/7700)\n",
      "Test: Loss: 1.105 | Acc: 60.821% (4744/7800)\n",
      "Test: Loss: 1.107 | Acc: 60.772% (4801/7900)\n",
      "Test: Loss: 1.109 | Acc: 60.700% (4856/8000)\n",
      "Test: Loss: 1.109 | Acc: 60.753% (4921/8100)\n",
      "Test: Loss: 1.108 | Acc: 60.756% (4982/8200)\n",
      "Test: Loss: 1.107 | Acc: 60.795% (5046/8300)\n",
      "Test: Loss: 1.109 | Acc: 60.690% (5098/8400)\n",
      "Test: Loss: 1.110 | Acc: 60.671% (5157/8500)\n",
      "Test: Loss: 1.108 | Acc: 60.756% (5225/8600)\n",
      "Test: Loss: 1.110 | Acc: 60.690% (5280/8700)\n",
      "Test: Loss: 1.111 | Acc: 60.682% (5340/8800)\n",
      "Test: Loss: 1.114 | Acc: 60.629% (5396/8900)\n",
      "Test: Loss: 1.115 | Acc: 60.600% (5454/9000)\n",
      "Test: Loss: 1.116 | Acc: 60.505% (5506/9100)\n",
      "Test: Loss: 1.116 | Acc: 60.576% (5573/9200)\n",
      "Test: Loss: 1.115 | Acc: 60.613% (5637/9300)\n",
      "Test: Loss: 1.115 | Acc: 60.617% (5698/9400)\n",
      "Test: Loss: 1.115 | Acc: 60.621% (5759/9500)\n",
      "Test: Loss: 1.113 | Acc: 60.667% (5824/9600)\n",
      "Test: Loss: 1.112 | Acc: 60.742% (5892/9700)\n",
      "Test: Loss: 1.113 | Acc: 60.745% (5953/9800)\n",
      "Test: Loss: 1.114 | Acc: 60.707% (6010/9900)\n",
      "Test: Loss: 1.113 | Acc: 60.690% (6069/10000)\n",
      "Train: Loss: 1.002 | Acc: 64.062% (82/128)\n",
      "Train: Loss: 0.987 | Acc: 67.188% (172/256)\n",
      "Train: Loss: 0.971 | Acc: 66.406% (255/384)\n",
      "Train: Loss: 0.954 | Acc: 65.820% (337/512)\n",
      "Train: Loss: 0.968 | Acc: 65.000% (416/640)\n",
      "Train: Loss: 0.955 | Acc: 66.016% (507/768)\n",
      "Train: Loss: 0.947 | Acc: 66.071% (592/896)\n",
      "Train: Loss: 0.946 | Acc: 66.406% (680/1024)\n",
      "Train: Loss: 0.955 | Acc: 66.233% (763/1152)\n",
      "Train: Loss: 0.950 | Acc: 66.328% (849/1280)\n",
      "Train: Loss: 0.964 | Acc: 66.051% (930/1408)\n",
      "Train: Loss: 0.963 | Acc: 65.820% (1011/1536)\n",
      "Train: Loss: 0.948 | Acc: 66.226% (1102/1664)\n",
      "Train: Loss: 0.952 | Acc: 66.239% (1187/1792)\n",
      "Train: Loss: 0.941 | Acc: 67.031% (1287/1920)\n",
      "Train: Loss: 0.931 | Acc: 67.432% (1381/2048)\n",
      "Train: Loss: 0.925 | Acc: 67.509% (1469/2176)\n",
      "Train: Loss: 0.921 | Acc: 67.708% (1560/2304)\n",
      "Train: Loss: 0.926 | Acc: 67.270% (1636/2432)\n",
      "Train: Loss: 0.925 | Acc: 67.305% (1723/2560)\n",
      "Train: Loss: 0.934 | Acc: 66.927% (1799/2688)\n",
      "Train: Loss: 0.950 | Acc: 66.619% (1876/2816)\n",
      "Train: Loss: 0.947 | Acc: 66.576% (1960/2944)\n",
      "Train: Loss: 0.953 | Acc: 66.471% (2042/3072)\n",
      "Train: Loss: 0.951 | Acc: 66.562% (2130/3200)\n",
      "Train: Loss: 0.950 | Acc: 66.496% (2213/3328)\n",
      "Train: Loss: 0.950 | Acc: 66.464% (2297/3456)\n",
      "Train: Loss: 0.944 | Acc: 66.657% (2389/3584)\n",
      "Train: Loss: 0.943 | Acc: 66.703% (2476/3712)\n",
      "Train: Loss: 0.942 | Acc: 66.589% (2557/3840)\n",
      "Train: Loss: 0.941 | Acc: 66.532% (2640/3968)\n",
      "Train: Loss: 0.944 | Acc: 66.675% (2731/4096)\n",
      "Train: Loss: 0.944 | Acc: 66.643% (2815/4224)\n",
      "Train: Loss: 0.946 | Acc: 66.475% (2893/4352)\n",
      "Train: Loss: 0.947 | Acc: 66.540% (2981/4480)\n",
      "Train: Loss: 0.954 | Acc: 66.363% (3058/4608)\n",
      "Train: Loss: 0.957 | Acc: 66.237% (3137/4736)\n",
      "Train: Loss: 0.956 | Acc: 66.283% (3224/4864)\n",
      "Train: Loss: 0.954 | Acc: 66.206% (3305/4992)\n",
      "Train: Loss: 0.955 | Acc: 66.172% (3388/5120)\n",
      "Train: Loss: 0.955 | Acc: 66.178% (3473/5248)\n",
      "Train: Loss: 0.956 | Acc: 66.295% (3564/5376)\n",
      "Train: Loss: 0.961 | Acc: 66.025% (3634/5504)\n",
      "Train: Loss: 0.962 | Acc: 65.927% (3713/5632)\n",
      "Train: Loss: 0.962 | Acc: 65.816% (3791/5760)\n",
      "Train: Loss: 0.960 | Acc: 65.914% (3881/5888)\n",
      "Train: Loss: 0.961 | Acc: 65.957% (3968/6016)\n",
      "Train: Loss: 0.959 | Acc: 66.097% (4061/6144)\n",
      "Train: Loss: 0.957 | Acc: 66.040% (4142/6272)\n",
      "Train: Loss: 0.958 | Acc: 66.078% (4229/6400)\n",
      "Train: Loss: 0.962 | Acc: 65.916% (4303/6528)\n",
      "Train: Loss: 0.959 | Acc: 66.076% (4398/6656)\n",
      "Train: Loss: 0.956 | Acc: 66.126% (4486/6784)\n",
      "Train: Loss: 0.957 | Acc: 66.088% (4568/6912)\n",
      "Train: Loss: 0.956 | Acc: 66.051% (4650/7040)\n",
      "Train: Loss: 0.958 | Acc: 65.932% (4726/7168)\n",
      "Train: Loss: 0.961 | Acc: 65.803% (4801/7296)\n",
      "Train: Loss: 0.961 | Acc: 65.841% (4888/7424)\n",
      "Train: Loss: 0.962 | Acc: 65.837% (4972/7552)\n",
      "Train: Loss: 0.962 | Acc: 65.768% (5051/7680)\n",
      "Train: Loss: 0.962 | Acc: 65.791% (5137/7808)\n",
      "Train: Loss: 0.962 | Acc: 65.814% (5223/7936)\n",
      "Train: Loss: 0.958 | Acc: 65.898% (5314/8064)\n",
      "Train: Loss: 0.957 | Acc: 65.991% (5406/8192)\n",
      "Train: Loss: 0.954 | Acc: 66.034% (5494/8320)\n",
      "Train: Loss: 0.952 | Acc: 66.087% (5583/8448)\n",
      "Train: Loss: 0.952 | Acc: 66.138% (5672/8576)\n",
      "Train: Loss: 0.953 | Acc: 66.131% (5756/8704)\n",
      "Train: Loss: 0.953 | Acc: 66.146% (5842/8832)\n",
      "Train: Loss: 0.953 | Acc: 66.116% (5924/8960)\n",
      "Train: Loss: 0.953 | Acc: 66.087% (6006/9088)\n",
      "Train: Loss: 0.954 | Acc: 66.037% (6086/9216)\n",
      "Train: Loss: 0.956 | Acc: 65.989% (6166/9344)\n",
      "Train: Loss: 0.955 | Acc: 65.995% (6251/9472)\n",
      "Train: Loss: 0.955 | Acc: 65.969% (6333/9600)\n",
      "Train: Loss: 0.955 | Acc: 65.954% (6416/9728)\n",
      "Train: Loss: 0.955 | Acc: 65.950% (6500/9856)\n",
      "Train: Loss: 0.953 | Acc: 66.026% (6592/9984)\n",
      "Train: Loss: 0.952 | Acc: 66.060% (6680/10112)\n",
      "Train: Loss: 0.951 | Acc: 66.172% (6776/10240)\n",
      "Train: Loss: 0.950 | Acc: 66.175% (6861/10368)\n",
      "Train: Loss: 0.950 | Acc: 66.159% (6944/10496)\n",
      "Train: Loss: 0.949 | Acc: 66.171% (7030/10624)\n",
      "Train: Loss: 0.948 | Acc: 66.164% (7114/10752)\n",
      "Train: Loss: 0.948 | Acc: 66.186% (7201/10880)\n",
      "Train: Loss: 0.947 | Acc: 66.225% (7290/11008)\n",
      "Train: Loss: 0.947 | Acc: 66.263% (7379/11136)\n",
      "Train: Loss: 0.946 | Acc: 66.317% (7470/11264)\n",
      "Train: Loss: 0.946 | Acc: 66.257% (7548/11392)\n",
      "Train: Loss: 0.945 | Acc: 66.311% (7639/11520)\n",
      "Train: Loss: 0.944 | Acc: 66.380% (7732/11648)\n",
      "Train: Loss: 0.945 | Acc: 66.381% (7817/11776)\n",
      "Train: Loss: 0.944 | Acc: 66.490% (7915/11904)\n",
      "Train: Loss: 0.943 | Acc: 66.539% (8006/12032)\n",
      "Train: Loss: 0.942 | Acc: 66.604% (8099/12160)\n",
      "Train: Loss: 0.943 | Acc: 66.561% (8179/12288)\n",
      "Train: Loss: 0.944 | Acc: 66.519% (8259/12416)\n",
      "Train: Loss: 0.947 | Acc: 66.446% (8335/12544)\n",
      "Train: Loss: 0.947 | Acc: 66.422% (8417/12672)\n",
      "Train: Loss: 0.947 | Acc: 66.414% (8501/12800)\n",
      "Train: Loss: 0.947 | Acc: 66.445% (8590/12928)\n",
      "Train: Loss: 0.945 | Acc: 66.445% (8675/13056)\n",
      "Train: Loss: 0.944 | Acc: 66.512% (8769/13184)\n",
      "Train: Loss: 0.944 | Acc: 66.534% (8857/13312)\n",
      "Train: Loss: 0.942 | Acc: 66.615% (8953/13440)\n",
      "Train: Loss: 0.942 | Acc: 66.657% (9044/13568)\n",
      "Train: Loss: 0.943 | Acc: 66.625% (9125/13696)\n",
      "Train: Loss: 0.943 | Acc: 66.616% (9209/13824)\n",
      "Train: Loss: 0.944 | Acc: 66.614% (9294/13952)\n",
      "Train: Loss: 0.943 | Acc: 66.676% (9388/14080)\n",
      "Train: Loss: 0.942 | Acc: 66.695% (9476/14208)\n",
      "Train: Loss: 0.941 | Acc: 66.713% (9564/14336)\n",
      "Train: Loss: 0.943 | Acc: 66.669% (9643/14464)\n",
      "Train: Loss: 0.942 | Acc: 66.680% (9730/14592)\n",
      "Train: Loss: 0.941 | Acc: 66.712% (9820/14720)\n",
      "Train: Loss: 0.942 | Acc: 66.716% (9906/14848)\n",
      "Train: Loss: 0.943 | Acc: 66.693% (9988/14976)\n",
      "Train: Loss: 0.942 | Acc: 66.731% (10079/15104)\n",
      "Train: Loss: 0.942 | Acc: 66.682% (10157/15232)\n",
      "Train: Loss: 0.943 | Acc: 66.667% (10240/15360)\n",
      "Train: Loss: 0.942 | Acc: 66.729% (10335/15488)\n",
      "Train: Loss: 0.942 | Acc: 66.733% (10421/15616)\n",
      "Train: Loss: 0.943 | Acc: 66.724% (10505/15744)\n",
      "Train: Loss: 0.944 | Acc: 66.671% (10582/15872)\n",
      "Train: Loss: 0.945 | Acc: 66.669% (10667/16000)\n",
      "Train: Loss: 0.945 | Acc: 66.654% (10750/16128)\n",
      "Train: Loss: 0.945 | Acc: 66.658% (10836/16256)\n",
      "Train: Loss: 0.944 | Acc: 66.681% (10925/16384)\n",
      "Train: Loss: 0.944 | Acc: 66.667% (11008/16512)\n",
      "Train: Loss: 0.943 | Acc: 66.707% (11100/16640)\n",
      "Train: Loss: 0.942 | Acc: 66.728% (11189/16768)\n",
      "Train: Loss: 0.942 | Acc: 66.684% (11267/16896)\n",
      "Train: Loss: 0.941 | Acc: 66.747% (11363/17024)\n",
      "Train: Loss: 0.941 | Acc: 66.721% (11444/17152)\n",
      "Train: Loss: 0.940 | Acc: 66.730% (11531/17280)\n",
      "Train: Loss: 0.939 | Acc: 66.774% (11624/17408)\n",
      "Train: Loss: 0.938 | Acc: 66.805% (11715/17536)\n",
      "Train: Loss: 0.938 | Acc: 66.797% (11799/17664)\n",
      "Train: Loss: 0.939 | Acc: 66.755% (11877/17792)\n",
      "Train: Loss: 0.939 | Acc: 66.763% (11964/17920)\n",
      "Train: Loss: 0.937 | Acc: 66.800% (12056/18048)\n",
      "Train: Loss: 0.937 | Acc: 66.841% (12149/18176)\n",
      "Train: Loss: 0.937 | Acc: 66.865% (12239/18304)\n",
      "Train: Loss: 0.937 | Acc: 66.862% (12324/18432)\n",
      "Train: Loss: 0.937 | Acc: 66.853% (12408/18560)\n",
      "Train: Loss: 0.939 | Acc: 66.818% (12487/18688)\n",
      "Train: Loss: 0.939 | Acc: 66.810% (12571/18816)\n",
      "Train: Loss: 0.939 | Acc: 66.797% (12654/18944)\n",
      "Train: Loss: 0.940 | Acc: 66.747% (12730/19072)\n",
      "Train: Loss: 0.939 | Acc: 66.792% (12824/19200)\n",
      "Train: Loss: 0.939 | Acc: 66.805% (12912/19328)\n",
      "Train: Loss: 0.939 | Acc: 66.797% (12996/19456)\n",
      "Train: Loss: 0.939 | Acc: 66.825% (13087/19584)\n",
      "Train: Loss: 0.940 | Acc: 66.782% (13164/19712)\n",
      "Train: Loss: 0.940 | Acc: 66.754% (13244/19840)\n",
      "Train: Loss: 0.939 | Acc: 66.742% (13327/19968)\n",
      "Train: Loss: 0.939 | Acc: 66.715% (13407/20096)\n",
      "Train: Loss: 0.939 | Acc: 66.752% (13500/20224)\n",
      "Train: Loss: 0.939 | Acc: 66.711% (13577/20352)\n",
      "Train: Loss: 0.940 | Acc: 66.704% (13661/20480)\n",
      "Train: Loss: 0.940 | Acc: 66.688% (13743/20608)\n",
      "Train: Loss: 0.940 | Acc: 66.705% (13832/20736)\n",
      "Train: Loss: 0.940 | Acc: 66.751% (13927/20864)\n",
      "Train: Loss: 0.940 | Acc: 66.773% (14017/20992)\n",
      "Train: Loss: 0.940 | Acc: 66.742% (14096/21120)\n",
      "Train: Loss: 0.941 | Acc: 66.726% (14178/21248)\n",
      "Train: Loss: 0.941 | Acc: 66.738% (14266/21376)\n",
      "Train: Loss: 0.941 | Acc: 66.736% (14351/21504)\n",
      "Train: Loss: 0.940 | Acc: 66.776% (14445/21632)\n",
      "Train: Loss: 0.940 | Acc: 66.778% (14531/21760)\n",
      "Train: Loss: 0.939 | Acc: 66.859% (14634/21888)\n",
      "Train: Loss: 0.938 | Acc: 66.874% (14723/22016)\n",
      "Train: Loss: 0.937 | Acc: 66.885% (14811/22144)\n",
      "Train: Loss: 0.937 | Acc: 66.900% (14900/22272)\n",
      "Train: Loss: 0.937 | Acc: 66.920% (14990/22400)\n",
      "Train: Loss: 0.936 | Acc: 66.917% (15075/22528)\n",
      "Train: Loss: 0.936 | Acc: 66.927% (15163/22656)\n",
      "Train: Loss: 0.936 | Acc: 66.946% (15253/22784)\n",
      "Train: Loss: 0.935 | Acc: 66.978% (15346/22912)\n",
      "Train: Loss: 0.933 | Acc: 67.018% (15441/23040)\n",
      "Train: Loss: 0.933 | Acc: 67.015% (15526/23168)\n",
      "Train: Loss: 0.932 | Acc: 67.046% (15619/23296)\n",
      "Train: Loss: 0.931 | Acc: 67.072% (15711/23424)\n",
      "Train: Loss: 0.931 | Acc: 67.098% (15803/23552)\n",
      "Train: Loss: 0.931 | Acc: 67.103% (15890/23680)\n",
      "Train: Loss: 0.931 | Acc: 67.108% (15977/23808)\n",
      "Train: Loss: 0.930 | Acc: 67.142% (16071/23936)\n",
      "Train: Loss: 0.928 | Acc: 67.196% (16170/24064)\n",
      "Train: Loss: 0.928 | Acc: 67.179% (16252/24192)\n",
      "Train: Loss: 0.928 | Acc: 67.175% (16337/24320)\n",
      "Train: Loss: 0.928 | Acc: 67.163% (16420/24448)\n",
      "Train: Loss: 0.928 | Acc: 67.175% (16509/24576)\n",
      "Train: Loss: 0.928 | Acc: 67.175% (16595/24704)\n",
      "Train: Loss: 0.929 | Acc: 67.119% (16667/24832)\n",
      "Train: Loss: 0.930 | Acc: 67.075% (16742/24960)\n",
      "Train: Loss: 0.929 | Acc: 67.084% (16830/25088)\n",
      "Train: Loss: 0.929 | Acc: 67.112% (16923/25216)\n",
      "Train: Loss: 0.928 | Acc: 67.128% (17013/25344)\n",
      "Train: Loss: 0.929 | Acc: 67.105% (17093/25472)\n",
      "Train: Loss: 0.928 | Acc: 67.125% (17184/25600)\n",
      "Train: Loss: 0.928 | Acc: 67.114% (17267/25728)\n",
      "Train: Loss: 0.928 | Acc: 67.129% (17357/25856)\n",
      "Train: Loss: 0.928 | Acc: 67.130% (17443/25984)\n",
      "Train: Loss: 0.928 | Acc: 67.130% (17529/26112)\n",
      "Train: Loss: 0.927 | Acc: 67.176% (17627/26240)\n",
      "Train: Loss: 0.928 | Acc: 67.184% (17715/26368)\n",
      "Train: Loss: 0.928 | Acc: 67.169% (17797/26496)\n",
      "Train: Loss: 0.928 | Acc: 67.188% (17888/26624)\n",
      "Train: Loss: 0.927 | Acc: 67.217% (17982/26752)\n",
      "Train: Loss: 0.927 | Acc: 67.176% (18057/26880)\n",
      "Train: Loss: 0.927 | Acc: 67.176% (18143/27008)\n",
      "Train: Loss: 0.927 | Acc: 67.180% (18230/27136)\n",
      "Train: Loss: 0.927 | Acc: 67.199% (18321/27264)\n",
      "Train: Loss: 0.926 | Acc: 67.231% (18416/27392)\n",
      "Train: Loss: 0.926 | Acc: 67.264% (18511/27520)\n",
      "Train: Loss: 0.925 | Acc: 67.263% (18597/27648)\n",
      "Train: Loss: 0.925 | Acc: 67.288% (18690/27776)\n",
      "Train: Loss: 0.924 | Acc: 67.299% (18779/27904)\n",
      "Train: Loss: 0.924 | Acc: 67.323% (18872/28032)\n",
      "Train: Loss: 0.924 | Acc: 67.337% (18962/28160)\n",
      "Train: Loss: 0.924 | Acc: 67.329% (19046/28288)\n",
      "Train: Loss: 0.923 | Acc: 67.353% (19139/28416)\n",
      "Train: Loss: 0.923 | Acc: 67.366% (19229/28544)\n",
      "Train: Loss: 0.923 | Acc: 67.383% (19320/28672)\n",
      "Train: Loss: 0.922 | Acc: 67.420% (19417/28800)\n",
      "Train: Loss: 0.922 | Acc: 67.429% (19506/28928)\n",
      "Train: Loss: 0.921 | Acc: 67.411% (19587/29056)\n",
      "Train: Loss: 0.921 | Acc: 67.431% (19679/29184)\n",
      "Train: Loss: 0.921 | Acc: 67.430% (19765/29312)\n",
      "Train: Loss: 0.922 | Acc: 67.405% (19844/29440)\n",
      "Train: Loss: 0.922 | Acc: 67.401% (19929/29568)\n",
      "Train: Loss: 0.922 | Acc: 67.406% (20017/29696)\n",
      "Train: Loss: 0.921 | Acc: 67.426% (20109/29824)\n",
      "Train: Loss: 0.921 | Acc: 67.445% (20201/29952)\n",
      "Train: Loss: 0.920 | Acc: 67.467% (20294/30080)\n",
      "Train: Loss: 0.920 | Acc: 67.495% (20389/30208)\n",
      "Train: Loss: 0.920 | Acc: 67.484% (20472/30336)\n",
      "Train: Loss: 0.920 | Acc: 67.486% (20559/30464)\n",
      "Train: Loss: 0.920 | Acc: 67.495% (20648/30592)\n",
      "Train: Loss: 0.920 | Acc: 67.510% (20739/30720)\n",
      "Train: Loss: 0.919 | Acc: 67.499% (20822/30848)\n",
      "Train: Loss: 0.920 | Acc: 67.481% (20903/30976)\n",
      "Train: Loss: 0.920 | Acc: 67.461% (20983/31104)\n",
      "Train: Loss: 0.921 | Acc: 67.450% (21066/31232)\n",
      "Train: Loss: 0.920 | Acc: 67.459% (21155/31360)\n",
      "Train: Loss: 0.920 | Acc: 67.476% (21247/31488)\n",
      "Train: Loss: 0.920 | Acc: 67.494% (21339/31616)\n",
      "Train: Loss: 0.920 | Acc: 67.496% (21426/31744)\n",
      "Train: Loss: 0.919 | Acc: 67.508% (21516/31872)\n",
      "Train: Loss: 0.919 | Acc: 67.500% (21600/32000)\n",
      "Train: Loss: 0.919 | Acc: 67.521% (21693/32128)\n",
      "Train: Loss: 0.919 | Acc: 67.535% (21784/32256)\n",
      "Train: Loss: 0.919 | Acc: 67.533% (21870/32384)\n",
      "Train: Loss: 0.919 | Acc: 67.547% (21961/32512)\n",
      "Train: Loss: 0.919 | Acc: 67.546% (22047/32640)\n",
      "Train: Loss: 0.919 | Acc: 67.563% (22139/32768)\n",
      "Train: Loss: 0.919 | Acc: 67.577% (22230/32896)\n",
      "Train: Loss: 0.919 | Acc: 67.584% (22319/33024)\n",
      "Train: Loss: 0.919 | Acc: 67.568% (22400/33152)\n",
      "Train: Loss: 0.919 | Acc: 67.560% (22484/33280)\n",
      "Train: Loss: 0.919 | Acc: 67.562% (22571/33408)\n",
      "Train: Loss: 0.918 | Acc: 67.593% (22668/33536)\n",
      "Train: Loss: 0.918 | Acc: 67.618% (22763/33664)\n",
      "Train: Loss: 0.918 | Acc: 67.608% (22846/33792)\n",
      "Train: Loss: 0.918 | Acc: 67.624% (22938/33920)\n",
      "Train: Loss: 0.917 | Acc: 67.625% (23025/34048)\n",
      "Train: Loss: 0.918 | Acc: 67.612% (23107/34176)\n",
      "Train: Loss: 0.918 | Acc: 67.616% (23195/34304)\n",
      "Train: Loss: 0.918 | Acc: 67.635% (23288/34432)\n",
      "Train: Loss: 0.917 | Acc: 67.656% (23382/34560)\n",
      "Train: Loss: 0.917 | Acc: 67.663% (23471/34688)\n",
      "Train: Loss: 0.916 | Acc: 67.673% (23561/34816)\n",
      "Train: Loss: 0.916 | Acc: 67.663% (23644/34944)\n",
      "Train: Loss: 0.917 | Acc: 67.649% (23726/35072)\n",
      "Train: Loss: 0.917 | Acc: 67.625% (23804/35200)\n",
      "Train: Loss: 0.917 | Acc: 67.638% (23895/35328)\n",
      "Train: Loss: 0.917 | Acc: 67.630% (23979/35456)\n",
      "Train: Loss: 0.916 | Acc: 67.640% (24069/35584)\n",
      "Train: Loss: 0.916 | Acc: 67.650% (24159/35712)\n",
      "Train: Loss: 0.916 | Acc: 67.665% (24251/35840)\n",
      "Train: Loss: 0.916 | Acc: 67.666% (24338/35968)\n",
      "Train: Loss: 0.916 | Acc: 67.658% (24422/36096)\n",
      "Train: Loss: 0.915 | Acc: 67.665% (24511/36224)\n",
      "Train: Loss: 0.916 | Acc: 67.658% (24595/36352)\n",
      "Train: Loss: 0.916 | Acc: 67.664% (24684/36480)\n",
      "Train: Loss: 0.915 | Acc: 67.693% (24781/36608)\n",
      "Train: Loss: 0.915 | Acc: 67.707% (24873/36736)\n",
      "Train: Loss: 0.915 | Acc: 67.727% (24967/36864)\n",
      "Train: Loss: 0.915 | Acc: 67.709% (25047/36992)\n",
      "Train: Loss: 0.914 | Acc: 67.742% (25146/37120)\n",
      "Train: Loss: 0.914 | Acc: 67.722% (25225/37248)\n",
      "Train: Loss: 0.915 | Acc: 67.715% (25309/37376)\n",
      "Train: Loss: 0.915 | Acc: 67.723% (25399/37504)\n",
      "Train: Loss: 0.915 | Acc: 67.730% (25488/37632)\n",
      "Train: Loss: 0.915 | Acc: 67.728% (25574/37760)\n",
      "Train: Loss: 0.915 | Acc: 67.736% (25664/37888)\n",
      "Train: Loss: 0.914 | Acc: 67.743% (25753/38016)\n",
      "Train: Loss: 0.914 | Acc: 67.756% (25845/38144)\n",
      "Train: Loss: 0.914 | Acc: 67.765% (25935/38272)\n",
      "Train: Loss: 0.913 | Acc: 67.779% (26027/38400)\n",
      "Train: Loss: 0.913 | Acc: 67.779% (26114/38528)\n",
      "Train: Loss: 0.913 | Acc: 67.795% (26207/38656)\n",
      "Train: Loss: 0.913 | Acc: 67.806% (26298/38784)\n",
      "Train: Loss: 0.912 | Acc: 67.822% (26391/38912)\n",
      "Train: Loss: 0.912 | Acc: 67.828% (26480/39040)\n",
      "Train: Loss: 0.912 | Acc: 67.836% (26570/39168)\n",
      "Train: Loss: 0.912 | Acc: 67.824% (26652/39296)\n",
      "Train: Loss: 0.912 | Acc: 67.804% (26731/39424)\n",
      "Train: Loss: 0.912 | Acc: 67.822% (26825/39552)\n",
      "Train: Loss: 0.912 | Acc: 67.800% (26903/39680)\n",
      "Train: Loss: 0.911 | Acc: 67.810% (26994/39808)\n",
      "Train: Loss: 0.911 | Acc: 67.798% (27076/39936)\n",
      "Train: Loss: 0.912 | Acc: 67.779% (27155/40064)\n",
      "Train: Loss: 0.912 | Acc: 67.780% (27242/40192)\n",
      "Train: Loss: 0.911 | Acc: 67.803% (27338/40320)\n",
      "Train: Loss: 0.910 | Acc: 67.833% (27437/40448)\n",
      "Train: Loss: 0.910 | Acc: 67.850% (27531/40576)\n",
      "Train: Loss: 0.909 | Acc: 67.848% (27617/40704)\n",
      "Train: Loss: 0.909 | Acc: 67.856% (27707/40832)\n",
      "Train: Loss: 0.909 | Acc: 67.874% (27801/40960)\n",
      "Train: Loss: 0.909 | Acc: 67.876% (27889/41088)\n",
      "Train: Loss: 0.909 | Acc: 67.898% (27985/41216)\n",
      "Train: Loss: 0.909 | Acc: 67.894% (28070/41344)\n",
      "Train: Loss: 0.908 | Acc: 67.906% (28162/41472)\n",
      "Train: Loss: 0.908 | Acc: 67.921% (28255/41600)\n",
      "Train: Loss: 0.907 | Acc: 67.933% (28347/41728)\n",
      "Train: Loss: 0.907 | Acc: 67.942% (28438/41856)\n",
      "Train: Loss: 0.907 | Acc: 67.959% (28532/41984)\n",
      "Train: Loss: 0.907 | Acc: 67.945% (28613/42112)\n",
      "Train: Loss: 0.907 | Acc: 67.955% (28704/42240)\n",
      "Train: Loss: 0.906 | Acc: 67.964% (28795/42368)\n",
      "Train: Loss: 0.907 | Acc: 67.952% (28877/42496)\n",
      "Train: Loss: 0.907 | Acc: 67.948% (28962/42624)\n",
      "Train: Loss: 0.907 | Acc: 67.962% (29055/42752)\n",
      "Train: Loss: 0.907 | Acc: 67.966% (29144/42880)\n",
      "Train: Loss: 0.906 | Acc: 67.985% (29239/43008)\n",
      "Train: Loss: 0.906 | Acc: 68.006% (29335/43136)\n",
      "Train: Loss: 0.905 | Acc: 68.038% (29436/43264)\n",
      "Train: Loss: 0.906 | Acc: 68.038% (29523/43392)\n",
      "Train: Loss: 0.905 | Acc: 68.047% (29614/43520)\n",
      "Train: Loss: 0.905 | Acc: 68.051% (29703/43648)\n",
      "Train: Loss: 0.905 | Acc: 68.056% (29792/43776)\n",
      "Train: Loss: 0.905 | Acc: 68.067% (29884/43904)\n",
      "Train: Loss: 0.904 | Acc: 68.080% (29977/44032)\n",
      "Train: Loss: 0.905 | Acc: 68.059% (30055/44160)\n",
      "Train: Loss: 0.905 | Acc: 68.061% (30143/44288)\n",
      "Train: Loss: 0.904 | Acc: 68.084% (30240/44416)\n",
      "Train: Loss: 0.904 | Acc: 68.106% (30337/44544)\n",
      "Train: Loss: 0.903 | Acc: 68.123% (30432/44672)\n",
      "Train: Loss: 0.904 | Acc: 68.109% (30513/44800)\n",
      "Train: Loss: 0.904 | Acc: 68.109% (30600/44928)\n",
      "Train: Loss: 0.903 | Acc: 68.117% (30691/45056)\n",
      "Train: Loss: 0.903 | Acc: 68.137% (30787/45184)\n",
      "Train: Loss: 0.903 | Acc: 68.139% (30875/45312)\n",
      "Train: Loss: 0.902 | Acc: 68.162% (30973/45440)\n",
      "Train: Loss: 0.902 | Acc: 68.157% (31058/45568)\n",
      "Train: Loss: 0.903 | Acc: 68.150% (31142/45696)\n",
      "Train: Loss: 0.902 | Acc: 68.167% (31237/45824)\n",
      "Train: Loss: 0.903 | Acc: 68.158% (31320/45952)\n",
      "Train: Loss: 0.902 | Acc: 68.188% (31421/46080)\n",
      "Train: Loss: 0.902 | Acc: 68.205% (31516/46208)\n",
      "Train: Loss: 0.902 | Acc: 68.200% (31601/46336)\n",
      "Train: Loss: 0.902 | Acc: 68.210% (31693/46464)\n",
      "Train: Loss: 0.901 | Acc: 68.228% (31789/46592)\n",
      "Train: Loss: 0.901 | Acc: 68.253% (31888/46720)\n",
      "Train: Loss: 0.900 | Acc: 68.276% (31986/46848)\n",
      "Train: Loss: 0.899 | Acc: 68.299% (32084/46976)\n",
      "Train: Loss: 0.899 | Acc: 68.315% (32179/47104)\n",
      "Train: Loss: 0.899 | Acc: 68.312% (32265/47232)\n",
      "Train: Loss: 0.898 | Acc: 68.330% (32361/47360)\n",
      "Train: Loss: 0.898 | Acc: 68.358% (32462/47488)\n",
      "Train: Loss: 0.897 | Acc: 68.374% (32557/47616)\n",
      "Train: Loss: 0.897 | Acc: 68.373% (32644/47744)\n",
      "Train: Loss: 0.897 | Acc: 68.366% (32728/47872)\n",
      "Train: Loss: 0.898 | Acc: 68.367% (32816/48000)\n",
      "Train: Loss: 0.898 | Acc: 68.384% (32912/48128)\n",
      "Train: Loss: 0.898 | Acc: 68.383% (32999/48256)\n",
      "Train: Loss: 0.898 | Acc: 68.372% (33081/48384)\n",
      "Train: Loss: 0.897 | Acc: 68.375% (33170/48512)\n",
      "Train: Loss: 0.897 | Acc: 68.376% (33258/48640)\n",
      "Train: Loss: 0.897 | Acc: 68.369% (33342/48768)\n",
      "Train: Loss: 0.897 | Acc: 68.368% (33429/48896)\n",
      "Train: Loss: 0.897 | Acc: 68.364% (33515/49024)\n",
      "Train: Loss: 0.897 | Acc: 68.361% (33601/49152)\n",
      "Train: Loss: 0.897 | Acc: 68.354% (33685/49280)\n",
      "Train: Loss: 0.896 | Acc: 68.369% (33780/49408)\n",
      "Train: Loss: 0.896 | Acc: 68.379% (33872/49536)\n",
      "Train: Loss: 0.896 | Acc: 68.396% (33968/49664)\n",
      "Train: Loss: 0.896 | Acc: 68.386% (34051/49792)\n",
      "Train: Loss: 0.896 | Acc: 68.371% (34131/49920)\n",
      "Train: Loss: 0.896 | Acc: 68.362% (34181/50000)\n",
      "Test: Loss: 0.847 | Acc: 70.000% (70/100)\n",
      "Test: Loss: 0.894 | Acc: 68.000% (136/200)\n",
      "Test: Loss: 0.925 | Acc: 66.333% (199/300)\n",
      "Test: Loss: 0.933 | Acc: 66.250% (265/400)\n",
      "Test: Loss: 0.933 | Acc: 67.400% (337/500)\n",
      "Test: Loss: 0.904 | Acc: 68.333% (410/600)\n",
      "Test: Loss: 0.894 | Acc: 68.714% (481/700)\n",
      "Test: Loss: 0.908 | Acc: 68.125% (545/800)\n",
      "Test: Loss: 0.913 | Acc: 68.556% (617/900)\n",
      "Test: Loss: 0.906 | Acc: 68.400% (684/1000)\n",
      "Test: Loss: 0.895 | Acc: 68.455% (753/1100)\n",
      "Test: Loss: 0.896 | Acc: 68.500% (822/1200)\n",
      "Test: Loss: 0.895 | Acc: 68.462% (890/1300)\n",
      "Test: Loss: 0.891 | Acc: 68.143% (954/1400)\n",
      "Test: Loss: 0.885 | Acc: 68.000% (1020/1500)\n",
      "Test: Loss: 0.886 | Acc: 68.062% (1089/1600)\n",
      "Test: Loss: 0.886 | Acc: 68.353% (1162/1700)\n",
      "Test: Loss: 0.896 | Acc: 68.000% (1224/1800)\n",
      "Test: Loss: 0.895 | Acc: 68.368% (1299/1900)\n",
      "Test: Loss: 0.895 | Acc: 68.350% (1367/2000)\n",
      "Test: Loss: 0.892 | Acc: 68.190% (1432/2100)\n",
      "Test: Loss: 0.896 | Acc: 68.091% (1498/2200)\n",
      "Test: Loss: 0.898 | Acc: 68.000% (1564/2300)\n",
      "Test: Loss: 0.898 | Acc: 68.042% (1633/2400)\n",
      "Test: Loss: 0.906 | Acc: 67.840% (1696/2500)\n",
      "Test: Loss: 0.914 | Acc: 67.654% (1759/2600)\n",
      "Test: Loss: 0.909 | Acc: 67.704% (1828/2700)\n",
      "Test: Loss: 0.912 | Acc: 67.607% (1893/2800)\n",
      "Test: Loss: 0.910 | Acc: 67.690% (1963/2900)\n",
      "Test: Loss: 0.905 | Acc: 67.967% (2039/3000)\n",
      "Test: Loss: 0.901 | Acc: 68.000% (2108/3100)\n",
      "Test: Loss: 0.903 | Acc: 67.938% (2174/3200)\n",
      "Test: Loss: 0.902 | Acc: 67.879% (2240/3300)\n",
      "Test: Loss: 0.907 | Acc: 67.676% (2301/3400)\n",
      "Test: Loss: 0.910 | Acc: 67.571% (2365/3500)\n",
      "Test: Loss: 0.909 | Acc: 67.722% (2438/3600)\n",
      "Test: Loss: 0.910 | Acc: 67.568% (2500/3700)\n",
      "Test: Loss: 0.913 | Acc: 67.342% (2559/3800)\n",
      "Test: Loss: 0.909 | Acc: 67.615% (2637/3900)\n",
      "Test: Loss: 0.910 | Acc: 67.600% (2704/4000)\n",
      "Test: Loss: 0.909 | Acc: 67.659% (2774/4100)\n",
      "Test: Loss: 0.910 | Acc: 67.595% (2839/4200)\n",
      "Test: Loss: 0.907 | Acc: 67.744% (2913/4300)\n",
      "Test: Loss: 0.906 | Acc: 67.773% (2982/4400)\n",
      "Test: Loss: 0.909 | Acc: 67.733% (3048/4500)\n",
      "Test: Loss: 0.908 | Acc: 67.804% (3119/4600)\n",
      "Test: Loss: 0.906 | Acc: 67.979% (3195/4700)\n",
      "Test: Loss: 0.905 | Acc: 67.979% (3263/4800)\n",
      "Test: Loss: 0.902 | Acc: 68.143% (3339/4900)\n",
      "Test: Loss: 0.904 | Acc: 68.120% (3406/5000)\n",
      "Test: Loss: 0.903 | Acc: 68.137% (3475/5100)\n",
      "Test: Loss: 0.903 | Acc: 68.077% (3540/5200)\n",
      "Test: Loss: 0.900 | Acc: 68.094% (3609/5300)\n",
      "Test: Loss: 0.902 | Acc: 67.963% (3670/5400)\n",
      "Test: Loss: 0.901 | Acc: 67.945% (3737/5500)\n",
      "Test: Loss: 0.903 | Acc: 67.875% (3801/5600)\n",
      "Test: Loss: 0.904 | Acc: 67.912% (3871/5700)\n",
      "Test: Loss: 0.901 | Acc: 67.931% (3940/5800)\n",
      "Test: Loss: 0.904 | Acc: 67.864% (4004/5900)\n",
      "Test: Loss: 0.904 | Acc: 67.950% (4077/6000)\n",
      "Test: Loss: 0.904 | Acc: 67.852% (4139/6100)\n",
      "Test: Loss: 0.903 | Acc: 67.855% (4207/6200)\n",
      "Test: Loss: 0.902 | Acc: 67.952% (4281/6300)\n",
      "Test: Loss: 0.900 | Acc: 68.062% (4356/6400)\n",
      "Test: Loss: 0.900 | Acc: 68.092% (4426/6500)\n",
      "Test: Loss: 0.900 | Acc: 68.182% (4500/6600)\n",
      "Test: Loss: 0.902 | Acc: 68.119% (4564/6700)\n",
      "Test: Loss: 0.901 | Acc: 68.147% (4634/6800)\n",
      "Test: Loss: 0.904 | Acc: 68.072% (4697/6900)\n",
      "Test: Loss: 0.906 | Acc: 67.971% (4758/7000)\n",
      "Test: Loss: 0.905 | Acc: 67.986% (4827/7100)\n",
      "Test: Loss: 0.906 | Acc: 67.931% (4891/7200)\n",
      "Test: Loss: 0.904 | Acc: 67.918% (4958/7300)\n",
      "Test: Loss: 0.901 | Acc: 68.027% (5034/7400)\n",
      "Test: Loss: 0.903 | Acc: 68.040% (5103/7500)\n",
      "Test: Loss: 0.901 | Acc: 68.105% (5176/7600)\n",
      "Test: Loss: 0.902 | Acc: 68.104% (5244/7700)\n",
      "Test: Loss: 0.901 | Acc: 68.154% (5316/7800)\n",
      "Test: Loss: 0.904 | Acc: 68.051% (5376/7900)\n",
      "Test: Loss: 0.903 | Acc: 68.013% (5441/8000)\n",
      "Test: Loss: 0.902 | Acc: 68.025% (5510/8100)\n",
      "Test: Loss: 0.902 | Acc: 68.024% (5578/8200)\n",
      "Test: Loss: 0.900 | Acc: 68.084% (5651/8300)\n",
      "Test: Loss: 0.902 | Acc: 67.988% (5711/8400)\n",
      "Test: Loss: 0.905 | Acc: 67.859% (5768/8500)\n",
      "Test: Loss: 0.905 | Acc: 67.930% (5842/8600)\n",
      "Test: Loss: 0.905 | Acc: 67.851% (5903/8700)\n",
      "Test: Loss: 0.905 | Acc: 67.818% (5968/8800)\n",
      "Test: Loss: 0.906 | Acc: 67.798% (6034/8900)\n",
      "Test: Loss: 0.907 | Acc: 67.767% (6099/9000)\n",
      "Test: Loss: 0.906 | Acc: 67.758% (6166/9100)\n",
      "Test: Loss: 0.905 | Acc: 67.837% (6241/9200)\n",
      "Test: Loss: 0.906 | Acc: 67.871% (6312/9300)\n",
      "Test: Loss: 0.906 | Acc: 67.862% (6379/9400)\n",
      "Test: Loss: 0.905 | Acc: 67.895% (6450/9500)\n",
      "Test: Loss: 0.904 | Acc: 67.958% (6524/9600)\n",
      "Test: Loss: 0.903 | Acc: 68.010% (6597/9700)\n",
      "Test: Loss: 0.905 | Acc: 67.959% (6660/9800)\n",
      "Test: Loss: 0.906 | Acc: 67.899% (6722/9900)\n",
      "Test: Loss: 0.906 | Acc: 67.910% (6791/10000)\n",
      "Train: Loss: 0.669 | Acc: 77.344% (99/128)\n",
      "Train: Loss: 0.679 | Acc: 75.781% (194/256)\n",
      "Train: Loss: 0.687 | Acc: 76.562% (294/384)\n",
      "Train: Loss: 0.717 | Acc: 74.414% (381/512)\n",
      "Train: Loss: 0.728 | Acc: 73.906% (473/640)\n",
      "Train: Loss: 0.733 | Acc: 73.828% (567/768)\n",
      "Train: Loss: 0.737 | Acc: 74.107% (664/896)\n",
      "Train: Loss: 0.743 | Acc: 74.219% (760/1024)\n",
      "Train: Loss: 0.728 | Acc: 74.826% (862/1152)\n",
      "Train: Loss: 0.717 | Acc: 75.156% (962/1280)\n",
      "Train: Loss: 0.712 | Acc: 75.426% (1062/1408)\n",
      "Train: Loss: 0.711 | Acc: 75.716% (1163/1536)\n",
      "Train: Loss: 0.714 | Acc: 75.661% (1259/1664)\n",
      "Train: Loss: 0.715 | Acc: 75.558% (1354/1792)\n",
      "Train: Loss: 0.717 | Acc: 75.469% (1449/1920)\n",
      "Train: Loss: 0.717 | Acc: 75.244% (1541/2048)\n",
      "Train: Loss: 0.712 | Acc: 75.414% (1641/2176)\n",
      "Train: Loss: 0.719 | Acc: 75.304% (1735/2304)\n",
      "Train: Loss: 0.721 | Acc: 75.493% (1836/2432)\n",
      "Train: Loss: 0.717 | Acc: 75.508% (1933/2560)\n",
      "Train: Loss: 0.720 | Acc: 75.298% (2024/2688)\n",
      "Train: Loss: 0.715 | Acc: 75.284% (2120/2816)\n",
      "Train: Loss: 0.724 | Acc: 75.000% (2208/2944)\n",
      "Train: Loss: 0.726 | Acc: 75.033% (2305/3072)\n",
      "Train: Loss: 0.728 | Acc: 74.906% (2397/3200)\n",
      "Train: Loss: 0.730 | Acc: 74.609% (2483/3328)\n",
      "Train: Loss: 0.731 | Acc: 74.508% (2575/3456)\n",
      "Train: Loss: 0.730 | Acc: 74.498% (2670/3584)\n",
      "Train: Loss: 0.735 | Acc: 74.219% (2755/3712)\n",
      "Train: Loss: 0.736 | Acc: 74.219% (2850/3840)\n",
      "Train: Loss: 0.734 | Acc: 74.244% (2946/3968)\n",
      "Train: Loss: 0.728 | Acc: 74.438% (3049/4096)\n",
      "Train: Loss: 0.728 | Acc: 74.408% (3143/4224)\n",
      "Train: Loss: 0.729 | Acc: 74.334% (3235/4352)\n",
      "Train: Loss: 0.728 | Acc: 74.420% (3334/4480)\n",
      "Train: Loss: 0.730 | Acc: 74.371% (3427/4608)\n",
      "Train: Loss: 0.730 | Acc: 74.367% (3522/4736)\n",
      "Train: Loss: 0.728 | Acc: 74.507% (3624/4864)\n",
      "Train: Loss: 0.729 | Acc: 74.519% (3720/4992)\n",
      "Train: Loss: 0.731 | Acc: 74.531% (3816/5120)\n",
      "Train: Loss: 0.733 | Acc: 74.428% (3906/5248)\n",
      "Train: Loss: 0.733 | Acc: 74.461% (4003/5376)\n",
      "Train: Loss: 0.735 | Acc: 74.382% (4094/5504)\n",
      "Train: Loss: 0.737 | Acc: 74.361% (4188/5632)\n",
      "Train: Loss: 0.738 | Acc: 74.306% (4280/5760)\n",
      "Train: Loss: 0.742 | Acc: 74.151% (4366/5888)\n",
      "Train: Loss: 0.741 | Acc: 74.169% (4462/6016)\n",
      "Train: Loss: 0.742 | Acc: 74.170% (4557/6144)\n",
      "Train: Loss: 0.742 | Acc: 74.171% (4652/6272)\n",
      "Train: Loss: 0.742 | Acc: 74.172% (4747/6400)\n",
      "Train: Loss: 0.743 | Acc: 74.142% (4840/6528)\n",
      "Train: Loss: 0.741 | Acc: 74.084% (4931/6656)\n",
      "Train: Loss: 0.741 | Acc: 74.042% (5023/6784)\n",
      "Train: Loss: 0.741 | Acc: 74.016% (5116/6912)\n",
      "Train: Loss: 0.739 | Acc: 74.034% (5212/7040)\n",
      "Train: Loss: 0.738 | Acc: 74.093% (5311/7168)\n",
      "Train: Loss: 0.739 | Acc: 74.054% (5403/7296)\n",
      "Train: Loss: 0.739 | Acc: 74.111% (5502/7424)\n",
      "Train: Loss: 0.740 | Acc: 73.954% (5585/7552)\n",
      "Train: Loss: 0.736 | Acc: 74.154% (5695/7680)\n",
      "Train: Loss: 0.737 | Acc: 74.129% (5788/7808)\n",
      "Train: Loss: 0.736 | Acc: 74.143% (5884/7936)\n",
      "Train: Loss: 0.737 | Acc: 74.157% (5980/8064)\n",
      "Train: Loss: 0.738 | Acc: 74.109% (6071/8192)\n",
      "Train: Loss: 0.740 | Acc: 74.026% (6159/8320)\n",
      "Train: Loss: 0.740 | Acc: 73.946% (6247/8448)\n",
      "Train: Loss: 0.739 | Acc: 73.974% (6344/8576)\n",
      "Train: Loss: 0.740 | Acc: 73.909% (6433/8704)\n",
      "Train: Loss: 0.742 | Acc: 73.879% (6525/8832)\n",
      "Train: Loss: 0.741 | Acc: 73.884% (6620/8960)\n",
      "Train: Loss: 0.743 | Acc: 73.768% (6704/9088)\n",
      "Train: Loss: 0.742 | Acc: 73.796% (6801/9216)\n",
      "Train: Loss: 0.743 | Acc: 73.726% (6889/9344)\n",
      "Train: Loss: 0.743 | Acc: 73.701% (6981/9472)\n",
      "Train: Loss: 0.745 | Acc: 73.677% (7073/9600)\n",
      "Train: Loss: 0.744 | Acc: 73.777% (7177/9728)\n",
      "Train: Loss: 0.742 | Acc: 73.782% (7272/9856)\n",
      "Train: Loss: 0.745 | Acc: 73.688% (7357/9984)\n",
      "Train: Loss: 0.745 | Acc: 73.645% (7447/10112)\n",
      "Train: Loss: 0.746 | Acc: 73.604% (7537/10240)\n",
      "Train: Loss: 0.744 | Acc: 73.746% (7646/10368)\n",
      "Train: Loss: 0.744 | Acc: 73.771% (7743/10496)\n",
      "Train: Loss: 0.746 | Acc: 73.748% (7835/10624)\n",
      "Train: Loss: 0.746 | Acc: 73.735% (7928/10752)\n",
      "Train: Loss: 0.746 | Acc: 73.722% (8021/10880)\n",
      "Train: Loss: 0.744 | Acc: 73.774% (8121/11008)\n",
      "Train: Loss: 0.743 | Acc: 73.815% (8220/11136)\n",
      "Train: Loss: 0.743 | Acc: 73.846% (8318/11264)\n",
      "Train: Loss: 0.743 | Acc: 73.876% (8416/11392)\n",
      "Train: Loss: 0.744 | Acc: 73.828% (8505/11520)\n",
      "Train: Loss: 0.747 | Acc: 73.738% (8589/11648)\n",
      "Train: Loss: 0.750 | Acc: 73.641% (8672/11776)\n",
      "Train: Loss: 0.749 | Acc: 73.648% (8767/11904)\n",
      "Train: Loss: 0.748 | Acc: 73.712% (8869/12032)\n",
      "Train: Loss: 0.748 | Acc: 73.766% (8970/12160)\n",
      "Train: Loss: 0.749 | Acc: 73.722% (9059/12288)\n",
      "Train: Loss: 0.749 | Acc: 73.727% (9154/12416)\n",
      "Train: Loss: 0.748 | Acc: 73.772% (9254/12544)\n",
      "Train: Loss: 0.748 | Acc: 73.777% (9349/12672)\n",
      "Train: Loss: 0.745 | Acc: 73.852% (9453/12800)\n",
      "Train: Loss: 0.745 | Acc: 73.894% (9553/12928)\n",
      "Train: Loss: 0.745 | Acc: 73.843% (9641/13056)\n",
      "Train: Loss: 0.745 | Acc: 73.817% (9732/13184)\n",
      "Train: Loss: 0.745 | Acc: 73.828% (9828/13312)\n",
      "Train: Loss: 0.743 | Acc: 73.876% (9929/13440)\n",
      "Train: Loss: 0.743 | Acc: 73.880% (10024/13568)\n",
      "Train: Loss: 0.744 | Acc: 73.810% (10109/13696)\n",
      "Train: Loss: 0.744 | Acc: 73.857% (10210/13824)\n",
      "Train: Loss: 0.744 | Acc: 73.839% (10302/13952)\n",
      "Train: Loss: 0.743 | Acc: 73.835% (10396/14080)\n",
      "Train: Loss: 0.743 | Acc: 73.839% (10491/14208)\n",
      "Train: Loss: 0.742 | Acc: 73.898% (10594/14336)\n",
      "Train: Loss: 0.743 | Acc: 73.873% (10685/14464)\n",
      "Train: Loss: 0.743 | Acc: 73.856% (10777/14592)\n",
      "Train: Loss: 0.742 | Acc: 73.845% (10870/14720)\n",
      "Train: Loss: 0.743 | Acc: 73.869% (10968/14848)\n",
      "Train: Loss: 0.742 | Acc: 73.918% (11070/14976)\n",
      "Train: Loss: 0.742 | Acc: 73.914% (11164/15104)\n",
      "Train: Loss: 0.741 | Acc: 73.923% (11260/15232)\n",
      "Train: Loss: 0.741 | Acc: 73.952% (11359/15360)\n",
      "Train: Loss: 0.740 | Acc: 73.954% (11454/15488)\n",
      "Train: Loss: 0.740 | Acc: 73.950% (11548/15616)\n",
      "Train: Loss: 0.740 | Acc: 73.958% (11644/15744)\n",
      "Train: Loss: 0.739 | Acc: 73.960% (11739/15872)\n",
      "Train: Loss: 0.739 | Acc: 73.956% (11833/16000)\n",
      "Train: Loss: 0.739 | Acc: 73.946% (11926/16128)\n",
      "Train: Loss: 0.739 | Acc: 73.960% (12023/16256)\n",
      "Train: Loss: 0.738 | Acc: 73.999% (12124/16384)\n",
      "Train: Loss: 0.738 | Acc: 73.995% (12218/16512)\n",
      "Train: Loss: 0.737 | Acc: 74.038% (12320/16640)\n",
      "Train: Loss: 0.737 | Acc: 74.028% (12413/16768)\n",
      "Train: Loss: 0.737 | Acc: 74.006% (12504/16896)\n",
      "Train: Loss: 0.738 | Acc: 74.001% (12598/17024)\n",
      "Train: Loss: 0.737 | Acc: 74.056% (12702/17152)\n",
      "Train: Loss: 0.736 | Acc: 74.080% (12801/17280)\n",
      "Train: Loss: 0.736 | Acc: 74.098% (12899/17408)\n",
      "Train: Loss: 0.736 | Acc: 74.076% (12990/17536)\n",
      "Train: Loss: 0.735 | Acc: 74.072% (13084/17664)\n",
      "Train: Loss: 0.736 | Acc: 74.061% (13177/17792)\n",
      "Train: Loss: 0.737 | Acc: 73.990% (13259/17920)\n",
      "Train: Loss: 0.736 | Acc: 74.019% (13359/18048)\n",
      "Train: Loss: 0.737 | Acc: 73.993% (13449/18176)\n",
      "Train: Loss: 0.736 | Acc: 73.984% (13542/18304)\n",
      "Train: Loss: 0.736 | Acc: 74.023% (13644/18432)\n",
      "Train: Loss: 0.736 | Acc: 74.041% (13742/18560)\n",
      "Train: Loss: 0.736 | Acc: 74.026% (13834/18688)\n",
      "Train: Loss: 0.735 | Acc: 74.059% (13935/18816)\n",
      "Train: Loss: 0.736 | Acc: 74.029% (14024/18944)\n",
      "Train: Loss: 0.736 | Acc: 74.051% (14123/19072)\n",
      "Train: Loss: 0.735 | Acc: 74.062% (14220/19200)\n",
      "Train: Loss: 0.736 | Acc: 74.038% (14310/19328)\n",
      "Train: Loss: 0.736 | Acc: 74.065% (14410/19456)\n",
      "Train: Loss: 0.735 | Acc: 74.112% (14514/19584)\n",
      "Train: Loss: 0.735 | Acc: 74.122% (14611/19712)\n",
      "Train: Loss: 0.734 | Acc: 74.153% (14712/19840)\n",
      "Train: Loss: 0.734 | Acc: 74.149% (14806/19968)\n",
      "Train: Loss: 0.734 | Acc: 74.129% (14897/20096)\n",
      "Train: Loss: 0.733 | Acc: 74.159% (14998/20224)\n",
      "Train: Loss: 0.732 | Acc: 74.199% (15101/20352)\n",
      "Train: Loss: 0.732 | Acc: 74.189% (15194/20480)\n",
      "Train: Loss: 0.732 | Acc: 74.185% (15288/20608)\n",
      "Train: Loss: 0.732 | Acc: 74.185% (15383/20736)\n",
      "Train: Loss: 0.732 | Acc: 74.180% (15477/20864)\n",
      "Train: Loss: 0.732 | Acc: 74.185% (15573/20992)\n",
      "Train: Loss: 0.732 | Acc: 74.205% (15672/21120)\n",
      "Train: Loss: 0.732 | Acc: 74.242% (15775/21248)\n",
      "Train: Loss: 0.732 | Acc: 74.228% (15867/21376)\n",
      "Train: Loss: 0.732 | Acc: 74.205% (15957/21504)\n",
      "Train: Loss: 0.733 | Acc: 74.200% (16051/21632)\n",
      "Train: Loss: 0.733 | Acc: 74.191% (16144/21760)\n",
      "Train: Loss: 0.733 | Acc: 74.205% (16242/21888)\n",
      "Train: Loss: 0.733 | Acc: 74.191% (16334/22016)\n",
      "Train: Loss: 0.733 | Acc: 74.201% (16431/22144)\n",
      "Train: Loss: 0.733 | Acc: 74.232% (16533/22272)\n",
      "Train: Loss: 0.732 | Acc: 74.268% (16636/22400)\n",
      "Train: Loss: 0.732 | Acc: 74.268% (16731/22528)\n",
      "Train: Loss: 0.732 | Acc: 74.263% (16825/22656)\n",
      "Train: Loss: 0.731 | Acc: 74.276% (16923/22784)\n",
      "Train: Loss: 0.732 | Acc: 74.254% (17013/22912)\n",
      "Train: Loss: 0.733 | Acc: 74.223% (17101/23040)\n",
      "Train: Loss: 0.733 | Acc: 74.214% (17194/23168)\n",
      "Train: Loss: 0.734 | Acc: 74.197% (17285/23296)\n",
      "Train: Loss: 0.734 | Acc: 74.206% (17382/23424)\n",
      "Train: Loss: 0.734 | Acc: 74.189% (17473/23552)\n",
      "Train: Loss: 0.734 | Acc: 74.160% (17561/23680)\n",
      "Train: Loss: 0.734 | Acc: 74.177% (17660/23808)\n",
      "Train: Loss: 0.734 | Acc: 74.190% (17758/23936)\n",
      "Train: Loss: 0.734 | Acc: 74.186% (17852/24064)\n",
      "Train: Loss: 0.734 | Acc: 74.169% (17943/24192)\n",
      "Train: Loss: 0.734 | Acc: 74.186% (18042/24320)\n",
      "Train: Loss: 0.733 | Acc: 74.182% (18136/24448)\n",
      "Train: Loss: 0.733 | Acc: 74.178% (18230/24576)\n",
      "Train: Loss: 0.734 | Acc: 74.170% (18323/24704)\n",
      "Train: Loss: 0.733 | Acc: 74.170% (18418/24832)\n",
      "Train: Loss: 0.734 | Acc: 74.163% (18511/24960)\n",
      "Train: Loss: 0.733 | Acc: 74.183% (18611/25088)\n",
      "Train: Loss: 0.733 | Acc: 74.175% (18704/25216)\n",
      "Train: Loss: 0.733 | Acc: 74.160% (18795/25344)\n",
      "Train: Loss: 0.733 | Acc: 74.136% (18884/25472)\n",
      "Train: Loss: 0.733 | Acc: 74.141% (18980/25600)\n",
      "Train: Loss: 0.733 | Acc: 74.164% (19081/25728)\n",
      "Train: Loss: 0.732 | Acc: 74.176% (19179/25856)\n",
      "Train: Loss: 0.733 | Acc: 74.138% (19264/25984)\n",
      "Train: Loss: 0.733 | Acc: 74.154% (19363/26112)\n",
      "Train: Loss: 0.733 | Acc: 74.150% (19457/26240)\n",
      "Train: Loss: 0.733 | Acc: 74.147% (19551/26368)\n",
      "Train: Loss: 0.733 | Acc: 74.132% (19642/26496)\n",
      "Train: Loss: 0.733 | Acc: 74.129% (19736/26624)\n",
      "Train: Loss: 0.734 | Acc: 74.099% (19823/26752)\n",
      "Train: Loss: 0.735 | Acc: 74.096% (19917/26880)\n",
      "Train: Loss: 0.735 | Acc: 74.108% (20015/27008)\n",
      "Train: Loss: 0.734 | Acc: 74.130% (20116/27136)\n",
      "Train: Loss: 0.735 | Acc: 74.123% (20209/27264)\n",
      "Train: Loss: 0.735 | Acc: 74.109% (20300/27392)\n",
      "Train: Loss: 0.735 | Acc: 74.124% (20399/27520)\n",
      "Train: Loss: 0.735 | Acc: 74.132% (20496/27648)\n",
      "Train: Loss: 0.735 | Acc: 74.132% (20591/27776)\n",
      "Train: Loss: 0.735 | Acc: 74.104% (20678/27904)\n",
      "Train: Loss: 0.735 | Acc: 74.108% (20774/28032)\n",
      "Train: Loss: 0.735 | Acc: 74.137% (20877/28160)\n",
      "Train: Loss: 0.735 | Acc: 74.137% (20972/28288)\n",
      "Train: Loss: 0.735 | Acc: 74.138% (21067/28416)\n",
      "Train: Loss: 0.736 | Acc: 74.124% (21158/28544)\n",
      "Train: Loss: 0.736 | Acc: 74.132% (21255/28672)\n",
      "Train: Loss: 0.736 | Acc: 74.115% (21345/28800)\n",
      "Train: Loss: 0.736 | Acc: 74.122% (21442/28928)\n",
      "Train: Loss: 0.736 | Acc: 74.119% (21536/29056)\n",
      "Train: Loss: 0.736 | Acc: 74.092% (21623/29184)\n",
      "Train: Loss: 0.736 | Acc: 74.096% (21719/29312)\n",
      "Train: Loss: 0.736 | Acc: 74.120% (21821/29440)\n",
      "Train: Loss: 0.736 | Acc: 74.114% (21914/29568)\n",
      "Train: Loss: 0.736 | Acc: 74.094% (22003/29696)\n",
      "Train: Loss: 0.737 | Acc: 74.075% (22092/29824)\n",
      "Train: Loss: 0.737 | Acc: 74.065% (22184/29952)\n",
      "Train: Loss: 0.737 | Acc: 74.062% (22278/30080)\n",
      "Train: Loss: 0.738 | Acc: 74.040% (22366/30208)\n",
      "Train: Loss: 0.737 | Acc: 74.034% (22459/30336)\n",
      "Train: Loss: 0.737 | Acc: 74.019% (22549/30464)\n",
      "Train: Loss: 0.738 | Acc: 74.003% (22639/30592)\n",
      "Train: Loss: 0.738 | Acc: 73.975% (22725/30720)\n",
      "Train: Loss: 0.738 | Acc: 73.979% (22821/30848)\n",
      "Train: Loss: 0.737 | Acc: 74.002% (22923/30976)\n",
      "Train: Loss: 0.737 | Acc: 74.032% (23027/31104)\n",
      "Train: Loss: 0.738 | Acc: 73.991% (23109/31232)\n",
      "Train: Loss: 0.737 | Acc: 73.996% (23205/31360)\n",
      "Train: Loss: 0.737 | Acc: 73.990% (23298/31488)\n",
      "Train: Loss: 0.737 | Acc: 74.004% (23397/31616)\n",
      "Train: Loss: 0.737 | Acc: 74.001% (23491/31744)\n",
      "Train: Loss: 0.737 | Acc: 74.005% (23587/31872)\n",
      "Train: Loss: 0.737 | Acc: 73.987% (23676/32000)\n",
      "Train: Loss: 0.738 | Acc: 73.982% (23769/32128)\n",
      "Train: Loss: 0.738 | Acc: 73.999% (23869/32256)\n",
      "Train: Loss: 0.737 | Acc: 74.012% (23968/32384)\n",
      "Train: Loss: 0.737 | Acc: 74.034% (24070/32512)\n",
      "Train: Loss: 0.737 | Acc: 74.047% (24169/32640)\n",
      "Train: Loss: 0.737 | Acc: 74.060% (24268/32768)\n",
      "Train: Loss: 0.737 | Acc: 74.079% (24369/32896)\n",
      "Train: Loss: 0.737 | Acc: 74.064% (24459/33024)\n",
      "Train: Loss: 0.736 | Acc: 74.095% (24564/33152)\n",
      "Train: Loss: 0.736 | Acc: 74.081% (24654/33280)\n",
      "Train: Loss: 0.736 | Acc: 74.087% (24751/33408)\n",
      "Train: Loss: 0.735 | Acc: 74.129% (24860/33536)\n",
      "Train: Loss: 0.735 | Acc: 74.150% (24962/33664)\n",
      "Train: Loss: 0.734 | Acc: 74.165% (25062/33792)\n",
      "Train: Loss: 0.734 | Acc: 74.172% (25159/33920)\n",
      "Train: Loss: 0.733 | Acc: 74.195% (25262/34048)\n",
      "Train: Loss: 0.733 | Acc: 74.210% (25362/34176)\n",
      "Train: Loss: 0.734 | Acc: 74.184% (25448/34304)\n",
      "Train: Loss: 0.733 | Acc: 74.210% (25552/34432)\n",
      "Train: Loss: 0.733 | Acc: 74.213% (25648/34560)\n",
      "Train: Loss: 0.733 | Acc: 74.236% (25751/34688)\n",
      "Train: Loss: 0.733 | Acc: 74.236% (25846/34816)\n",
      "Train: Loss: 0.733 | Acc: 74.256% (25948/34944)\n",
      "Train: Loss: 0.732 | Acc: 74.259% (26044/35072)\n",
      "Train: Loss: 0.733 | Acc: 74.253% (26137/35200)\n",
      "Train: Loss: 0.732 | Acc: 74.258% (26234/35328)\n",
      "Train: Loss: 0.732 | Acc: 74.261% (26330/35456)\n",
      "Train: Loss: 0.732 | Acc: 74.281% (26432/35584)\n",
      "Train: Loss: 0.732 | Acc: 74.283% (26528/35712)\n",
      "Train: Loss: 0.731 | Acc: 74.280% (26622/35840)\n",
      "Train: Loss: 0.732 | Acc: 74.258% (26709/35968)\n",
      "Train: Loss: 0.731 | Acc: 74.277% (26811/36096)\n",
      "Train: Loss: 0.732 | Acc: 74.271% (26904/36224)\n",
      "Train: Loss: 0.732 | Acc: 74.255% (26993/36352)\n",
      "Train: Loss: 0.731 | Acc: 74.271% (27094/36480)\n",
      "Train: Loss: 0.731 | Acc: 74.262% (27186/36608)\n",
      "Train: Loss: 0.731 | Acc: 74.284% (27289/36736)\n",
      "Train: Loss: 0.731 | Acc: 74.284% (27384/36864)\n",
      "Train: Loss: 0.731 | Acc: 74.286% (27480/36992)\n",
      "Train: Loss: 0.731 | Acc: 74.289% (27576/37120)\n",
      "Train: Loss: 0.731 | Acc: 74.286% (27670/37248)\n",
      "Train: Loss: 0.732 | Acc: 74.283% (27764/37376)\n",
      "Train: Loss: 0.732 | Acc: 74.275% (27856/37504)\n",
      "Train: Loss: 0.733 | Acc: 74.259% (27945/37632)\n",
      "Train: Loss: 0.732 | Acc: 74.256% (28039/37760)\n",
      "Train: Loss: 0.732 | Acc: 74.279% (28143/37888)\n",
      "Train: Loss: 0.732 | Acc: 74.295% (28244/38016)\n",
      "Train: Loss: 0.732 | Acc: 74.311% (28345/38144)\n",
      "Train: Loss: 0.732 | Acc: 74.308% (28439/38272)\n",
      "Train: Loss: 0.732 | Acc: 74.307% (28534/38400)\n",
      "Train: Loss: 0.731 | Acc: 74.328% (28637/38528)\n",
      "Train: Loss: 0.731 | Acc: 74.320% (28729/38656)\n",
      "Train: Loss: 0.731 | Acc: 74.304% (28818/38784)\n",
      "Train: Loss: 0.732 | Acc: 74.301% (28912/38912)\n",
      "Train: Loss: 0.732 | Acc: 74.288% (29002/39040)\n",
      "Train: Loss: 0.732 | Acc: 74.285% (29096/39168)\n",
      "Train: Loss: 0.732 | Acc: 74.280% (29189/39296)\n",
      "Train: Loss: 0.732 | Acc: 74.269% (29280/39424)\n",
      "Train: Loss: 0.732 | Acc: 74.279% (29379/39552)\n",
      "Train: Loss: 0.732 | Acc: 74.279% (29474/39680)\n",
      "Train: Loss: 0.732 | Acc: 74.282% (29570/39808)\n",
      "Train: Loss: 0.731 | Acc: 74.276% (29663/39936)\n",
      "Train: Loss: 0.731 | Acc: 74.296% (29766/40064)\n",
      "Train: Loss: 0.731 | Acc: 74.318% (29870/40192)\n",
      "Train: Loss: 0.730 | Acc: 74.355% (29980/40320)\n",
      "Train: Loss: 0.730 | Acc: 74.370% (30081/40448)\n",
      "Train: Loss: 0.729 | Acc: 74.391% (30185/40576)\n",
      "Train: Loss: 0.729 | Acc: 74.396% (30282/40704)\n",
      "Train: Loss: 0.729 | Acc: 74.400% (30379/40832)\n",
      "Train: Loss: 0.728 | Acc: 74.417% (30481/40960)\n",
      "Train: Loss: 0.729 | Acc: 74.392% (30566/41088)\n",
      "Train: Loss: 0.729 | Acc: 74.381% (30657/41216)\n",
      "Train: Loss: 0.729 | Acc: 74.376% (30750/41344)\n",
      "Train: Loss: 0.730 | Acc: 74.366% (30841/41472)\n",
      "Train: Loss: 0.730 | Acc: 74.353% (30931/41600)\n",
      "Train: Loss: 0.730 | Acc: 74.370% (31033/41728)\n",
      "Train: Loss: 0.730 | Acc: 74.364% (31126/41856)\n",
      "Train: Loss: 0.730 | Acc: 74.350% (31215/41984)\n",
      "Train: Loss: 0.730 | Acc: 74.342% (31307/42112)\n",
      "Train: Loss: 0.730 | Acc: 74.349% (31405/42240)\n",
      "Train: Loss: 0.730 | Acc: 74.327% (31491/42368)\n",
      "Train: Loss: 0.730 | Acc: 74.329% (31587/42496)\n",
      "Train: Loss: 0.730 | Acc: 74.334% (31684/42624)\n",
      "Train: Loss: 0.730 | Acc: 74.354% (31788/42752)\n",
      "Train: Loss: 0.729 | Acc: 74.384% (31896/42880)\n",
      "Train: Loss: 0.729 | Acc: 74.384% (31991/43008)\n",
      "Train: Loss: 0.729 | Acc: 74.386% (32087/43136)\n",
      "Train: Loss: 0.729 | Acc: 74.383% (32181/43264)\n",
      "Train: Loss: 0.729 | Acc: 74.392% (32280/43392)\n",
      "Train: Loss: 0.729 | Acc: 74.405% (32381/43520)\n",
      "Train: Loss: 0.729 | Acc: 74.411% (32479/43648)\n",
      "Train: Loss: 0.729 | Acc: 74.408% (32573/43776)\n",
      "Train: Loss: 0.729 | Acc: 74.419% (32673/43904)\n",
      "Train: Loss: 0.729 | Acc: 74.432% (32774/44032)\n",
      "Train: Loss: 0.728 | Acc: 74.452% (32878/44160)\n",
      "Train: Loss: 0.728 | Acc: 74.478% (32985/44288)\n",
      "Train: Loss: 0.728 | Acc: 74.473% (33078/44416)\n",
      "Train: Loss: 0.728 | Acc: 74.459% (33167/44544)\n",
      "Train: Loss: 0.728 | Acc: 74.467% (33266/44672)\n",
      "Train: Loss: 0.728 | Acc: 74.475% (33365/44800)\n",
      "Train: Loss: 0.728 | Acc: 74.470% (33458/44928)\n",
      "Train: Loss: 0.728 | Acc: 74.472% (33554/45056)\n",
      "Train: Loss: 0.727 | Acc: 74.489% (33657/45184)\n",
      "Train: Loss: 0.727 | Acc: 74.499% (33757/45312)\n",
      "Train: Loss: 0.727 | Acc: 74.509% (33857/45440)\n",
      "Train: Loss: 0.728 | Acc: 74.500% (33948/45568)\n",
      "Train: Loss: 0.727 | Acc: 74.516% (34051/45696)\n",
      "Train: Loss: 0.728 | Acc: 74.513% (34145/45824)\n",
      "Train: Loss: 0.727 | Acc: 74.517% (34242/45952)\n",
      "Train: Loss: 0.728 | Acc: 74.505% (34332/46080)\n",
      "Train: Loss: 0.728 | Acc: 74.498% (34424/46208)\n",
      "Train: Loss: 0.727 | Acc: 74.501% (34521/46336)\n",
      "Train: Loss: 0.727 | Acc: 74.494% (34613/46464)\n",
      "Train: Loss: 0.727 | Acc: 74.496% (34709/46592)\n",
      "Train: Loss: 0.727 | Acc: 74.484% (34799/46720)\n",
      "Train: Loss: 0.728 | Acc: 74.483% (34894/46848)\n",
      "Train: Loss: 0.727 | Acc: 74.504% (34999/46976)\n",
      "Train: Loss: 0.727 | Acc: 74.499% (35092/47104)\n",
      "Train: Loss: 0.728 | Acc: 74.481% (35179/47232)\n",
      "Train: Loss: 0.728 | Acc: 74.481% (35274/47360)\n",
      "Train: Loss: 0.728 | Acc: 74.480% (35369/47488)\n",
      "Train: Loss: 0.728 | Acc: 74.475% (35462/47616)\n",
      "Train: Loss: 0.728 | Acc: 74.464% (35552/47744)\n",
      "Train: Loss: 0.728 | Acc: 74.469% (35650/47872)\n",
      "Train: Loss: 0.727 | Acc: 74.490% (35755/48000)\n",
      "Train: Loss: 0.728 | Acc: 74.472% (35842/48128)\n",
      "Train: Loss: 0.728 | Acc: 74.478% (35940/48256)\n",
      "Train: Loss: 0.728 | Acc: 74.479% (36036/48384)\n",
      "Train: Loss: 0.728 | Acc: 74.483% (36133/48512)\n",
      "Train: Loss: 0.727 | Acc: 74.494% (36234/48640)\n",
      "Train: Loss: 0.727 | Acc: 74.473% (36319/48768)\n",
      "Train: Loss: 0.728 | Acc: 74.464% (36410/48896)\n",
      "Train: Loss: 0.727 | Acc: 74.482% (36514/49024)\n",
      "Train: Loss: 0.726 | Acc: 74.508% (36622/49152)\n",
      "Train: Loss: 0.726 | Acc: 74.531% (36729/49280)\n",
      "Train: Loss: 0.726 | Acc: 74.532% (36825/49408)\n",
      "Train: Loss: 0.726 | Acc: 74.520% (36914/49536)\n",
      "Train: Loss: 0.727 | Acc: 74.519% (37009/49664)\n",
      "Train: Loss: 0.726 | Acc: 74.528% (37109/49792)\n",
      "Train: Loss: 0.726 | Acc: 74.543% (37212/49920)\n",
      "Train: Loss: 0.726 | Acc: 74.552% (37276/50000)\n",
      "Test: Loss: 0.673 | Acc: 76.000% (76/100)\n",
      "Test: Loss: 0.706 | Acc: 74.500% (149/200)\n",
      "Test: Loss: 0.728 | Acc: 73.667% (221/300)\n",
      "Test: Loss: 0.748 | Acc: 74.250% (297/400)\n",
      "Test: Loss: 0.736 | Acc: 74.200% (371/500)\n",
      "Test: Loss: 0.702 | Acc: 75.167% (451/600)\n",
      "Test: Loss: 0.704 | Acc: 74.857% (524/700)\n",
      "Test: Loss: 0.721 | Acc: 74.500% (596/800)\n",
      "Test: Loss: 0.716 | Acc: 74.778% (673/900)\n",
      "Test: Loss: 0.717 | Acc: 74.400% (744/1000)\n",
      "Test: Loss: 0.723 | Acc: 74.091% (815/1100)\n",
      "Test: Loss: 0.730 | Acc: 73.750% (885/1200)\n",
      "Test: Loss: 0.720 | Acc: 73.923% (961/1300)\n",
      "Test: Loss: 0.717 | Acc: 74.071% (1037/1400)\n",
      "Test: Loss: 0.707 | Acc: 74.400% (1116/1500)\n",
      "Test: Loss: 0.715 | Acc: 74.125% (1186/1600)\n",
      "Test: Loss: 0.720 | Acc: 74.294% (1263/1700)\n",
      "Test: Loss: 0.725 | Acc: 74.222% (1336/1800)\n",
      "Test: Loss: 0.725 | Acc: 74.211% (1410/1900)\n",
      "Test: Loss: 0.736 | Acc: 74.000% (1480/2000)\n",
      "Test: Loss: 0.743 | Acc: 73.714% (1548/2100)\n",
      "Test: Loss: 0.744 | Acc: 73.727% (1622/2200)\n",
      "Test: Loss: 0.747 | Acc: 73.565% (1692/2300)\n",
      "Test: Loss: 0.748 | Acc: 73.417% (1762/2400)\n",
      "Test: Loss: 0.749 | Acc: 73.480% (1837/2500)\n",
      "Test: Loss: 0.757 | Acc: 73.154% (1902/2600)\n",
      "Test: Loss: 0.754 | Acc: 73.222% (1977/2700)\n",
      "Test: Loss: 0.755 | Acc: 73.000% (2044/2800)\n",
      "Test: Loss: 0.754 | Acc: 73.241% (2124/2900)\n",
      "Test: Loss: 0.748 | Acc: 73.400% (2202/3000)\n",
      "Test: Loss: 0.745 | Acc: 73.484% (2278/3100)\n",
      "Test: Loss: 0.740 | Acc: 73.594% (2355/3200)\n",
      "Test: Loss: 0.742 | Acc: 73.515% (2426/3300)\n",
      "Test: Loss: 0.740 | Acc: 73.618% (2503/3400)\n",
      "Test: Loss: 0.743 | Acc: 73.571% (2575/3500)\n",
      "Test: Loss: 0.744 | Acc: 73.667% (2652/3600)\n",
      "Test: Loss: 0.747 | Acc: 73.622% (2724/3700)\n",
      "Test: Loss: 0.749 | Acc: 73.658% (2799/3800)\n",
      "Test: Loss: 0.745 | Acc: 73.923% (2883/3900)\n",
      "Test: Loss: 0.743 | Acc: 73.950% (2958/4000)\n",
      "Test: Loss: 0.741 | Acc: 74.049% (3036/4100)\n",
      "Test: Loss: 0.743 | Acc: 74.143% (3114/4200)\n",
      "Test: Loss: 0.740 | Acc: 74.326% (3196/4300)\n",
      "Test: Loss: 0.741 | Acc: 74.364% (3272/4400)\n",
      "Test: Loss: 0.743 | Acc: 74.311% (3344/4500)\n",
      "Test: Loss: 0.742 | Acc: 74.261% (3416/4600)\n",
      "Test: Loss: 0.741 | Acc: 74.426% (3498/4700)\n",
      "Test: Loss: 0.745 | Acc: 74.333% (3568/4800)\n",
      "Test: Loss: 0.742 | Acc: 74.449% (3648/4900)\n",
      "Test: Loss: 0.742 | Acc: 74.500% (3725/5000)\n",
      "Test: Loss: 0.739 | Acc: 74.569% (3803/5100)\n",
      "Test: Loss: 0.738 | Acc: 74.558% (3877/5200)\n",
      "Test: Loss: 0.736 | Acc: 74.623% (3955/5300)\n",
      "Test: Loss: 0.736 | Acc: 74.630% (4030/5400)\n",
      "Test: Loss: 0.737 | Acc: 74.545% (4100/5500)\n",
      "Test: Loss: 0.739 | Acc: 74.571% (4176/5600)\n",
      "Test: Loss: 0.741 | Acc: 74.491% (4246/5700)\n",
      "Test: Loss: 0.739 | Acc: 74.586% (4326/5800)\n",
      "Test: Loss: 0.744 | Acc: 74.441% (4392/5900)\n",
      "Test: Loss: 0.745 | Acc: 74.467% (4468/6000)\n",
      "Test: Loss: 0.746 | Acc: 74.410% (4539/6100)\n",
      "Test: Loss: 0.745 | Acc: 74.435% (4615/6200)\n",
      "Test: Loss: 0.745 | Acc: 74.429% (4689/6300)\n",
      "Test: Loss: 0.744 | Acc: 74.469% (4766/6400)\n",
      "Test: Loss: 0.745 | Acc: 74.492% (4842/6500)\n",
      "Test: Loss: 0.750 | Acc: 74.439% (4913/6600)\n",
      "Test: Loss: 0.749 | Acc: 74.507% (4992/6700)\n",
      "Test: Loss: 0.752 | Acc: 74.368% (5057/6800)\n",
      "Test: Loss: 0.753 | Acc: 74.246% (5123/6900)\n",
      "Test: Loss: 0.757 | Acc: 74.114% (5188/7000)\n",
      "Test: Loss: 0.757 | Acc: 74.169% (5266/7100)\n",
      "Test: Loss: 0.756 | Acc: 74.167% (5340/7200)\n",
      "Test: Loss: 0.756 | Acc: 74.192% (5416/7300)\n",
      "Test: Loss: 0.755 | Acc: 74.230% (5493/7400)\n",
      "Test: Loss: 0.754 | Acc: 74.240% (5568/7500)\n",
      "Test: Loss: 0.754 | Acc: 74.237% (5642/7600)\n",
      "Test: Loss: 0.756 | Acc: 74.234% (5716/7700)\n",
      "Test: Loss: 0.755 | Acc: 74.244% (5791/7800)\n",
      "Test: Loss: 0.756 | Acc: 74.165% (5859/7900)\n",
      "Test: Loss: 0.757 | Acc: 74.050% (5924/8000)\n",
      "Test: Loss: 0.754 | Acc: 74.173% (6008/8100)\n",
      "Test: Loss: 0.754 | Acc: 74.183% (6083/8200)\n",
      "Test: Loss: 0.753 | Acc: 74.217% (6160/8300)\n",
      "Test: Loss: 0.755 | Acc: 74.143% (6228/8400)\n",
      "Test: Loss: 0.754 | Acc: 74.188% (6306/8500)\n",
      "Test: Loss: 0.755 | Acc: 74.174% (6379/8600)\n",
      "Test: Loss: 0.755 | Acc: 74.126% (6449/8700)\n",
      "Test: Loss: 0.755 | Acc: 74.114% (6522/8800)\n",
      "Test: Loss: 0.756 | Acc: 74.112% (6596/8900)\n",
      "Test: Loss: 0.757 | Acc: 74.111% (6670/9000)\n",
      "Test: Loss: 0.757 | Acc: 74.110% (6744/9100)\n",
      "Test: Loss: 0.755 | Acc: 74.185% (6825/9200)\n",
      "Test: Loss: 0.754 | Acc: 74.194% (6900/9300)\n",
      "Test: Loss: 0.755 | Acc: 74.138% (6969/9400)\n",
      "Test: Loss: 0.755 | Acc: 74.137% (7043/9500)\n",
      "Test: Loss: 0.754 | Acc: 74.177% (7121/9600)\n",
      "Test: Loss: 0.755 | Acc: 74.165% (7194/9700)\n",
      "Test: Loss: 0.755 | Acc: 74.112% (7263/9800)\n",
      "Test: Loss: 0.755 | Acc: 74.162% (7342/9900)\n",
      "Test: Loss: 0.756 | Acc: 74.160% (7416/10000)\n",
      "Train: Loss: 0.672 | Acc: 80.469% (103/128)\n",
      "Train: Loss: 0.598 | Acc: 81.250% (208/256)\n",
      "Train: Loss: 0.562 | Acc: 82.292% (316/384)\n",
      "Train: Loss: 0.607 | Acc: 80.469% (412/512)\n",
      "Train: Loss: 0.619 | Acc: 79.219% (507/640)\n",
      "Train: Loss: 0.615 | Acc: 78.255% (601/768)\n",
      "Train: Loss: 0.618 | Acc: 78.013% (699/896)\n",
      "Train: Loss: 0.604 | Acc: 78.711% (806/1024)\n",
      "Train: Loss: 0.613 | Acc: 78.906% (909/1152)\n",
      "Train: Loss: 0.622 | Acc: 78.516% (1005/1280)\n",
      "Train: Loss: 0.616 | Acc: 78.835% (1110/1408)\n",
      "Train: Loss: 0.616 | Acc: 78.971% (1213/1536)\n",
      "Train: Loss: 0.615 | Acc: 78.966% (1314/1664)\n",
      "Train: Loss: 0.616 | Acc: 78.962% (1415/1792)\n",
      "Train: Loss: 0.630 | Acc: 78.594% (1509/1920)\n",
      "Train: Loss: 0.621 | Acc: 78.857% (1615/2048)\n",
      "Train: Loss: 0.611 | Acc: 79.090% (1721/2176)\n",
      "Train: Loss: 0.600 | Acc: 79.297% (1827/2304)\n",
      "Train: Loss: 0.590 | Acc: 79.729% (1939/2432)\n",
      "Train: Loss: 0.594 | Acc: 79.492% (2035/2560)\n",
      "Train: Loss: 0.596 | Acc: 79.539% (2138/2688)\n",
      "Train: Loss: 0.596 | Acc: 79.616% (2242/2816)\n",
      "Train: Loss: 0.592 | Acc: 79.721% (2347/2944)\n",
      "Train: Loss: 0.586 | Acc: 79.883% (2454/3072)\n",
      "Train: Loss: 0.584 | Acc: 80.031% (2561/3200)\n",
      "Train: Loss: 0.582 | Acc: 80.078% (2665/3328)\n",
      "Train: Loss: 0.582 | Acc: 80.093% (2768/3456)\n",
      "Train: Loss: 0.583 | Acc: 80.078% (2870/3584)\n",
      "Train: Loss: 0.584 | Acc: 80.038% (2971/3712)\n",
      "Train: Loss: 0.583 | Acc: 80.000% (3072/3840)\n",
      "Train: Loss: 0.586 | Acc: 79.839% (3168/3968)\n",
      "Train: Loss: 0.583 | Acc: 79.858% (3271/4096)\n",
      "Train: Loss: 0.584 | Acc: 79.735% (3368/4224)\n",
      "Train: Loss: 0.589 | Acc: 79.481% (3459/4352)\n",
      "Train: Loss: 0.587 | Acc: 79.576% (3565/4480)\n",
      "Train: Loss: 0.588 | Acc: 79.297% (3654/4608)\n",
      "Train: Loss: 0.590 | Acc: 79.371% (3759/4736)\n",
      "Train: Loss: 0.589 | Acc: 79.461% (3865/4864)\n",
      "Train: Loss: 0.588 | Acc: 79.487% (3968/4992)\n",
      "Train: Loss: 0.588 | Acc: 79.492% (4070/5120)\n",
      "Train: Loss: 0.584 | Acc: 79.668% (4181/5248)\n",
      "Train: Loss: 0.587 | Acc: 79.539% (4276/5376)\n",
      "Train: Loss: 0.589 | Acc: 79.397% (4370/5504)\n",
      "Train: Loss: 0.592 | Acc: 79.368% (4470/5632)\n",
      "Train: Loss: 0.593 | Acc: 79.323% (4569/5760)\n",
      "Train: Loss: 0.592 | Acc: 79.348% (4672/5888)\n",
      "Train: Loss: 0.590 | Acc: 79.272% (4769/6016)\n",
      "Train: Loss: 0.590 | Acc: 79.313% (4873/6144)\n",
      "Train: Loss: 0.593 | Acc: 79.257% (4971/6272)\n",
      "Train: Loss: 0.595 | Acc: 79.234% (5071/6400)\n",
      "Train: Loss: 0.593 | Acc: 79.320% (5178/6528)\n",
      "Train: Loss: 0.592 | Acc: 79.312% (5279/6656)\n",
      "Train: Loss: 0.592 | Acc: 79.334% (5382/6784)\n",
      "Train: Loss: 0.590 | Acc: 79.297% (5481/6912)\n",
      "Train: Loss: 0.593 | Acc: 79.176% (5574/7040)\n",
      "Train: Loss: 0.596 | Acc: 79.060% (5667/7168)\n",
      "Train: Loss: 0.596 | Acc: 79.084% (5770/7296)\n",
      "Train: Loss: 0.595 | Acc: 79.108% (5873/7424)\n",
      "Train: Loss: 0.594 | Acc: 79.092% (5973/7552)\n",
      "Train: Loss: 0.597 | Acc: 79.062% (6072/7680)\n",
      "Train: Loss: 0.598 | Acc: 78.957% (6165/7808)\n",
      "Train: Loss: 0.596 | Acc: 79.007% (6270/7936)\n",
      "Train: Loss: 0.595 | Acc: 79.030% (6373/8064)\n",
      "Train: Loss: 0.595 | Acc: 79.065% (6477/8192)\n",
      "Train: Loss: 0.596 | Acc: 78.966% (6570/8320)\n",
      "Train: Loss: 0.594 | Acc: 79.001% (6674/8448)\n",
      "Train: Loss: 0.595 | Acc: 78.976% (6773/8576)\n",
      "Train: Loss: 0.595 | Acc: 78.941% (6871/8704)\n",
      "Train: Loss: 0.594 | Acc: 78.952% (6973/8832)\n",
      "Train: Loss: 0.596 | Acc: 78.962% (7075/8960)\n",
      "Train: Loss: 0.598 | Acc: 78.862% (7167/9088)\n",
      "Train: Loss: 0.597 | Acc: 78.917% (7273/9216)\n",
      "Train: Loss: 0.596 | Acc: 78.896% (7372/9344)\n",
      "Train: Loss: 0.597 | Acc: 78.896% (7473/9472)\n",
      "Train: Loss: 0.597 | Acc: 78.927% (7577/9600)\n",
      "Train: Loss: 0.598 | Acc: 78.896% (7675/9728)\n",
      "Train: Loss: 0.598 | Acc: 78.977% (7784/9856)\n",
      "Train: Loss: 0.596 | Acc: 79.077% (7895/9984)\n",
      "Train: Loss: 0.596 | Acc: 79.144% (8003/10112)\n",
      "Train: Loss: 0.595 | Acc: 79.170% (8107/10240)\n",
      "Train: Loss: 0.593 | Acc: 79.253% (8217/10368)\n",
      "Train: Loss: 0.594 | Acc: 79.278% (8321/10496)\n",
      "Train: Loss: 0.592 | Acc: 79.367% (8432/10624)\n",
      "Train: Loss: 0.592 | Acc: 79.306% (8527/10752)\n",
      "Train: Loss: 0.592 | Acc: 79.311% (8629/10880)\n",
      "Train: Loss: 0.593 | Acc: 79.270% (8726/11008)\n",
      "Train: Loss: 0.593 | Acc: 79.274% (8828/11136)\n",
      "Train: Loss: 0.594 | Acc: 79.261% (8928/11264)\n",
      "Train: Loss: 0.593 | Acc: 79.275% (9031/11392)\n",
      "Train: Loss: 0.594 | Acc: 79.253% (9130/11520)\n",
      "Train: Loss: 0.593 | Acc: 79.241% (9230/11648)\n",
      "Train: Loss: 0.592 | Acc: 79.271% (9335/11776)\n",
      "Train: Loss: 0.592 | Acc: 79.242% (9433/11904)\n",
      "Train: Loss: 0.592 | Acc: 79.222% (9532/12032)\n",
      "Train: Loss: 0.592 | Acc: 79.219% (9633/12160)\n",
      "Train: Loss: 0.592 | Acc: 79.248% (9738/12288)\n",
      "Train: Loss: 0.592 | Acc: 79.220% (9836/12416)\n",
      "Train: Loss: 0.592 | Acc: 79.257% (9942/12544)\n",
      "Train: Loss: 0.592 | Acc: 79.214% (10038/12672)\n",
      "Train: Loss: 0.592 | Acc: 79.203% (10138/12800)\n",
      "Train: Loss: 0.591 | Acc: 79.216% (10241/12928)\n",
      "Train: Loss: 0.594 | Acc: 79.159% (10335/13056)\n",
      "Train: Loss: 0.593 | Acc: 79.172% (10438/13184)\n",
      "Train: Loss: 0.593 | Acc: 79.177% (10540/13312)\n",
      "Train: Loss: 0.593 | Acc: 79.174% (10641/13440)\n",
      "Train: Loss: 0.593 | Acc: 79.201% (10746/13568)\n",
      "Train: Loss: 0.592 | Acc: 79.213% (10849/13696)\n",
      "Train: Loss: 0.592 | Acc: 79.196% (10948/13824)\n",
      "Train: Loss: 0.592 | Acc: 79.193% (11049/13952)\n",
      "Train: Loss: 0.593 | Acc: 79.141% (11143/14080)\n",
      "Train: Loss: 0.593 | Acc: 79.139% (11244/14208)\n",
      "Train: Loss: 0.593 | Acc: 79.109% (11341/14336)\n",
      "Train: Loss: 0.592 | Acc: 79.141% (11447/14464)\n",
      "Train: Loss: 0.591 | Acc: 79.180% (11554/14592)\n",
      "Train: Loss: 0.591 | Acc: 79.219% (11661/14720)\n",
      "Train: Loss: 0.590 | Acc: 79.203% (11760/14848)\n",
      "Train: Loss: 0.589 | Acc: 79.240% (11867/14976)\n",
      "Train: Loss: 0.588 | Acc: 79.277% (11974/15104)\n",
      "Train: Loss: 0.588 | Acc: 79.307% (12080/15232)\n",
      "Train: Loss: 0.587 | Acc: 79.323% (12184/15360)\n",
      "Train: Loss: 0.587 | Acc: 79.326% (12286/15488)\n",
      "Train: Loss: 0.587 | Acc: 79.348% (12391/15616)\n",
      "Train: Loss: 0.587 | Acc: 79.370% (12496/15744)\n",
      "Train: Loss: 0.586 | Acc: 79.385% (12600/15872)\n",
      "Train: Loss: 0.586 | Acc: 79.362% (12698/16000)\n",
      "Train: Loss: 0.586 | Acc: 79.415% (12808/16128)\n",
      "Train: Loss: 0.585 | Acc: 79.405% (12908/16256)\n",
      "Train: Loss: 0.584 | Acc: 79.413% (13011/16384)\n",
      "Train: Loss: 0.584 | Acc: 79.445% (13118/16512)\n",
      "Train: Loss: 0.584 | Acc: 79.411% (13214/16640)\n",
      "Train: Loss: 0.584 | Acc: 79.401% (13314/16768)\n",
      "Train: Loss: 0.584 | Acc: 79.421% (13419/16896)\n",
      "Train: Loss: 0.584 | Acc: 79.429% (13522/17024)\n",
      "Train: Loss: 0.585 | Acc: 79.402% (13619/17152)\n",
      "Train: Loss: 0.584 | Acc: 79.433% (13726/17280)\n",
      "Train: Loss: 0.584 | Acc: 79.458% (13832/17408)\n",
      "Train: Loss: 0.583 | Acc: 79.477% (13937/17536)\n",
      "Train: Loss: 0.584 | Acc: 79.438% (14032/17664)\n",
      "Train: Loss: 0.585 | Acc: 79.395% (14126/17792)\n",
      "Train: Loss: 0.586 | Acc: 79.375% (14224/17920)\n",
      "Train: Loss: 0.585 | Acc: 79.405% (14331/18048)\n",
      "Train: Loss: 0.587 | Acc: 79.319% (14417/18176)\n",
      "Train: Loss: 0.587 | Acc: 79.321% (14519/18304)\n",
      "Train: Loss: 0.588 | Acc: 79.324% (14621/18432)\n",
      "Train: Loss: 0.587 | Acc: 79.359% (14729/18560)\n",
      "Train: Loss: 0.587 | Acc: 79.372% (14833/18688)\n",
      "Train: Loss: 0.587 | Acc: 79.358% (14932/18816)\n",
      "Train: Loss: 0.587 | Acc: 79.350% (15032/18944)\n",
      "Train: Loss: 0.587 | Acc: 79.341% (15132/19072)\n",
      "Train: Loss: 0.588 | Acc: 79.323% (15230/19200)\n",
      "Train: Loss: 0.588 | Acc: 79.299% (15327/19328)\n",
      "Train: Loss: 0.588 | Acc: 79.312% (15431/19456)\n",
      "Train: Loss: 0.588 | Acc: 79.325% (15535/19584)\n",
      "Train: Loss: 0.587 | Acc: 79.337% (15639/19712)\n",
      "Train: Loss: 0.587 | Acc: 79.375% (15748/19840)\n",
      "Train: Loss: 0.586 | Acc: 79.407% (15856/19968)\n",
      "Train: Loss: 0.586 | Acc: 79.394% (15955/20096)\n",
      "Train: Loss: 0.588 | Acc: 79.386% (16055/20224)\n",
      "Train: Loss: 0.588 | Acc: 79.393% (16158/20352)\n",
      "Train: Loss: 0.588 | Acc: 79.390% (16259/20480)\n",
      "Train: Loss: 0.589 | Acc: 79.362% (16355/20608)\n",
      "Train: Loss: 0.588 | Acc: 79.413% (16467/20736)\n",
      "Train: Loss: 0.588 | Acc: 79.371% (16560/20864)\n",
      "Train: Loss: 0.589 | Acc: 79.368% (16661/20992)\n",
      "Train: Loss: 0.588 | Acc: 79.408% (16771/21120)\n",
      "Train: Loss: 0.589 | Acc: 79.400% (16871/21248)\n",
      "Train: Loss: 0.588 | Acc: 79.388% (16970/21376)\n",
      "Train: Loss: 0.588 | Acc: 79.385% (17071/21504)\n",
      "Train: Loss: 0.589 | Acc: 79.350% (17165/21632)\n",
      "Train: Loss: 0.589 | Acc: 79.389% (17275/21760)\n",
      "Train: Loss: 0.589 | Acc: 79.377% (17374/21888)\n",
      "Train: Loss: 0.590 | Acc: 79.379% (17476/22016)\n",
      "Train: Loss: 0.590 | Acc: 79.358% (17573/22144)\n",
      "Train: Loss: 0.590 | Acc: 79.373% (17678/22272)\n",
      "Train: Loss: 0.591 | Acc: 79.357% (17776/22400)\n",
      "Train: Loss: 0.591 | Acc: 79.346% (17875/22528)\n",
      "Train: Loss: 0.591 | Acc: 79.321% (17971/22656)\n",
      "Train: Loss: 0.591 | Acc: 79.319% (18072/22784)\n",
      "Train: Loss: 0.591 | Acc: 79.338% (18178/22912)\n",
      "Train: Loss: 0.591 | Acc: 79.340% (18280/23040)\n",
      "Train: Loss: 0.591 | Acc: 79.342% (18382/23168)\n",
      "Train: Loss: 0.592 | Acc: 79.318% (18478/23296)\n",
      "Train: Loss: 0.593 | Acc: 79.295% (18574/23424)\n",
      "Train: Loss: 0.592 | Acc: 79.297% (18676/23552)\n",
      "Train: Loss: 0.592 | Acc: 79.299% (18778/23680)\n",
      "Train: Loss: 0.592 | Acc: 79.305% (18881/23808)\n",
      "Train: Loss: 0.592 | Acc: 79.291% (18979/23936)\n",
      "Train: Loss: 0.593 | Acc: 79.259% (19073/24064)\n",
      "Train: Loss: 0.593 | Acc: 79.270% (19177/24192)\n",
      "Train: Loss: 0.592 | Acc: 79.276% (19280/24320)\n",
      "Train: Loss: 0.592 | Acc: 79.278% (19382/24448)\n",
      "Train: Loss: 0.592 | Acc: 79.285% (19485/24576)\n",
      "Train: Loss: 0.593 | Acc: 79.271% (19583/24704)\n",
      "Train: Loss: 0.593 | Acc: 79.261% (19682/24832)\n",
      "Train: Loss: 0.593 | Acc: 79.279% (19788/24960)\n",
      "Train: Loss: 0.593 | Acc: 79.253% (19883/25088)\n",
      "Train: Loss: 0.593 | Acc: 79.251% (19984/25216)\n",
      "Train: Loss: 0.593 | Acc: 79.257% (20087/25344)\n",
      "Train: Loss: 0.593 | Acc: 79.248% (20186/25472)\n",
      "Train: Loss: 0.594 | Acc: 79.238% (20285/25600)\n",
      "Train: Loss: 0.594 | Acc: 79.225% (20383/25728)\n",
      "Train: Loss: 0.594 | Acc: 79.235% (20487/25856)\n",
      "Train: Loss: 0.595 | Acc: 79.226% (20586/25984)\n",
      "Train: Loss: 0.594 | Acc: 79.224% (20687/26112)\n",
      "Train: Loss: 0.594 | Acc: 79.234% (20791/26240)\n",
      "Train: Loss: 0.594 | Acc: 79.244% (20895/26368)\n",
      "Train: Loss: 0.594 | Acc: 79.223% (20991/26496)\n",
      "Train: Loss: 0.594 | Acc: 79.229% (21094/26624)\n",
      "Train: Loss: 0.594 | Acc: 79.205% (21189/26752)\n",
      "Train: Loss: 0.594 | Acc: 79.200% (21289/26880)\n",
      "Train: Loss: 0.594 | Acc: 79.225% (21397/27008)\n",
      "Train: Loss: 0.594 | Acc: 79.208% (21494/27136)\n",
      "Train: Loss: 0.594 | Acc: 79.196% (21592/27264)\n",
      "Train: Loss: 0.594 | Acc: 79.187% (21691/27392)\n",
      "Train: Loss: 0.594 | Acc: 79.201% (21796/27520)\n",
      "Train: Loss: 0.594 | Acc: 79.192% (21895/27648)\n",
      "Train: Loss: 0.594 | Acc: 79.205% (22000/27776)\n",
      "Train: Loss: 0.594 | Acc: 79.225% (22107/27904)\n",
      "Train: Loss: 0.594 | Acc: 79.220% (22207/28032)\n",
      "Train: Loss: 0.594 | Acc: 79.226% (22310/28160)\n",
      "Train: Loss: 0.593 | Acc: 79.249% (22418/28288)\n",
      "Train: Loss: 0.594 | Acc: 79.234% (22515/28416)\n",
      "Train: Loss: 0.593 | Acc: 79.225% (22614/28544)\n",
      "Train: Loss: 0.593 | Acc: 79.220% (22714/28672)\n",
      "Train: Loss: 0.594 | Acc: 79.215% (22814/28800)\n",
      "Train: Loss: 0.594 | Acc: 79.210% (22914/28928)\n",
      "Train: Loss: 0.594 | Acc: 79.202% (23013/29056)\n",
      "Train: Loss: 0.594 | Acc: 79.191% (23111/29184)\n",
      "Train: Loss: 0.595 | Acc: 79.183% (23210/29312)\n",
      "Train: Loss: 0.595 | Acc: 79.195% (23315/29440)\n",
      "Train: Loss: 0.595 | Acc: 79.197% (23417/29568)\n",
      "Train: Loss: 0.595 | Acc: 79.199% (23519/29696)\n",
      "Train: Loss: 0.594 | Acc: 79.221% (23627/29824)\n",
      "Train: Loss: 0.595 | Acc: 79.197% (23721/29952)\n",
      "Train: Loss: 0.595 | Acc: 79.172% (23815/30080)\n",
      "Train: Loss: 0.596 | Acc: 79.171% (23916/30208)\n",
      "Train: Loss: 0.595 | Acc: 79.167% (24016/30336)\n",
      "Train: Loss: 0.595 | Acc: 79.162% (24116/30464)\n",
      "Train: Loss: 0.595 | Acc: 79.158% (24216/30592)\n",
      "Train: Loss: 0.595 | Acc: 79.154% (24316/30720)\n",
      "Train: Loss: 0.595 | Acc: 79.153% (24417/30848)\n",
      "Train: Loss: 0.595 | Acc: 79.148% (24517/30976)\n",
      "Train: Loss: 0.595 | Acc: 79.151% (24619/31104)\n",
      "Train: Loss: 0.595 | Acc: 79.137% (24716/31232)\n",
      "Train: Loss: 0.596 | Acc: 79.117% (24811/31360)\n",
      "Train: Loss: 0.596 | Acc: 79.122% (24914/31488)\n",
      "Train: Loss: 0.596 | Acc: 79.109% (25011/31616)\n",
      "Train: Loss: 0.596 | Acc: 79.095% (25108/31744)\n",
      "Train: Loss: 0.596 | Acc: 79.085% (25206/31872)\n",
      "Train: Loss: 0.596 | Acc: 79.097% (25311/32000)\n",
      "Train: Loss: 0.596 | Acc: 79.071% (25404/32128)\n",
      "Train: Loss: 0.596 | Acc: 79.074% (25506/32256)\n",
      "Train: Loss: 0.596 | Acc: 79.082% (25610/32384)\n",
      "Train: Loss: 0.596 | Acc: 79.085% (25712/32512)\n",
      "Train: Loss: 0.596 | Acc: 79.066% (25807/32640)\n",
      "Train: Loss: 0.597 | Acc: 79.062% (25907/32768)\n",
      "Train: Loss: 0.596 | Acc: 79.058% (26007/32896)\n",
      "Train: Loss: 0.597 | Acc: 79.027% (26098/33024)\n",
      "Train: Loss: 0.596 | Acc: 79.060% (26210/33152)\n",
      "Train: Loss: 0.596 | Acc: 79.059% (26311/33280)\n",
      "Train: Loss: 0.596 | Acc: 79.065% (26414/33408)\n",
      "Train: Loss: 0.596 | Acc: 79.076% (26519/33536)\n",
      "Train: Loss: 0.596 | Acc: 79.073% (26619/33664)\n",
      "Train: Loss: 0.597 | Acc: 79.066% (26718/33792)\n",
      "Train: Loss: 0.597 | Acc: 79.060% (26817/33920)\n",
      "Train: Loss: 0.597 | Acc: 79.059% (26918/34048)\n",
      "Train: Loss: 0.596 | Acc: 79.055% (27018/34176)\n",
      "Train: Loss: 0.596 | Acc: 79.043% (27115/34304)\n",
      "Train: Loss: 0.597 | Acc: 79.031% (27212/34432)\n",
      "Train: Loss: 0.597 | Acc: 79.039% (27316/34560)\n",
      "Train: Loss: 0.596 | Acc: 79.065% (27426/34688)\n",
      "Train: Loss: 0.597 | Acc: 79.038% (27518/34816)\n",
      "Train: Loss: 0.597 | Acc: 79.024% (27614/34944)\n",
      "Train: Loss: 0.597 | Acc: 79.029% (27717/35072)\n",
      "Train: Loss: 0.597 | Acc: 79.028% (27818/35200)\n",
      "Train: Loss: 0.597 | Acc: 79.025% (27918/35328)\n",
      "Train: Loss: 0.597 | Acc: 79.028% (28020/35456)\n",
      "Train: Loss: 0.597 | Acc: 79.019% (28118/35584)\n",
      "Train: Loss: 0.596 | Acc: 79.018% (28219/35712)\n",
      "Train: Loss: 0.596 | Acc: 79.029% (28324/35840)\n",
      "Train: Loss: 0.596 | Acc: 79.042% (28430/35968)\n",
      "Train: Loss: 0.596 | Acc: 79.034% (28528/36096)\n",
      "Train: Loss: 0.596 | Acc: 79.044% (28633/36224)\n",
      "Train: Loss: 0.596 | Acc: 79.030% (28729/36352)\n",
      "Train: Loss: 0.596 | Acc: 79.043% (28835/36480)\n",
      "Train: Loss: 0.596 | Acc: 79.046% (28937/36608)\n",
      "Train: Loss: 0.596 | Acc: 79.031% (29033/36736)\n",
      "Train: Loss: 0.596 | Acc: 79.028% (29133/36864)\n",
      "Train: Loss: 0.596 | Acc: 79.039% (29238/36992)\n",
      "Train: Loss: 0.596 | Acc: 79.046% (29342/37120)\n",
      "Train: Loss: 0.596 | Acc: 79.051% (29445/37248)\n",
      "Train: Loss: 0.596 | Acc: 79.048% (29545/37376)\n",
      "Train: Loss: 0.596 | Acc: 79.026% (29638/37504)\n",
      "Train: Loss: 0.596 | Acc: 79.021% (29737/37632)\n",
      "Train: Loss: 0.596 | Acc: 79.020% (29838/37760)\n",
      "Train: Loss: 0.596 | Acc: 79.028% (29942/37888)\n",
      "Train: Loss: 0.597 | Acc: 79.017% (30039/38016)\n",
      "Train: Loss: 0.596 | Acc: 79.027% (30144/38144)\n",
      "Train: Loss: 0.597 | Acc: 79.013% (30240/38272)\n",
      "Train: Loss: 0.597 | Acc: 79.016% (30342/38400)\n",
      "Train: Loss: 0.596 | Acc: 79.036% (30451/38528)\n",
      "Train: Loss: 0.597 | Acc: 79.010% (30542/38656)\n",
      "Train: Loss: 0.597 | Acc: 79.012% (30644/38784)\n",
      "Train: Loss: 0.597 | Acc: 79.024% (30750/38912)\n",
      "Train: Loss: 0.597 | Acc: 79.019% (30849/39040)\n",
      "Train: Loss: 0.597 | Acc: 79.016% (30949/39168)\n",
      "Train: Loss: 0.597 | Acc: 79.031% (31056/39296)\n",
      "Train: Loss: 0.597 | Acc: 79.031% (31157/39424)\n",
      "Train: Loss: 0.597 | Acc: 79.040% (31262/39552)\n",
      "Train: Loss: 0.597 | Acc: 79.037% (31362/39680)\n",
      "Train: Loss: 0.598 | Acc: 79.017% (31455/39808)\n",
      "Train: Loss: 0.598 | Acc: 79.014% (31555/39936)\n",
      "Train: Loss: 0.598 | Acc: 79.014% (31656/40064)\n",
      "Train: Loss: 0.598 | Acc: 79.006% (31754/40192)\n",
      "Train: Loss: 0.598 | Acc: 79.033% (31866/40320)\n",
      "Train: Loss: 0.597 | Acc: 79.069% (31982/40448)\n",
      "Train: Loss: 0.597 | Acc: 79.079% (32087/40576)\n",
      "Train: Loss: 0.597 | Acc: 79.073% (32186/40704)\n",
      "Train: Loss: 0.596 | Acc: 79.078% (32289/40832)\n",
      "Train: Loss: 0.596 | Acc: 79.082% (32392/40960)\n",
      "Train: Loss: 0.596 | Acc: 79.077% (32491/41088)\n",
      "Train: Loss: 0.596 | Acc: 79.088% (32597/41216)\n",
      "Train: Loss: 0.596 | Acc: 79.090% (32699/41344)\n",
      "Train: Loss: 0.596 | Acc: 79.087% (32799/41472)\n",
      "Train: Loss: 0.596 | Acc: 79.089% (32901/41600)\n",
      "Train: Loss: 0.596 | Acc: 79.098% (33006/41728)\n",
      "Train: Loss: 0.596 | Acc: 79.112% (33113/41856)\n",
      "Train: Loss: 0.596 | Acc: 79.099% (33209/41984)\n",
      "Train: Loss: 0.596 | Acc: 79.099% (33310/42112)\n",
      "Train: Loss: 0.596 | Acc: 79.103% (33413/42240)\n",
      "Train: Loss: 0.596 | Acc: 79.112% (33518/42368)\n",
      "Train: Loss: 0.596 | Acc: 79.111% (33619/42496)\n",
      "Train: Loss: 0.596 | Acc: 79.115% (33722/42624)\n",
      "Train: Loss: 0.596 | Acc: 79.112% (33822/42752)\n",
      "Train: Loss: 0.595 | Acc: 79.132% (33932/42880)\n",
      "Train: Loss: 0.595 | Acc: 79.153% (34042/43008)\n",
      "Train: Loss: 0.594 | Acc: 79.177% (34154/43136)\n",
      "Train: Loss: 0.594 | Acc: 79.174% (34254/43264)\n",
      "Train: Loss: 0.595 | Acc: 79.160% (34349/43392)\n",
      "Train: Loss: 0.595 | Acc: 79.168% (34454/43520)\n",
      "Train: Loss: 0.595 | Acc: 79.167% (34555/43648)\n",
      "Train: Loss: 0.595 | Acc: 79.148% (34648/43776)\n",
      "Train: Loss: 0.595 | Acc: 79.152% (34751/43904)\n",
      "Train: Loss: 0.595 | Acc: 79.138% (34846/44032)\n",
      "Train: Loss: 0.595 | Acc: 79.121% (34940/44160)\n",
      "Train: Loss: 0.595 | Acc: 79.130% (35045/44288)\n",
      "Train: Loss: 0.595 | Acc: 79.145% (35153/44416)\n",
      "Train: Loss: 0.595 | Acc: 79.149% (35256/44544)\n",
      "Train: Loss: 0.594 | Acc: 79.164% (35364/44672)\n",
      "Train: Loss: 0.594 | Acc: 79.156% (35462/44800)\n",
      "Train: Loss: 0.595 | Acc: 79.144% (35558/44928)\n",
      "Train: Loss: 0.595 | Acc: 79.146% (35660/45056)\n",
      "Train: Loss: 0.594 | Acc: 79.161% (35768/45184)\n",
      "Train: Loss: 0.594 | Acc: 79.169% (35873/45312)\n",
      "Train: Loss: 0.594 | Acc: 79.170% (35975/45440)\n",
      "Train: Loss: 0.594 | Acc: 79.165% (36074/45568)\n",
      "Train: Loss: 0.594 | Acc: 79.171% (36178/45696)\n",
      "Train: Loss: 0.594 | Acc: 79.181% (36284/45824)\n",
      "Train: Loss: 0.594 | Acc: 79.189% (36389/45952)\n",
      "Train: Loss: 0.594 | Acc: 79.191% (36491/46080)\n",
      "Train: Loss: 0.593 | Acc: 79.183% (36589/46208)\n",
      "Train: Loss: 0.593 | Acc: 79.193% (36695/46336)\n",
      "Train: Loss: 0.593 | Acc: 79.195% (36797/46464)\n",
      "Train: Loss: 0.593 | Acc: 79.198% (36900/46592)\n",
      "Train: Loss: 0.593 | Acc: 79.219% (37011/46720)\n",
      "Train: Loss: 0.592 | Acc: 79.226% (37116/46848)\n",
      "Train: Loss: 0.592 | Acc: 79.234% (37221/46976)\n",
      "Train: Loss: 0.592 | Acc: 79.244% (37327/47104)\n",
      "Train: Loss: 0.592 | Acc: 79.245% (37429/47232)\n",
      "Train: Loss: 0.593 | Acc: 79.223% (37520/47360)\n",
      "Train: Loss: 0.592 | Acc: 79.235% (37627/47488)\n",
      "Train: Loss: 0.592 | Acc: 79.238% (37730/47616)\n",
      "Train: Loss: 0.592 | Acc: 79.233% (37829/47744)\n",
      "Train: Loss: 0.592 | Acc: 79.232% (37930/47872)\n",
      "Train: Loss: 0.592 | Acc: 79.217% (38024/48000)\n",
      "Train: Loss: 0.592 | Acc: 79.218% (38126/48128)\n",
      "Train: Loss: 0.593 | Acc: 79.215% (38226/48256)\n",
      "Train: Loss: 0.593 | Acc: 79.218% (38329/48384)\n",
      "Train: Loss: 0.593 | Acc: 79.211% (38427/48512)\n",
      "Train: Loss: 0.593 | Acc: 79.206% (38526/48640)\n",
      "Train: Loss: 0.593 | Acc: 79.202% (38625/48768)\n",
      "Train: Loss: 0.593 | Acc: 79.199% (38725/48896)\n",
      "Train: Loss: 0.593 | Acc: 79.200% (38827/49024)\n",
      "Train: Loss: 0.593 | Acc: 79.195% (38926/49152)\n",
      "Train: Loss: 0.593 | Acc: 79.200% (39030/49280)\n",
      "Train: Loss: 0.593 | Acc: 79.204% (39133/49408)\n",
      "Train: Loss: 0.593 | Acc: 79.205% (39235/49536)\n",
      "Train: Loss: 0.593 | Acc: 79.200% (39334/49664)\n",
      "Train: Loss: 0.593 | Acc: 79.199% (39435/49792)\n",
      "Train: Loss: 0.593 | Acc: 79.205% (39539/49920)\n",
      "Train: Loss: 0.593 | Acc: 79.198% (39599/50000)\n",
      "Test: Loss: 0.784 | Acc: 76.000% (76/100)\n",
      "Test: Loss: 0.802 | Acc: 72.500% (145/200)\n",
      "Test: Loss: 0.778 | Acc: 73.333% (220/300)\n",
      "Test: Loss: 0.800 | Acc: 72.500% (290/400)\n",
      "Test: Loss: 0.768 | Acc: 73.200% (366/500)\n",
      "Test: Loss: 0.764 | Acc: 72.833% (437/600)\n",
      "Test: Loss: 0.787 | Acc: 72.857% (510/700)\n",
      "Test: Loss: 0.808 | Acc: 72.000% (576/800)\n",
      "Test: Loss: 0.797 | Acc: 71.889% (647/900)\n",
      "Test: Loss: 0.783 | Acc: 72.500% (725/1000)\n",
      "Test: Loss: 0.775 | Acc: 73.091% (804/1100)\n",
      "Test: Loss: 0.775 | Acc: 72.917% (875/1200)\n",
      "Test: Loss: 0.777 | Acc: 72.769% (946/1300)\n",
      "Test: Loss: 0.777 | Acc: 72.571% (1016/1400)\n",
      "Test: Loss: 0.772 | Acc: 72.933% (1094/1500)\n",
      "Test: Loss: 0.773 | Acc: 72.812% (1165/1600)\n",
      "Test: Loss: 0.780 | Acc: 72.588% (1234/1700)\n",
      "Test: Loss: 0.786 | Acc: 72.611% (1307/1800)\n",
      "Test: Loss: 0.783 | Acc: 72.632% (1380/1900)\n",
      "Test: Loss: 0.791 | Acc: 72.450% (1449/2000)\n",
      "Test: Loss: 0.789 | Acc: 72.381% (1520/2100)\n",
      "Test: Loss: 0.791 | Acc: 72.318% (1591/2200)\n",
      "Test: Loss: 0.791 | Acc: 72.130% (1659/2300)\n",
      "Test: Loss: 0.788 | Acc: 72.167% (1732/2400)\n",
      "Test: Loss: 0.792 | Acc: 72.080% (1802/2500)\n",
      "Test: Loss: 0.804 | Acc: 71.692% (1864/2600)\n",
      "Test: Loss: 0.796 | Acc: 72.000% (1944/2700)\n",
      "Test: Loss: 0.794 | Acc: 72.107% (2019/2800)\n",
      "Test: Loss: 0.794 | Acc: 72.276% (2096/2900)\n",
      "Test: Loss: 0.790 | Acc: 72.400% (2172/3000)\n",
      "Test: Loss: 0.790 | Acc: 72.323% (2242/3100)\n",
      "Test: Loss: 0.789 | Acc: 72.281% (2313/3200)\n",
      "Test: Loss: 0.785 | Acc: 72.455% (2391/3300)\n",
      "Test: Loss: 0.791 | Acc: 72.235% (2456/3400)\n",
      "Test: Loss: 0.795 | Acc: 72.000% (2520/3500)\n",
      "Test: Loss: 0.790 | Acc: 72.167% (2598/3600)\n",
      "Test: Loss: 0.791 | Acc: 72.243% (2673/3700)\n",
      "Test: Loss: 0.792 | Acc: 72.184% (2743/3800)\n",
      "Test: Loss: 0.791 | Acc: 72.179% (2815/3900)\n",
      "Test: Loss: 0.789 | Acc: 72.225% (2889/4000)\n",
      "Test: Loss: 0.789 | Acc: 72.171% (2959/4100)\n",
      "Test: Loss: 0.790 | Acc: 72.286% (3036/4200)\n",
      "Test: Loss: 0.788 | Acc: 72.349% (3111/4300)\n",
      "Test: Loss: 0.786 | Acc: 72.432% (3187/4400)\n",
      "Test: Loss: 0.786 | Acc: 72.444% (3260/4500)\n",
      "Test: Loss: 0.785 | Acc: 72.500% (3335/4600)\n",
      "Test: Loss: 0.784 | Acc: 72.426% (3404/4700)\n",
      "Test: Loss: 0.784 | Acc: 72.479% (3479/4800)\n",
      "Test: Loss: 0.779 | Acc: 72.735% (3564/4900)\n",
      "Test: Loss: 0.779 | Acc: 72.640% (3632/5000)\n",
      "Test: Loss: 0.777 | Acc: 72.725% (3709/5100)\n",
      "Test: Loss: 0.780 | Acc: 72.596% (3775/5200)\n",
      "Test: Loss: 0.779 | Acc: 72.642% (3850/5300)\n",
      "Test: Loss: 0.780 | Acc: 72.685% (3925/5400)\n",
      "Test: Loss: 0.780 | Acc: 72.655% (3996/5500)\n",
      "Test: Loss: 0.782 | Acc: 72.643% (4068/5600)\n",
      "Test: Loss: 0.784 | Acc: 72.544% (4135/5700)\n",
      "Test: Loss: 0.781 | Acc: 72.707% (4217/5800)\n",
      "Test: Loss: 0.786 | Acc: 72.576% (4282/5900)\n",
      "Test: Loss: 0.787 | Acc: 72.633% (4358/6000)\n",
      "Test: Loss: 0.786 | Acc: 72.689% (4434/6100)\n",
      "Test: Loss: 0.785 | Acc: 72.694% (4507/6200)\n",
      "Test: Loss: 0.784 | Acc: 72.698% (4580/6300)\n",
      "Test: Loss: 0.781 | Acc: 72.797% (4659/6400)\n",
      "Test: Loss: 0.781 | Acc: 72.815% (4733/6500)\n",
      "Test: Loss: 0.782 | Acc: 72.773% (4803/6600)\n",
      "Test: Loss: 0.782 | Acc: 72.746% (4874/6700)\n",
      "Test: Loss: 0.785 | Acc: 72.662% (4941/6800)\n",
      "Test: Loss: 0.787 | Acc: 72.507% (5003/6900)\n",
      "Test: Loss: 0.789 | Acc: 72.414% (5069/7000)\n",
      "Test: Loss: 0.790 | Acc: 72.408% (5141/7100)\n",
      "Test: Loss: 0.791 | Acc: 72.417% (5214/7200)\n",
      "Test: Loss: 0.788 | Acc: 72.589% (5299/7300)\n",
      "Test: Loss: 0.786 | Acc: 72.635% (5375/7400)\n",
      "Test: Loss: 0.788 | Acc: 72.587% (5444/7500)\n",
      "Test: Loss: 0.788 | Acc: 72.579% (5516/7600)\n",
      "Test: Loss: 0.790 | Acc: 72.558% (5587/7700)\n",
      "Test: Loss: 0.790 | Acc: 72.487% (5654/7800)\n",
      "Test: Loss: 0.790 | Acc: 72.418% (5721/7900)\n",
      "Test: Loss: 0.789 | Acc: 72.450% (5796/8000)\n",
      "Test: Loss: 0.789 | Acc: 72.519% (5874/8100)\n",
      "Test: Loss: 0.789 | Acc: 72.537% (5948/8200)\n",
      "Test: Loss: 0.789 | Acc: 72.530% (6020/8300)\n",
      "Test: Loss: 0.788 | Acc: 72.619% (6100/8400)\n",
      "Test: Loss: 0.788 | Acc: 72.612% (6172/8500)\n",
      "Test: Loss: 0.788 | Acc: 72.616% (6245/8600)\n",
      "Test: Loss: 0.788 | Acc: 72.575% (6314/8700)\n",
      "Test: Loss: 0.789 | Acc: 72.545% (6384/8800)\n",
      "Test: Loss: 0.789 | Acc: 72.494% (6452/8900)\n",
      "Test: Loss: 0.788 | Acc: 72.544% (6529/9000)\n",
      "Test: Loss: 0.789 | Acc: 72.484% (6596/9100)\n",
      "Test: Loss: 0.787 | Acc: 72.543% (6674/9200)\n",
      "Test: Loss: 0.787 | Acc: 72.581% (6750/9300)\n",
      "Test: Loss: 0.786 | Acc: 72.585% (6823/9400)\n",
      "Test: Loss: 0.787 | Acc: 72.547% (6892/9500)\n",
      "Test: Loss: 0.785 | Acc: 72.594% (6969/9600)\n",
      "Test: Loss: 0.784 | Acc: 72.619% (7044/9700)\n",
      "Test: Loss: 0.788 | Acc: 72.500% (7105/9800)\n",
      "Test: Loss: 0.787 | Acc: 72.495% (7177/9900)\n",
      "Test: Loss: 0.788 | Acc: 72.480% (7248/10000)\n",
      "Train: Loss: 0.551 | Acc: 82.031% (105/128)\n",
      "Train: Loss: 0.507 | Acc: 83.984% (215/256)\n",
      "Train: Loss: 0.474 | Acc: 84.375% (324/384)\n",
      "Train: Loss: 0.469 | Acc: 85.156% (436/512)\n",
      "Train: Loss: 0.487 | Acc: 84.531% (541/640)\n",
      "Train: Loss: 0.484 | Acc: 83.724% (643/768)\n",
      "Train: Loss: 0.477 | Acc: 84.040% (753/896)\n",
      "Train: Loss: 0.465 | Acc: 84.668% (867/1024)\n",
      "Train: Loss: 0.452 | Acc: 84.809% (977/1152)\n",
      "Train: Loss: 0.452 | Acc: 84.766% (1085/1280)\n",
      "Train: Loss: 0.450 | Acc: 84.943% (1196/1408)\n",
      "Train: Loss: 0.447 | Acc: 84.896% (1304/1536)\n",
      "Train: Loss: 0.445 | Acc: 85.036% (1415/1664)\n",
      "Train: Loss: 0.449 | Acc: 84.877% (1521/1792)\n",
      "Train: Loss: 0.443 | Acc: 85.156% (1635/1920)\n",
      "Train: Loss: 0.441 | Acc: 85.303% (1747/2048)\n",
      "Train: Loss: 0.440 | Acc: 85.340% (1857/2176)\n",
      "Train: Loss: 0.436 | Acc: 85.503% (1970/2304)\n",
      "Train: Loss: 0.441 | Acc: 85.403% (2077/2432)\n",
      "Train: Loss: 0.443 | Acc: 85.273% (2183/2560)\n",
      "Train: Loss: 0.443 | Acc: 85.231% (2291/2688)\n",
      "Train: Loss: 0.444 | Acc: 85.263% (2401/2816)\n",
      "Train: Loss: 0.441 | Acc: 85.258% (2510/2944)\n",
      "Train: Loss: 0.440 | Acc: 85.221% (2618/3072)\n",
      "Train: Loss: 0.441 | Acc: 85.281% (2729/3200)\n",
      "Train: Loss: 0.440 | Acc: 85.397% (2842/3328)\n",
      "Train: Loss: 0.438 | Acc: 85.417% (2952/3456)\n",
      "Train: Loss: 0.439 | Acc: 85.379% (3060/3584)\n",
      "Train: Loss: 0.438 | Acc: 85.372% (3169/3712)\n",
      "Train: Loss: 0.441 | Acc: 85.260% (3274/3840)\n",
      "Train: Loss: 0.441 | Acc: 85.333% (3386/3968)\n",
      "Train: Loss: 0.440 | Acc: 85.278% (3493/4096)\n",
      "Train: Loss: 0.449 | Acc: 85.085% (3594/4224)\n",
      "Train: Loss: 0.451 | Acc: 84.972% (3698/4352)\n",
      "Train: Loss: 0.449 | Acc: 84.978% (3807/4480)\n",
      "Train: Loss: 0.449 | Acc: 85.004% (3917/4608)\n",
      "Train: Loss: 0.447 | Acc: 85.093% (4030/4736)\n",
      "Train: Loss: 0.448 | Acc: 84.910% (4130/4864)\n",
      "Train: Loss: 0.449 | Acc: 84.856% (4236/4992)\n",
      "Train: Loss: 0.451 | Acc: 84.844% (4344/5120)\n",
      "Train: Loss: 0.452 | Acc: 84.737% (4447/5248)\n",
      "Train: Loss: 0.454 | Acc: 84.598% (4548/5376)\n",
      "Train: Loss: 0.455 | Acc: 84.502% (4651/5504)\n",
      "Train: Loss: 0.457 | Acc: 84.393% (4753/5632)\n",
      "Train: Loss: 0.459 | Acc: 84.288% (4855/5760)\n",
      "Train: Loss: 0.459 | Acc: 84.341% (4966/5888)\n",
      "Train: Loss: 0.458 | Acc: 84.392% (5077/6016)\n",
      "Train: Loss: 0.460 | Acc: 84.342% (5182/6144)\n",
      "Train: Loss: 0.463 | Acc: 84.200% (5281/6272)\n",
      "Train: Loss: 0.463 | Acc: 84.156% (5386/6400)\n",
      "Train: Loss: 0.461 | Acc: 84.268% (5501/6528)\n",
      "Train: Loss: 0.462 | Acc: 84.180% (5603/6656)\n",
      "Train: Loss: 0.462 | Acc: 84.110% (5706/6784)\n",
      "Train: Loss: 0.462 | Acc: 84.086% (5812/6912)\n",
      "Train: Loss: 0.459 | Acc: 84.247% (5931/7040)\n",
      "Train: Loss: 0.459 | Acc: 84.263% (6040/7168)\n",
      "Train: Loss: 0.461 | Acc: 84.128% (6138/7296)\n",
      "Train: Loss: 0.464 | Acc: 83.957% (6233/7424)\n",
      "Train: Loss: 0.465 | Acc: 83.912% (6337/7552)\n",
      "Train: Loss: 0.465 | Acc: 83.906% (6444/7680)\n",
      "Train: Loss: 0.467 | Acc: 83.824% (6545/7808)\n",
      "Train: Loss: 0.467 | Acc: 83.808% (6651/7936)\n",
      "Train: Loss: 0.467 | Acc: 83.767% (6755/8064)\n",
      "Train: Loss: 0.466 | Acc: 83.765% (6862/8192)\n",
      "Train: Loss: 0.465 | Acc: 83.774% (6970/8320)\n",
      "Train: Loss: 0.464 | Acc: 83.866% (7085/8448)\n",
      "Train: Loss: 0.463 | Acc: 83.944% (7199/8576)\n",
      "Train: Loss: 0.461 | Acc: 83.984% (7310/8704)\n",
      "Train: Loss: 0.461 | Acc: 83.990% (7418/8832)\n",
      "Train: Loss: 0.462 | Acc: 83.940% (7521/8960)\n",
      "Train: Loss: 0.462 | Acc: 83.968% (7631/9088)\n",
      "Train: Loss: 0.462 | Acc: 83.941% (7736/9216)\n",
      "Train: Loss: 0.463 | Acc: 83.947% (7844/9344)\n",
      "Train: Loss: 0.462 | Acc: 83.995% (7956/9472)\n",
      "Train: Loss: 0.462 | Acc: 83.979% (8062/9600)\n",
      "Train: Loss: 0.462 | Acc: 83.954% (8167/9728)\n",
      "Train: Loss: 0.462 | Acc: 83.949% (8274/9856)\n",
      "Train: Loss: 0.462 | Acc: 83.954% (8382/9984)\n",
      "Train: Loss: 0.461 | Acc: 83.970% (8491/10112)\n",
      "Train: Loss: 0.462 | Acc: 83.965% (8598/10240)\n",
      "Train: Loss: 0.461 | Acc: 84.008% (8710/10368)\n",
      "Train: Loss: 0.461 | Acc: 84.022% (8819/10496)\n",
      "Train: Loss: 0.460 | Acc: 83.989% (8923/10624)\n",
      "Train: Loss: 0.462 | Acc: 83.910% (9022/10752)\n",
      "Train: Loss: 0.463 | Acc: 83.934% (9132/10880)\n",
      "Train: Loss: 0.462 | Acc: 83.975% (9244/11008)\n",
      "Train: Loss: 0.461 | Acc: 84.025% (9357/11136)\n",
      "Train: Loss: 0.462 | Acc: 83.993% (9461/11264)\n",
      "Train: Loss: 0.463 | Acc: 83.927% (9561/11392)\n",
      "Train: Loss: 0.464 | Acc: 83.880% (9663/11520)\n",
      "Train: Loss: 0.465 | Acc: 83.808% (9762/11648)\n",
      "Train: Loss: 0.467 | Acc: 83.755% (9863/11776)\n",
      "Train: Loss: 0.466 | Acc: 83.779% (9973/11904)\n",
      "Train: Loss: 0.466 | Acc: 83.735% (10075/12032)\n",
      "Train: Loss: 0.467 | Acc: 83.676% (10175/12160)\n",
      "Train: Loss: 0.467 | Acc: 83.691% (10284/12288)\n",
      "Train: Loss: 0.469 | Acc: 83.610% (10381/12416)\n",
      "Train: Loss: 0.468 | Acc: 83.658% (10494/12544)\n",
      "Train: Loss: 0.468 | Acc: 83.649% (10600/12672)\n",
      "Train: Loss: 0.468 | Acc: 83.664% (10709/12800)\n",
      "Train: Loss: 0.469 | Acc: 83.687% (10819/12928)\n",
      "Train: Loss: 0.469 | Acc: 83.678% (10925/13056)\n",
      "Train: Loss: 0.469 | Acc: 83.685% (11033/13184)\n",
      "Train: Loss: 0.470 | Acc: 83.661% (11137/13312)\n",
      "Train: Loss: 0.469 | Acc: 83.720% (11252/13440)\n",
      "Train: Loss: 0.470 | Acc: 83.667% (11352/13568)\n",
      "Train: Loss: 0.470 | Acc: 83.674% (11460/13696)\n",
      "Train: Loss: 0.470 | Acc: 83.644% (11563/13824)\n",
      "Train: Loss: 0.471 | Acc: 83.601% (11664/13952)\n",
      "Train: Loss: 0.471 | Acc: 83.622% (11774/14080)\n",
      "Train: Loss: 0.471 | Acc: 83.622% (11881/14208)\n",
      "Train: Loss: 0.472 | Acc: 83.608% (11986/14336)\n",
      "Train: Loss: 0.473 | Acc: 83.559% (12086/14464)\n",
      "Train: Loss: 0.472 | Acc: 83.594% (12198/14592)\n",
      "Train: Loss: 0.472 | Acc: 83.587% (12304/14720)\n",
      "Train: Loss: 0.473 | Acc: 83.587% (12411/14848)\n",
      "Train: Loss: 0.472 | Acc: 83.627% (12524/14976)\n",
      "Train: Loss: 0.473 | Acc: 83.587% (12625/15104)\n",
      "Train: Loss: 0.473 | Acc: 83.561% (12728/15232)\n",
      "Train: Loss: 0.474 | Acc: 83.529% (12830/15360)\n",
      "Train: Loss: 0.474 | Acc: 83.516% (12935/15488)\n",
      "Train: Loss: 0.474 | Acc: 83.459% (13033/15616)\n",
      "Train: Loss: 0.473 | Acc: 83.479% (13143/15744)\n",
      "Train: Loss: 0.473 | Acc: 83.499% (13253/15872)\n",
      "Train: Loss: 0.473 | Acc: 83.494% (13359/16000)\n",
      "Train: Loss: 0.473 | Acc: 83.507% (13468/16128)\n",
      "Train: Loss: 0.472 | Acc: 83.520% (13577/16256)\n",
      "Train: Loss: 0.472 | Acc: 83.514% (13683/16384)\n",
      "Train: Loss: 0.472 | Acc: 83.515% (13790/16512)\n",
      "Train: Loss: 0.471 | Acc: 83.576% (13907/16640)\n",
      "Train: Loss: 0.470 | Acc: 83.570% (14013/16768)\n",
      "Train: Loss: 0.471 | Acc: 83.546% (14116/16896)\n",
      "Train: Loss: 0.470 | Acc: 83.564% (14226/17024)\n",
      "Train: Loss: 0.470 | Acc: 83.570% (14334/17152)\n",
      "Train: Loss: 0.471 | Acc: 83.524% (14433/17280)\n",
      "Train: Loss: 0.472 | Acc: 83.508% (14537/17408)\n",
      "Train: Loss: 0.473 | Acc: 83.468% (14637/17536)\n",
      "Train: Loss: 0.472 | Acc: 83.526% (14754/17664)\n",
      "Train: Loss: 0.472 | Acc: 83.487% (14854/17792)\n",
      "Train: Loss: 0.472 | Acc: 83.471% (14958/17920)\n",
      "Train: Loss: 0.472 | Acc: 83.477% (15066/18048)\n",
      "Train: Loss: 0.472 | Acc: 83.500% (15177/18176)\n",
      "Train: Loss: 0.471 | Acc: 83.556% (15294/18304)\n",
      "Train: Loss: 0.471 | Acc: 83.556% (15401/18432)\n",
      "Train: Loss: 0.471 | Acc: 83.534% (15504/18560)\n",
      "Train: Loss: 0.471 | Acc: 83.519% (15608/18688)\n",
      "Train: Loss: 0.471 | Acc: 83.519% (15715/18816)\n",
      "Train: Loss: 0.471 | Acc: 83.509% (15820/18944)\n",
      "Train: Loss: 0.472 | Acc: 83.499% (15925/19072)\n",
      "Train: Loss: 0.471 | Acc: 83.516% (16035/19200)\n",
      "Train: Loss: 0.470 | Acc: 83.542% (16147/19328)\n",
      "Train: Loss: 0.470 | Acc: 83.563% (16258/19456)\n",
      "Train: Loss: 0.471 | Acc: 83.548% (16362/19584)\n",
      "Train: Loss: 0.471 | Acc: 83.543% (16468/19712)\n",
      "Train: Loss: 0.470 | Acc: 83.569% (16580/19840)\n",
      "Train: Loss: 0.471 | Acc: 83.549% (16683/19968)\n",
      "Train: Loss: 0.470 | Acc: 83.574% (16795/20096)\n",
      "Train: Loss: 0.470 | Acc: 83.569% (16901/20224)\n",
      "Train: Loss: 0.469 | Acc: 83.594% (17013/20352)\n",
      "Train: Loss: 0.470 | Acc: 83.599% (17121/20480)\n",
      "Train: Loss: 0.470 | Acc: 83.584% (17225/20608)\n",
      "Train: Loss: 0.471 | Acc: 83.560% (17327/20736)\n",
      "Train: Loss: 0.471 | Acc: 83.531% (17428/20864)\n",
      "Train: Loss: 0.471 | Acc: 83.518% (17532/20992)\n",
      "Train: Loss: 0.473 | Acc: 83.471% (17629/21120)\n",
      "Train: Loss: 0.473 | Acc: 83.462% (17734/21248)\n",
      "Train: Loss: 0.474 | Acc: 83.444% (17837/21376)\n",
      "Train: Loss: 0.474 | Acc: 83.450% (17945/21504)\n",
      "Train: Loss: 0.475 | Acc: 83.446% (18051/21632)\n",
      "Train: Loss: 0.474 | Acc: 83.479% (18165/21760)\n",
      "Train: Loss: 0.474 | Acc: 83.475% (18271/21888)\n",
      "Train: Loss: 0.474 | Acc: 83.498% (18383/22016)\n",
      "Train: Loss: 0.475 | Acc: 83.467% (18483/22144)\n",
      "Train: Loss: 0.475 | Acc: 83.464% (18589/22272)\n",
      "Train: Loss: 0.475 | Acc: 83.442% (18691/22400)\n",
      "Train: Loss: 0.475 | Acc: 83.430% (18795/22528)\n",
      "Train: Loss: 0.476 | Acc: 83.400% (18895/22656)\n",
      "Train: Loss: 0.477 | Acc: 83.409% (19004/22784)\n",
      "Train: Loss: 0.477 | Acc: 83.402% (19109/22912)\n",
      "Train: Loss: 0.477 | Acc: 83.372% (19209/23040)\n",
      "Train: Loss: 0.477 | Acc: 83.352% (19311/23168)\n",
      "Train: Loss: 0.477 | Acc: 83.362% (19420/23296)\n",
      "Train: Loss: 0.477 | Acc: 83.350% (19524/23424)\n",
      "Train: Loss: 0.477 | Acc: 83.343% (19629/23552)\n",
      "Train: Loss: 0.478 | Acc: 83.332% (19733/23680)\n",
      "Train: Loss: 0.478 | Acc: 83.329% (19839/23808)\n",
      "Train: Loss: 0.479 | Acc: 83.276% (19933/23936)\n",
      "Train: Loss: 0.479 | Acc: 83.286% (20042/24064)\n",
      "Train: Loss: 0.479 | Acc: 83.300% (20152/24192)\n",
      "Train: Loss: 0.479 | Acc: 83.289% (20256/24320)\n",
      "Train: Loss: 0.479 | Acc: 83.291% (20363/24448)\n",
      "Train: Loss: 0.480 | Acc: 83.252% (20460/24576)\n",
      "Train: Loss: 0.480 | Acc: 83.258% (20568/24704)\n",
      "Train: Loss: 0.480 | Acc: 83.260% (20675/24832)\n",
      "Train: Loss: 0.480 | Acc: 83.249% (20779/24960)\n",
      "Train: Loss: 0.480 | Acc: 83.247% (20885/25088)\n",
      "Train: Loss: 0.481 | Acc: 83.233% (20988/25216)\n",
      "Train: Loss: 0.481 | Acc: 83.211% (21089/25344)\n",
      "Train: Loss: 0.481 | Acc: 83.209% (21195/25472)\n",
      "Train: Loss: 0.481 | Acc: 83.215% (21303/25600)\n",
      "Train: Loss: 0.481 | Acc: 83.221% (21411/25728)\n",
      "Train: Loss: 0.482 | Acc: 83.192% (21510/25856)\n",
      "Train: Loss: 0.482 | Acc: 83.178% (21613/25984)\n",
      "Train: Loss: 0.482 | Acc: 83.188% (21722/26112)\n",
      "Train: Loss: 0.481 | Acc: 83.197% (21831/26240)\n",
      "Train: Loss: 0.481 | Acc: 83.203% (21939/26368)\n",
      "Train: Loss: 0.481 | Acc: 83.190% (22042/26496)\n",
      "Train: Loss: 0.482 | Acc: 83.173% (22144/26624)\n",
      "Train: Loss: 0.482 | Acc: 83.175% (22251/26752)\n",
      "Train: Loss: 0.482 | Acc: 83.162% (22354/26880)\n",
      "Train: Loss: 0.482 | Acc: 83.161% (22460/27008)\n",
      "Train: Loss: 0.482 | Acc: 83.155% (22565/27136)\n",
      "Train: Loss: 0.483 | Acc: 83.135% (22666/27264)\n",
      "Train: Loss: 0.483 | Acc: 83.130% (22771/27392)\n",
      "Train: Loss: 0.483 | Acc: 83.107% (22871/27520)\n",
      "Train: Loss: 0.484 | Acc: 83.091% (22973/27648)\n",
      "Train: Loss: 0.484 | Acc: 83.083% (23077/27776)\n",
      "Train: Loss: 0.484 | Acc: 83.085% (23184/27904)\n",
      "Train: Loss: 0.484 | Acc: 83.073% (23287/28032)\n",
      "Train: Loss: 0.483 | Acc: 83.082% (23396/28160)\n",
      "Train: Loss: 0.483 | Acc: 83.106% (23509/28288)\n",
      "Train: Loss: 0.483 | Acc: 83.108% (23616/28416)\n",
      "Train: Loss: 0.483 | Acc: 83.121% (23726/28544)\n",
      "Train: Loss: 0.483 | Acc: 83.098% (23826/28672)\n",
      "Train: Loss: 0.483 | Acc: 83.108% (23935/28800)\n",
      "Train: Loss: 0.483 | Acc: 83.127% (24047/28928)\n",
      "Train: Loss: 0.482 | Acc: 83.157% (24162/29056)\n",
      "Train: Loss: 0.482 | Acc: 83.159% (24269/29184)\n",
      "Train: Loss: 0.482 | Acc: 83.167% (24378/29312)\n",
      "Train: Loss: 0.482 | Acc: 83.183% (24489/29440)\n",
      "Train: Loss: 0.482 | Acc: 83.178% (24594/29568)\n",
      "Train: Loss: 0.482 | Acc: 83.200% (24707/29696)\n",
      "Train: Loss: 0.482 | Acc: 83.188% (24810/29824)\n",
      "Train: Loss: 0.481 | Acc: 83.203% (24921/29952)\n",
      "Train: Loss: 0.482 | Acc: 83.198% (25026/30080)\n",
      "Train: Loss: 0.481 | Acc: 83.206% (25135/30208)\n",
      "Train: Loss: 0.481 | Acc: 83.201% (25240/30336)\n",
      "Train: Loss: 0.481 | Acc: 83.213% (25350/30464)\n",
      "Train: Loss: 0.482 | Acc: 83.185% (25448/30592)\n",
      "Train: Loss: 0.482 | Acc: 83.167% (25549/30720)\n",
      "Train: Loss: 0.482 | Acc: 83.189% (25662/30848)\n",
      "Train: Loss: 0.482 | Acc: 83.177% (25765/30976)\n",
      "Train: Loss: 0.483 | Acc: 83.147% (25862/31104)\n",
      "Train: Loss: 0.483 | Acc: 83.129% (25963/31232)\n",
      "Train: Loss: 0.483 | Acc: 83.119% (26066/31360)\n",
      "Train: Loss: 0.483 | Acc: 83.124% (26174/31488)\n",
      "Train: Loss: 0.483 | Acc: 83.122% (26280/31616)\n",
      "Train: Loss: 0.483 | Acc: 83.115% (26384/31744)\n",
      "Train: Loss: 0.484 | Acc: 83.104% (26487/31872)\n",
      "Train: Loss: 0.484 | Acc: 83.097% (26591/32000)\n",
      "Train: Loss: 0.483 | Acc: 83.118% (26704/32128)\n",
      "Train: Loss: 0.484 | Acc: 83.104% (26806/32256)\n",
      "Train: Loss: 0.484 | Acc: 83.097% (26910/32384)\n",
      "Train: Loss: 0.484 | Acc: 83.092% (27015/32512)\n",
      "Train: Loss: 0.484 | Acc: 83.088% (27120/32640)\n",
      "Train: Loss: 0.484 | Acc: 83.093% (27228/32768)\n",
      "Train: Loss: 0.484 | Acc: 83.077% (27329/32896)\n",
      "Train: Loss: 0.485 | Acc: 83.070% (27433/33024)\n",
      "Train: Loss: 0.485 | Acc: 83.054% (27534/33152)\n",
      "Train: Loss: 0.484 | Acc: 83.068% (27645/33280)\n",
      "Train: Loss: 0.484 | Acc: 83.064% (27750/33408)\n",
      "Train: Loss: 0.484 | Acc: 83.069% (27858/33536)\n",
      "Train: Loss: 0.484 | Acc: 83.059% (27961/33664)\n",
      "Train: Loss: 0.484 | Acc: 83.040% (28061/33792)\n",
      "Train: Loss: 0.484 | Acc: 83.051% (28171/33920)\n",
      "Train: Loss: 0.485 | Acc: 83.030% (28270/34048)\n",
      "Train: Loss: 0.484 | Acc: 83.029% (28376/34176)\n",
      "Train: Loss: 0.484 | Acc: 83.028% (28482/34304)\n",
      "Train: Loss: 0.484 | Acc: 83.042% (28593/34432)\n",
      "Train: Loss: 0.484 | Acc: 83.050% (28702/34560)\n",
      "Train: Loss: 0.485 | Acc: 83.020% (28798/34688)\n",
      "Train: Loss: 0.484 | Acc: 83.048% (28914/34816)\n",
      "Train: Loss: 0.484 | Acc: 83.053% (29022/34944)\n",
      "Train: Loss: 0.484 | Acc: 83.055% (29129/35072)\n",
      "Train: Loss: 0.484 | Acc: 83.077% (29243/35200)\n",
      "Train: Loss: 0.484 | Acc: 83.062% (29344/35328)\n",
      "Train: Loss: 0.484 | Acc: 83.052% (29447/35456)\n",
      "Train: Loss: 0.485 | Acc: 83.051% (29553/35584)\n",
      "Train: Loss: 0.484 | Acc: 83.048% (29658/35712)\n",
      "Train: Loss: 0.484 | Acc: 83.050% (29765/35840)\n",
      "Train: Loss: 0.484 | Acc: 83.057% (29874/35968)\n",
      "Train: Loss: 0.485 | Acc: 83.040% (29974/36096)\n",
      "Train: Loss: 0.485 | Acc: 83.050% (30084/36224)\n",
      "Train: Loss: 0.485 | Acc: 83.027% (30182/36352)\n",
      "Train: Loss: 0.485 | Acc: 83.024% (30287/36480)\n",
      "Train: Loss: 0.485 | Acc: 83.034% (30397/36608)\n",
      "Train: Loss: 0.485 | Acc: 83.041% (30506/36736)\n",
      "Train: Loss: 0.485 | Acc: 83.051% (30616/36864)\n",
      "Train: Loss: 0.485 | Acc: 83.058% (30725/36992)\n",
      "Train: Loss: 0.485 | Acc: 83.047% (30827/37120)\n",
      "Train: Loss: 0.485 | Acc: 83.051% (30935/37248)\n",
      "Train: Loss: 0.485 | Acc: 83.027% (31032/37376)\n",
      "Train: Loss: 0.485 | Acc: 83.023% (31137/37504)\n",
      "Train: Loss: 0.486 | Acc: 83.020% (31242/37632)\n",
      "Train: Loss: 0.485 | Acc: 83.024% (31350/37760)\n",
      "Train: Loss: 0.485 | Acc: 83.029% (31458/37888)\n",
      "Train: Loss: 0.485 | Acc: 83.020% (31561/38016)\n",
      "Train: Loss: 0.485 | Acc: 83.027% (31670/38144)\n",
      "Train: Loss: 0.484 | Acc: 83.053% (31786/38272)\n",
      "Train: Loss: 0.484 | Acc: 83.065% (31897/38400)\n",
      "Train: Loss: 0.484 | Acc: 83.069% (32005/38528)\n",
      "Train: Loss: 0.484 | Acc: 83.082% (32116/38656)\n",
      "Train: Loss: 0.484 | Acc: 83.086% (32224/38784)\n",
      "Train: Loss: 0.484 | Acc: 83.090% (32332/38912)\n",
      "Train: Loss: 0.484 | Acc: 83.099% (32442/39040)\n",
      "Train: Loss: 0.484 | Acc: 83.106% (32551/39168)\n",
      "Train: Loss: 0.483 | Acc: 83.108% (32658/39296)\n",
      "Train: Loss: 0.484 | Acc: 83.092% (32758/39424)\n",
      "Train: Loss: 0.484 | Acc: 83.098% (32867/39552)\n",
      "Train: Loss: 0.484 | Acc: 83.097% (32973/39680)\n",
      "Train: Loss: 0.484 | Acc: 83.096% (33079/39808)\n",
      "Train: Loss: 0.484 | Acc: 83.098% (33186/39936)\n",
      "Train: Loss: 0.483 | Acc: 83.115% (33299/40064)\n",
      "Train: Loss: 0.484 | Acc: 83.111% (33404/40192)\n",
      "Train: Loss: 0.484 | Acc: 83.103% (33507/40320)\n",
      "Train: Loss: 0.484 | Acc: 83.102% (33613/40448)\n",
      "Train: Loss: 0.484 | Acc: 83.108% (33722/40576)\n",
      "Train: Loss: 0.484 | Acc: 83.095% (33823/40704)\n",
      "Train: Loss: 0.484 | Acc: 83.087% (33926/40832)\n",
      "Train: Loss: 0.484 | Acc: 83.101% (34038/40960)\n",
      "Train: Loss: 0.484 | Acc: 83.112% (34149/41088)\n",
      "Train: Loss: 0.484 | Acc: 83.116% (34257/41216)\n",
      "Train: Loss: 0.485 | Acc: 83.100% (34357/41344)\n",
      "Train: Loss: 0.485 | Acc: 83.078% (34454/41472)\n",
      "Train: Loss: 0.485 | Acc: 83.070% (34557/41600)\n",
      "Train: Loss: 0.485 | Acc: 83.062% (34660/41728)\n",
      "Train: Loss: 0.486 | Acc: 83.044% (34759/41856)\n",
      "Train: Loss: 0.486 | Acc: 83.041% (34864/41984)\n",
      "Train: Loss: 0.486 | Acc: 83.033% (34967/42112)\n",
      "Train: Loss: 0.486 | Acc: 83.030% (35072/42240)\n",
      "Train: Loss: 0.486 | Acc: 83.025% (35176/42368)\n",
      "Train: Loss: 0.487 | Acc: 83.015% (35278/42496)\n",
      "Train: Loss: 0.487 | Acc: 83.005% (35380/42624)\n",
      "Train: Loss: 0.487 | Acc: 83.004% (35486/42752)\n",
      "Train: Loss: 0.486 | Acc: 83.004% (35592/42880)\n",
      "Train: Loss: 0.486 | Acc: 83.005% (35699/43008)\n",
      "Train: Loss: 0.486 | Acc: 83.005% (35805/43136)\n",
      "Train: Loss: 0.486 | Acc: 83.014% (35915/43264)\n",
      "Train: Loss: 0.486 | Acc: 83.025% (36026/43392)\n",
      "Train: Loss: 0.486 | Acc: 83.038% (36138/43520)\n",
      "Train: Loss: 0.486 | Acc: 83.032% (36242/43648)\n",
      "Train: Loss: 0.486 | Acc: 83.027% (36346/43776)\n",
      "Train: Loss: 0.486 | Acc: 83.017% (36448/43904)\n",
      "Train: Loss: 0.486 | Acc: 83.006% (36549/44032)\n",
      "Train: Loss: 0.486 | Acc: 83.003% (36654/44160)\n",
      "Train: Loss: 0.486 | Acc: 82.993% (36756/44288)\n",
      "Train: Loss: 0.486 | Acc: 83.002% (36866/44416)\n",
      "Train: Loss: 0.486 | Acc: 83.003% (36973/44544)\n",
      "Train: Loss: 0.486 | Acc: 83.003% (37079/44672)\n",
      "Train: Loss: 0.486 | Acc: 83.007% (37187/44800)\n",
      "Train: Loss: 0.486 | Acc: 83.013% (37296/44928)\n",
      "Train: Loss: 0.486 | Acc: 83.008% (37400/45056)\n",
      "Train: Loss: 0.486 | Acc: 83.009% (37507/45184)\n",
      "Train: Loss: 0.486 | Acc: 83.009% (37613/45312)\n",
      "Train: Loss: 0.486 | Acc: 83.015% (37722/45440)\n",
      "Train: Loss: 0.487 | Acc: 83.010% (37826/45568)\n",
      "Train: Loss: 0.486 | Acc: 83.029% (37941/45696)\n",
      "Train: Loss: 0.487 | Acc: 83.013% (38040/45824)\n",
      "Train: Loss: 0.487 | Acc: 83.028% (38153/45952)\n",
      "Train: Loss: 0.486 | Acc: 83.038% (38264/46080)\n",
      "Train: Loss: 0.486 | Acc: 83.038% (38370/46208)\n",
      "Train: Loss: 0.486 | Acc: 83.052% (38483/46336)\n",
      "Train: Loss: 0.486 | Acc: 83.043% (38585/46464)\n",
      "Train: Loss: 0.486 | Acc: 83.049% (38694/46592)\n",
      "Train: Loss: 0.486 | Acc: 83.046% (38799/46720)\n",
      "Train: Loss: 0.486 | Acc: 83.041% (38903/46848)\n",
      "Train: Loss: 0.486 | Acc: 83.062% (39019/46976)\n",
      "Train: Loss: 0.486 | Acc: 83.065% (39127/47104)\n",
      "Train: Loss: 0.486 | Acc: 83.064% (39233/47232)\n",
      "Train: Loss: 0.486 | Acc: 83.057% (39336/47360)\n",
      "Train: Loss: 0.485 | Acc: 83.069% (39448/47488)\n",
      "Train: Loss: 0.485 | Acc: 83.081% (39560/47616)\n",
      "Train: Loss: 0.485 | Acc: 83.076% (39664/47744)\n",
      "Train: Loss: 0.486 | Acc: 83.067% (39766/47872)\n",
      "Train: Loss: 0.485 | Acc: 83.075% (39876/48000)\n",
      "Train: Loss: 0.486 | Acc: 83.058% (39974/48128)\n",
      "Train: Loss: 0.485 | Acc: 83.074% (40088/48256)\n",
      "Train: Loss: 0.486 | Acc: 83.075% (40195/48384)\n",
      "Train: Loss: 0.486 | Acc: 83.078% (40303/48512)\n",
      "Train: Loss: 0.486 | Acc: 83.080% (40410/48640)\n",
      "Train: Loss: 0.485 | Acc: 83.083% (40518/48768)\n",
      "Train: Loss: 0.485 | Acc: 83.093% (40629/48896)\n",
      "Train: Loss: 0.485 | Acc: 83.092% (40735/49024)\n",
      "Train: Loss: 0.485 | Acc: 83.079% (40835/49152)\n",
      "Train: Loss: 0.485 | Acc: 83.080% (40942/49280)\n",
      "Train: Loss: 0.486 | Acc: 83.065% (41041/49408)\n",
      "Train: Loss: 0.485 | Acc: 83.067% (41148/49536)\n",
      "Train: Loss: 0.485 | Acc: 83.066% (41254/49664)\n",
      "Train: Loss: 0.485 | Acc: 83.068% (41361/49792)\n",
      "Train: Loss: 0.485 | Acc: 83.073% (41470/49920)\n",
      "Train: Loss: 0.485 | Acc: 83.062% (41531/50000)\n",
      "Test: Loss: 0.708 | Acc: 80.000% (80/100)\n",
      "Test: Loss: 0.599 | Acc: 80.000% (160/200)\n",
      "Test: Loss: 0.593 | Acc: 79.667% (239/300)\n",
      "Test: Loss: 0.578 | Acc: 80.000% (320/400)\n",
      "Test: Loss: 0.569 | Acc: 80.000% (400/500)\n",
      "Test: Loss: 0.527 | Acc: 81.333% (488/600)\n",
      "Test: Loss: 0.526 | Acc: 81.286% (569/700)\n",
      "Test: Loss: 0.542 | Acc: 80.625% (645/800)\n",
      "Test: Loss: 0.550 | Acc: 80.000% (720/900)\n",
      "Test: Loss: 0.556 | Acc: 79.800% (798/1000)\n",
      "Test: Loss: 0.553 | Acc: 80.000% (880/1100)\n",
      "Test: Loss: 0.560 | Acc: 79.750% (957/1200)\n",
      "Test: Loss: 0.548 | Acc: 80.000% (1040/1300)\n",
      "Test: Loss: 0.546 | Acc: 79.929% (1119/1400)\n",
      "Test: Loss: 0.543 | Acc: 80.200% (1203/1500)\n",
      "Test: Loss: 0.548 | Acc: 80.125% (1282/1600)\n",
      "Test: Loss: 0.555 | Acc: 80.176% (1363/1700)\n",
      "Test: Loss: 0.564 | Acc: 79.944% (1439/1800)\n",
      "Test: Loss: 0.570 | Acc: 79.947% (1519/1900)\n",
      "Test: Loss: 0.581 | Acc: 79.800% (1596/2000)\n",
      "Test: Loss: 0.584 | Acc: 79.333% (1666/2100)\n",
      "Test: Loss: 0.588 | Acc: 79.273% (1744/2200)\n",
      "Test: Loss: 0.592 | Acc: 79.174% (1821/2300)\n",
      "Test: Loss: 0.597 | Acc: 79.167% (1900/2400)\n",
      "Test: Loss: 0.599 | Acc: 79.160% (1979/2500)\n",
      "Test: Loss: 0.605 | Acc: 79.115% (2057/2600)\n",
      "Test: Loss: 0.600 | Acc: 79.370% (2143/2700)\n",
      "Test: Loss: 0.601 | Acc: 79.286% (2220/2800)\n",
      "Test: Loss: 0.601 | Acc: 79.276% (2299/2900)\n",
      "Test: Loss: 0.602 | Acc: 79.333% (2380/3000)\n",
      "Test: Loss: 0.602 | Acc: 79.355% (2460/3100)\n",
      "Test: Loss: 0.600 | Acc: 79.312% (2538/3200)\n",
      "Test: Loss: 0.600 | Acc: 79.273% (2616/3300)\n",
      "Test: Loss: 0.602 | Acc: 79.059% (2688/3400)\n",
      "Test: Loss: 0.606 | Acc: 78.914% (2762/3500)\n",
      "Test: Loss: 0.610 | Acc: 78.917% (2841/3600)\n",
      "Test: Loss: 0.613 | Acc: 78.838% (2917/3700)\n",
      "Test: Loss: 0.614 | Acc: 78.842% (2996/3800)\n",
      "Test: Loss: 0.613 | Acc: 78.923% (3078/3900)\n",
      "Test: Loss: 0.607 | Acc: 79.100% (3164/4000)\n",
      "Test: Loss: 0.610 | Acc: 79.000% (3239/4100)\n",
      "Test: Loss: 0.608 | Acc: 79.119% (3323/4200)\n",
      "Test: Loss: 0.604 | Acc: 79.256% (3408/4300)\n",
      "Test: Loss: 0.603 | Acc: 79.364% (3492/4400)\n",
      "Test: Loss: 0.606 | Acc: 79.289% (3568/4500)\n",
      "Test: Loss: 0.608 | Acc: 79.152% (3641/4600)\n",
      "Test: Loss: 0.607 | Acc: 79.149% (3720/4700)\n",
      "Test: Loss: 0.607 | Acc: 79.125% (3798/4800)\n",
      "Test: Loss: 0.605 | Acc: 79.224% (3882/4900)\n",
      "Test: Loss: 0.608 | Acc: 79.100% (3955/5000)\n",
      "Test: Loss: 0.608 | Acc: 79.137% (4036/5100)\n",
      "Test: Loss: 0.608 | Acc: 79.192% (4118/5200)\n",
      "Test: Loss: 0.605 | Acc: 79.302% (4203/5300)\n",
      "Test: Loss: 0.605 | Acc: 79.296% (4282/5400)\n",
      "Test: Loss: 0.602 | Acc: 79.382% (4366/5500)\n",
      "Test: Loss: 0.602 | Acc: 79.446% (4449/5600)\n",
      "Test: Loss: 0.603 | Acc: 79.386% (4525/5700)\n",
      "Test: Loss: 0.601 | Acc: 79.500% (4611/5800)\n",
      "Test: Loss: 0.604 | Acc: 79.407% (4685/5900)\n",
      "Test: Loss: 0.604 | Acc: 79.350% (4761/6000)\n",
      "Test: Loss: 0.603 | Acc: 79.410% (4844/6100)\n",
      "Test: Loss: 0.602 | Acc: 79.403% (4923/6200)\n",
      "Test: Loss: 0.601 | Acc: 79.444% (5005/6300)\n",
      "Test: Loss: 0.597 | Acc: 79.531% (5090/6400)\n",
      "Test: Loss: 0.598 | Acc: 79.523% (5169/6500)\n",
      "Test: Loss: 0.600 | Acc: 79.409% (5241/6600)\n",
      "Test: Loss: 0.598 | Acc: 79.478% (5325/6700)\n",
      "Test: Loss: 0.598 | Acc: 79.456% (5403/6800)\n",
      "Test: Loss: 0.601 | Acc: 79.304% (5472/6900)\n",
      "Test: Loss: 0.603 | Acc: 79.229% (5546/7000)\n",
      "Test: Loss: 0.603 | Acc: 79.254% (5627/7100)\n",
      "Test: Loss: 0.604 | Acc: 79.236% (5705/7200)\n",
      "Test: Loss: 0.603 | Acc: 79.233% (5784/7300)\n",
      "Test: Loss: 0.603 | Acc: 79.243% (5864/7400)\n",
      "Test: Loss: 0.604 | Acc: 79.280% (5946/7500)\n",
      "Test: Loss: 0.603 | Acc: 79.329% (6029/7600)\n",
      "Test: Loss: 0.605 | Acc: 79.286% (6105/7700)\n",
      "Test: Loss: 0.605 | Acc: 79.231% (6180/7800)\n",
      "Test: Loss: 0.607 | Acc: 79.190% (6256/7900)\n",
      "Test: Loss: 0.608 | Acc: 79.125% (6330/8000)\n",
      "Test: Loss: 0.608 | Acc: 79.160% (6412/8100)\n",
      "Test: Loss: 0.608 | Acc: 79.146% (6490/8200)\n",
      "Test: Loss: 0.607 | Acc: 79.145% (6569/8300)\n",
      "Test: Loss: 0.610 | Acc: 79.071% (6642/8400)\n",
      "Test: Loss: 0.612 | Acc: 78.976% (6713/8500)\n",
      "Test: Loss: 0.613 | Acc: 78.942% (6789/8600)\n",
      "Test: Loss: 0.613 | Acc: 78.989% (6872/8700)\n",
      "Test: Loss: 0.614 | Acc: 78.977% (6950/8800)\n",
      "Test: Loss: 0.613 | Acc: 79.011% (7032/8900)\n",
      "Test: Loss: 0.614 | Acc: 78.978% (7108/9000)\n",
      "Test: Loss: 0.613 | Acc: 79.022% (7191/9100)\n",
      "Test: Loss: 0.612 | Acc: 79.054% (7273/9200)\n",
      "Test: Loss: 0.613 | Acc: 79.000% (7347/9300)\n",
      "Test: Loss: 0.613 | Acc: 78.968% (7423/9400)\n",
      "Test: Loss: 0.614 | Acc: 78.926% (7498/9500)\n",
      "Test: Loss: 0.612 | Acc: 78.958% (7580/9600)\n",
      "Test: Loss: 0.612 | Acc: 79.000% (7663/9700)\n",
      "Test: Loss: 0.613 | Acc: 78.969% (7739/9800)\n",
      "Test: Loss: 0.614 | Acc: 78.929% (7814/9900)\n",
      "Test: Loss: 0.614 | Acc: 78.930% (7893/10000)\n",
      "Train: Loss: 0.374 | Acc: 85.938% (110/128)\n",
      "Train: Loss: 0.376 | Acc: 87.109% (223/256)\n",
      "Train: Loss: 0.398 | Acc: 86.719% (333/384)\n",
      "Train: Loss: 0.365 | Acc: 88.281% (452/512)\n",
      "Train: Loss: 0.379 | Acc: 87.500% (560/640)\n",
      "Train: Loss: 0.403 | Acc: 86.719% (666/768)\n",
      "Train: Loss: 0.414 | Acc: 85.826% (769/896)\n",
      "Train: Loss: 0.407 | Acc: 85.938% (880/1024)\n",
      "Train: Loss: 0.404 | Acc: 86.285% (994/1152)\n",
      "Train: Loss: 0.396 | Acc: 86.641% (1109/1280)\n",
      "Train: Loss: 0.385 | Acc: 86.790% (1222/1408)\n",
      "Train: Loss: 0.379 | Acc: 86.914% (1335/1536)\n",
      "Train: Loss: 0.371 | Acc: 87.079% (1449/1664)\n",
      "Train: Loss: 0.383 | Acc: 86.775% (1555/1792)\n",
      "Train: Loss: 0.387 | Acc: 86.615% (1663/1920)\n",
      "Train: Loss: 0.380 | Acc: 86.914% (1780/2048)\n",
      "Train: Loss: 0.382 | Acc: 86.903% (1891/2176)\n",
      "Train: Loss: 0.379 | Acc: 87.153% (2008/2304)\n",
      "Train: Loss: 0.378 | Acc: 87.212% (2121/2432)\n",
      "Train: Loss: 0.378 | Acc: 87.344% (2236/2560)\n",
      "Train: Loss: 0.385 | Acc: 86.979% (2338/2688)\n",
      "Train: Loss: 0.387 | Acc: 86.932% (2448/2816)\n",
      "Train: Loss: 0.392 | Acc: 86.753% (2554/2944)\n",
      "Train: Loss: 0.388 | Acc: 86.882% (2669/3072)\n",
      "Train: Loss: 0.387 | Acc: 86.969% (2783/3200)\n",
      "Train: Loss: 0.389 | Acc: 86.929% (2893/3328)\n",
      "Train: Loss: 0.390 | Acc: 86.892% (3003/3456)\n",
      "Train: Loss: 0.392 | Acc: 86.802% (3111/3584)\n",
      "Train: Loss: 0.388 | Acc: 87.015% (3230/3712)\n",
      "Train: Loss: 0.387 | Acc: 86.927% (3338/3840)\n",
      "Train: Loss: 0.387 | Acc: 86.895% (3448/3968)\n",
      "Train: Loss: 0.385 | Acc: 86.890% (3559/4096)\n",
      "Train: Loss: 0.386 | Acc: 86.766% (3665/4224)\n",
      "Train: Loss: 0.383 | Acc: 86.834% (3779/4352)\n",
      "Train: Loss: 0.384 | Acc: 86.808% (3889/4480)\n",
      "Train: Loss: 0.387 | Acc: 86.719% (3996/4608)\n",
      "Train: Loss: 0.388 | Acc: 86.592% (4101/4736)\n",
      "Train: Loss: 0.388 | Acc: 86.657% (4215/4864)\n",
      "Train: Loss: 0.388 | Acc: 86.719% (4329/4992)\n",
      "Train: Loss: 0.388 | Acc: 86.699% (4439/5120)\n",
      "Train: Loss: 0.386 | Acc: 86.833% (4557/5248)\n",
      "Train: Loss: 0.384 | Acc: 86.942% (4674/5376)\n",
      "Train: Loss: 0.386 | Acc: 86.810% (4778/5504)\n",
      "Train: Loss: 0.386 | Acc: 86.790% (4888/5632)\n",
      "Train: Loss: 0.387 | Acc: 86.736% (4996/5760)\n",
      "Train: Loss: 0.387 | Acc: 86.685% (5104/5888)\n",
      "Train: Loss: 0.385 | Acc: 86.769% (5220/6016)\n",
      "Train: Loss: 0.385 | Acc: 86.800% (5333/6144)\n",
      "Train: Loss: 0.385 | Acc: 86.814% (5445/6272)\n",
      "Train: Loss: 0.386 | Acc: 86.734% (5551/6400)\n",
      "Train: Loss: 0.385 | Acc: 86.765% (5664/6528)\n",
      "Train: Loss: 0.385 | Acc: 86.779% (5776/6656)\n",
      "Train: Loss: 0.386 | Acc: 86.689% (5881/6784)\n",
      "Train: Loss: 0.388 | Acc: 86.617% (5987/6912)\n",
      "Train: Loss: 0.388 | Acc: 86.648% (6100/7040)\n",
      "Train: Loss: 0.388 | Acc: 86.691% (6214/7168)\n",
      "Train: Loss: 0.388 | Acc: 86.637% (6321/7296)\n",
      "Train: Loss: 0.388 | Acc: 86.611% (6430/7424)\n",
      "Train: Loss: 0.387 | Acc: 86.639% (6543/7552)\n",
      "Train: Loss: 0.388 | Acc: 86.628% (6653/7680)\n",
      "Train: Loss: 0.388 | Acc: 86.642% (6765/7808)\n",
      "Train: Loss: 0.388 | Acc: 86.605% (6873/7936)\n",
      "Train: Loss: 0.388 | Acc: 86.570% (6981/8064)\n",
      "Train: Loss: 0.393 | Acc: 86.414% (7079/8192)\n",
      "Train: Loss: 0.392 | Acc: 86.454% (7193/8320)\n",
      "Train: Loss: 0.390 | Acc: 86.506% (7308/8448)\n",
      "Train: Loss: 0.390 | Acc: 86.474% (7416/8576)\n",
      "Train: Loss: 0.390 | Acc: 86.455% (7525/8704)\n",
      "Train: Loss: 0.389 | Acc: 86.492% (7639/8832)\n",
      "Train: Loss: 0.388 | Acc: 86.529% (7753/8960)\n",
      "Train: Loss: 0.386 | Acc: 86.631% (7873/9088)\n",
      "Train: Loss: 0.384 | Acc: 86.697% (7990/9216)\n",
      "Train: Loss: 0.383 | Acc: 86.708% (8102/9344)\n",
      "Train: Loss: 0.383 | Acc: 86.729% (8215/9472)\n",
      "Train: Loss: 0.382 | Acc: 86.771% (8330/9600)\n",
      "Train: Loss: 0.383 | Acc: 86.750% (8439/9728)\n",
      "Train: Loss: 0.383 | Acc: 86.769% (8552/9856)\n",
      "Train: Loss: 0.383 | Acc: 86.769% (8663/9984)\n",
      "Train: Loss: 0.384 | Acc: 86.719% (8769/10112)\n",
      "Train: Loss: 0.383 | Acc: 86.768% (8885/10240)\n",
      "Train: Loss: 0.384 | Acc: 86.719% (8991/10368)\n",
      "Train: Loss: 0.384 | Acc: 86.700% (9100/10496)\n",
      "Train: Loss: 0.384 | Acc: 86.700% (9211/10624)\n",
      "Train: Loss: 0.385 | Acc: 86.691% (9321/10752)\n",
      "Train: Loss: 0.385 | Acc: 86.710% (9434/10880)\n",
      "Train: Loss: 0.385 | Acc: 86.764% (9551/11008)\n",
      "Train: Loss: 0.384 | Acc: 86.773% (9663/11136)\n",
      "Train: Loss: 0.384 | Acc: 86.781% (9775/11264)\n",
      "Train: Loss: 0.384 | Acc: 86.780% (9886/11392)\n",
      "Train: Loss: 0.382 | Acc: 86.806% (10000/11520)\n",
      "Train: Loss: 0.382 | Acc: 86.822% (10113/11648)\n",
      "Train: Loss: 0.383 | Acc: 86.770% (10218/11776)\n",
      "Train: Loss: 0.384 | Acc: 86.719% (10323/11904)\n",
      "Train: Loss: 0.384 | Acc: 86.719% (10434/12032)\n",
      "Train: Loss: 0.384 | Acc: 86.719% (10545/12160)\n",
      "Train: Loss: 0.386 | Acc: 86.662% (10649/12288)\n",
      "Train: Loss: 0.386 | Acc: 86.678% (10762/12416)\n",
      "Train: Loss: 0.386 | Acc: 86.679% (10873/12544)\n",
      "Train: Loss: 0.387 | Acc: 86.616% (10976/12672)\n",
      "Train: Loss: 0.386 | Acc: 86.641% (11090/12800)\n",
      "Train: Loss: 0.388 | Acc: 86.564% (11191/12928)\n",
      "Train: Loss: 0.390 | Acc: 86.520% (11296/13056)\n",
      "Train: Loss: 0.389 | Acc: 86.537% (11409/13184)\n",
      "Train: Loss: 0.389 | Acc: 86.553% (11522/13312)\n",
      "Train: Loss: 0.387 | Acc: 86.607% (11640/13440)\n",
      "Train: Loss: 0.388 | Acc: 86.586% (11748/13568)\n",
      "Train: Loss: 0.388 | Acc: 86.602% (11861/13696)\n",
      "Train: Loss: 0.390 | Acc: 86.552% (11965/13824)\n",
      "Train: Loss: 0.389 | Acc: 86.590% (12081/13952)\n",
      "Train: Loss: 0.390 | Acc: 86.577% (12190/14080)\n",
      "Train: Loss: 0.391 | Acc: 86.550% (12297/14208)\n",
      "Train: Loss: 0.391 | Acc: 86.565% (12410/14336)\n",
      "Train: Loss: 0.391 | Acc: 86.580% (12523/14464)\n",
      "Train: Loss: 0.392 | Acc: 86.568% (12632/14592)\n",
      "Train: Loss: 0.392 | Acc: 86.569% (12743/14720)\n",
      "Train: Loss: 0.391 | Acc: 86.557% (12852/14848)\n",
      "Train: Loss: 0.392 | Acc: 86.532% (12959/14976)\n",
      "Train: Loss: 0.392 | Acc: 86.540% (13071/15104)\n",
      "Train: Loss: 0.392 | Acc: 86.535% (13181/15232)\n",
      "Train: Loss: 0.393 | Acc: 86.504% (13287/15360)\n",
      "Train: Loss: 0.394 | Acc: 86.441% (13388/15488)\n",
      "Train: Loss: 0.394 | Acc: 86.450% (13500/15616)\n",
      "Train: Loss: 0.394 | Acc: 86.452% (13611/15744)\n",
      "Train: Loss: 0.395 | Acc: 86.379% (13710/15872)\n",
      "Train: Loss: 0.396 | Acc: 86.350% (13816/16000)\n",
      "Train: Loss: 0.398 | Acc: 86.322% (13922/16128)\n",
      "Train: Loss: 0.399 | Acc: 86.319% (14032/16256)\n",
      "Train: Loss: 0.400 | Acc: 86.298% (14139/16384)\n",
      "Train: Loss: 0.400 | Acc: 86.283% (14247/16512)\n",
      "Train: Loss: 0.402 | Acc: 86.262% (14354/16640)\n",
      "Train: Loss: 0.402 | Acc: 86.248% (14462/16768)\n",
      "Train: Loss: 0.403 | Acc: 86.204% (14565/16896)\n",
      "Train: Loss: 0.404 | Acc: 86.190% (14673/17024)\n",
      "Train: Loss: 0.404 | Acc: 86.171% (14780/17152)\n",
      "Train: Loss: 0.404 | Acc: 86.163% (14889/17280)\n",
      "Train: Loss: 0.404 | Acc: 86.167% (15000/17408)\n",
      "Train: Loss: 0.404 | Acc: 86.166% (15110/17536)\n",
      "Train: Loss: 0.404 | Acc: 86.153% (15218/17664)\n",
      "Train: Loss: 0.404 | Acc: 86.174% (15332/17792)\n",
      "Train: Loss: 0.405 | Acc: 86.166% (15441/17920)\n",
      "Train: Loss: 0.405 | Acc: 86.192% (15556/18048)\n",
      "Train: Loss: 0.405 | Acc: 86.163% (15661/18176)\n",
      "Train: Loss: 0.407 | Acc: 86.107% (15761/18304)\n",
      "Train: Loss: 0.407 | Acc: 86.111% (15872/18432)\n",
      "Train: Loss: 0.407 | Acc: 86.121% (15984/18560)\n",
      "Train: Loss: 0.407 | Acc: 86.146% (16099/18688)\n",
      "Train: Loss: 0.407 | Acc: 86.139% (16208/18816)\n",
      "Train: Loss: 0.407 | Acc: 86.117% (16314/18944)\n",
      "Train: Loss: 0.407 | Acc: 86.116% (16424/19072)\n",
      "Train: Loss: 0.407 | Acc: 86.120% (16535/19200)\n",
      "Train: Loss: 0.407 | Acc: 86.113% (16644/19328)\n",
      "Train: Loss: 0.407 | Acc: 86.133% (16758/19456)\n",
      "Train: Loss: 0.407 | Acc: 86.126% (16867/19584)\n",
      "Train: Loss: 0.408 | Acc: 86.105% (16973/19712)\n",
      "Train: Loss: 0.408 | Acc: 86.089% (17080/19840)\n",
      "Train: Loss: 0.408 | Acc: 86.078% (17188/19968)\n",
      "Train: Loss: 0.408 | Acc: 86.062% (17295/20096)\n",
      "Train: Loss: 0.407 | Acc: 86.091% (17411/20224)\n",
      "Train: Loss: 0.407 | Acc: 86.100% (17523/20352)\n",
      "Train: Loss: 0.407 | Acc: 86.099% (17633/20480)\n",
      "Train: Loss: 0.407 | Acc: 86.073% (17738/20608)\n",
      "Train: Loss: 0.407 | Acc: 86.068% (17847/20736)\n",
      "Train: Loss: 0.407 | Acc: 86.062% (17956/20864)\n",
      "Train: Loss: 0.408 | Acc: 86.042% (18062/20992)\n",
      "Train: Loss: 0.407 | Acc: 86.037% (18171/21120)\n",
      "Train: Loss: 0.407 | Acc: 86.060% (18286/21248)\n",
      "Train: Loss: 0.406 | Acc: 86.083% (18401/21376)\n",
      "Train: Loss: 0.407 | Acc: 86.077% (18510/21504)\n",
      "Train: Loss: 0.406 | Acc: 86.090% (18623/21632)\n",
      "Train: Loss: 0.407 | Acc: 86.089% (18733/21760)\n",
      "Train: Loss: 0.407 | Acc: 86.075% (18840/21888)\n",
      "Train: Loss: 0.408 | Acc: 86.033% (18941/22016)\n",
      "Train: Loss: 0.408 | Acc: 86.019% (19048/22144)\n",
      "Train: Loss: 0.409 | Acc: 85.982% (19150/22272)\n",
      "Train: Loss: 0.410 | Acc: 85.951% (19253/22400)\n",
      "Train: Loss: 0.410 | Acc: 85.960% (19365/22528)\n",
      "Train: Loss: 0.410 | Acc: 85.933% (19469/22656)\n",
      "Train: Loss: 0.410 | Acc: 85.920% (19576/22784)\n",
      "Train: Loss: 0.410 | Acc: 85.907% (19683/22912)\n",
      "Train: Loss: 0.411 | Acc: 85.907% (19793/23040)\n",
      "Train: Loss: 0.411 | Acc: 85.890% (19899/23168)\n",
      "Train: Loss: 0.411 | Acc: 85.865% (20003/23296)\n",
      "Train: Loss: 0.412 | Acc: 85.848% (20109/23424)\n",
      "Train: Loss: 0.412 | Acc: 85.853% (20220/23552)\n",
      "Train: Loss: 0.412 | Acc: 85.840% (20327/23680)\n",
      "Train: Loss: 0.412 | Acc: 85.837% (20436/23808)\n",
      "Train: Loss: 0.412 | Acc: 85.825% (20543/23936)\n",
      "Train: Loss: 0.413 | Acc: 85.792% (20645/24064)\n",
      "Train: Loss: 0.413 | Acc: 85.764% (20748/24192)\n",
      "Train: Loss: 0.413 | Acc: 85.757% (20856/24320)\n",
      "Train: Loss: 0.413 | Acc: 85.745% (20963/24448)\n",
      "Train: Loss: 0.413 | Acc: 85.758% (21076/24576)\n",
      "Train: Loss: 0.413 | Acc: 85.739% (21181/24704)\n",
      "Train: Loss: 0.413 | Acc: 85.736% (21290/24832)\n",
      "Train: Loss: 0.414 | Acc: 85.709% (21393/24960)\n",
      "Train: Loss: 0.414 | Acc: 85.718% (21505/25088)\n",
      "Train: Loss: 0.414 | Acc: 85.696% (21609/25216)\n",
      "Train: Loss: 0.414 | Acc: 85.720% (21725/25344)\n",
      "Train: Loss: 0.414 | Acc: 85.714% (21833/25472)\n",
      "Train: Loss: 0.413 | Acc: 85.727% (21946/25600)\n",
      "Train: Loss: 0.413 | Acc: 85.708% (22051/25728)\n",
      "Train: Loss: 0.414 | Acc: 85.694% (22157/25856)\n",
      "Train: Loss: 0.414 | Acc: 85.683% (22264/25984)\n",
      "Train: Loss: 0.414 | Acc: 85.681% (22373/26112)\n",
      "Train: Loss: 0.414 | Acc: 85.682% (22483/26240)\n",
      "Train: Loss: 0.414 | Acc: 85.657% (22586/26368)\n",
      "Train: Loss: 0.415 | Acc: 85.636% (22690/26496)\n",
      "Train: Loss: 0.415 | Acc: 85.622% (22796/26624)\n",
      "Train: Loss: 0.415 | Acc: 85.605% (22901/26752)\n",
      "Train: Loss: 0.416 | Acc: 85.595% (23008/26880)\n",
      "Train: Loss: 0.416 | Acc: 85.582% (23114/27008)\n",
      "Train: Loss: 0.416 | Acc: 85.573% (23221/27136)\n",
      "Train: Loss: 0.416 | Acc: 85.582% (23333/27264)\n",
      "Train: Loss: 0.417 | Acc: 85.554% (23435/27392)\n",
      "Train: Loss: 0.417 | Acc: 85.534% (23539/27520)\n",
      "Train: Loss: 0.417 | Acc: 85.540% (23650/27648)\n",
      "Train: Loss: 0.417 | Acc: 85.545% (23761/27776)\n",
      "Train: Loss: 0.417 | Acc: 85.547% (23871/27904)\n",
      "Train: Loss: 0.417 | Acc: 85.566% (23986/28032)\n",
      "Train: Loss: 0.416 | Acc: 85.575% (24098/28160)\n",
      "Train: Loss: 0.416 | Acc: 85.573% (24207/28288)\n",
      "Train: Loss: 0.417 | Acc: 85.543% (24308/28416)\n",
      "Train: Loss: 0.417 | Acc: 85.559% (24422/28544)\n",
      "Train: Loss: 0.417 | Acc: 85.550% (24529/28672)\n",
      "Train: Loss: 0.417 | Acc: 85.559% (24641/28800)\n",
      "Train: Loss: 0.417 | Acc: 85.547% (24747/28928)\n",
      "Train: Loss: 0.417 | Acc: 85.528% (24851/29056)\n",
      "Train: Loss: 0.418 | Acc: 85.509% (24955/29184)\n",
      "Train: Loss: 0.418 | Acc: 85.487% (25058/29312)\n",
      "Train: Loss: 0.418 | Acc: 85.489% (25168/29440)\n",
      "Train: Loss: 0.418 | Acc: 85.484% (25276/29568)\n",
      "Train: Loss: 0.418 | Acc: 85.466% (25380/29696)\n",
      "Train: Loss: 0.418 | Acc: 85.441% (25482/29824)\n",
      "Train: Loss: 0.418 | Acc: 85.433% (25589/29952)\n",
      "Train: Loss: 0.418 | Acc: 85.442% (25701/30080)\n",
      "Train: Loss: 0.418 | Acc: 85.454% (25814/30208)\n",
      "Train: Loss: 0.418 | Acc: 85.456% (25924/30336)\n",
      "Train: Loss: 0.418 | Acc: 85.455% (26033/30464)\n",
      "Train: Loss: 0.419 | Acc: 85.441% (26138/30592)\n",
      "Train: Loss: 0.419 | Acc: 85.433% (26245/30720)\n",
      "Train: Loss: 0.419 | Acc: 85.438% (26356/30848)\n",
      "Train: Loss: 0.419 | Acc: 85.434% (26464/30976)\n",
      "Train: Loss: 0.420 | Acc: 85.420% (26569/31104)\n",
      "Train: Loss: 0.420 | Acc: 85.400% (26672/31232)\n",
      "Train: Loss: 0.420 | Acc: 85.402% (26782/31360)\n",
      "Train: Loss: 0.420 | Acc: 85.404% (26892/31488)\n",
      "Train: Loss: 0.420 | Acc: 85.406% (27002/31616)\n",
      "Train: Loss: 0.420 | Acc: 85.396% (27108/31744)\n",
      "Train: Loss: 0.421 | Acc: 85.373% (27210/31872)\n",
      "Train: Loss: 0.420 | Acc: 85.397% (27327/32000)\n",
      "Train: Loss: 0.420 | Acc: 85.393% (27435/32128)\n",
      "Train: Loss: 0.420 | Acc: 85.383% (27541/32256)\n",
      "Train: Loss: 0.421 | Acc: 85.375% (27648/32384)\n",
      "Train: Loss: 0.421 | Acc: 85.359% (27752/32512)\n",
      "Train: Loss: 0.421 | Acc: 85.358% (27861/32640)\n",
      "Train: Loss: 0.421 | Acc: 85.361% (27971/32768)\n",
      "Train: Loss: 0.422 | Acc: 85.336% (28072/32896)\n",
      "Train: Loss: 0.422 | Acc: 85.338% (28182/33024)\n",
      "Train: Loss: 0.422 | Acc: 85.340% (28292/33152)\n",
      "Train: Loss: 0.422 | Acc: 85.337% (28400/33280)\n",
      "Train: Loss: 0.421 | Acc: 85.348% (28513/33408)\n",
      "Train: Loss: 0.422 | Acc: 85.350% (28623/33536)\n",
      "Train: Loss: 0.421 | Acc: 85.364% (28737/33664)\n",
      "Train: Loss: 0.421 | Acc: 85.360% (28845/33792)\n",
      "Train: Loss: 0.421 | Acc: 85.354% (28952/33920)\n",
      "Train: Loss: 0.421 | Acc: 85.356% (29062/34048)\n",
      "Train: Loss: 0.421 | Acc: 85.364% (29174/34176)\n",
      "Train: Loss: 0.421 | Acc: 85.354% (29280/34304)\n",
      "Train: Loss: 0.421 | Acc: 85.348% (29387/34432)\n",
      "Train: Loss: 0.421 | Acc: 85.347% (29496/34560)\n",
      "Train: Loss: 0.422 | Acc: 85.341% (29603/34688)\n",
      "Train: Loss: 0.421 | Acc: 85.349% (29715/34816)\n",
      "Train: Loss: 0.422 | Acc: 85.334% (29819/34944)\n",
      "Train: Loss: 0.422 | Acc: 85.322% (29924/35072)\n",
      "Train: Loss: 0.422 | Acc: 85.312% (30030/35200)\n",
      "Train: Loss: 0.422 | Acc: 85.292% (30132/35328)\n",
      "Train: Loss: 0.423 | Acc: 85.278% (30236/35456)\n",
      "Train: Loss: 0.423 | Acc: 85.277% (30345/35584)\n",
      "Train: Loss: 0.423 | Acc: 85.279% (30455/35712)\n",
      "Train: Loss: 0.423 | Acc: 85.271% (30561/35840)\n",
      "Train: Loss: 0.423 | Acc: 85.273% (30671/35968)\n",
      "Train: Loss: 0.424 | Acc: 85.267% (30778/36096)\n",
      "Train: Loss: 0.424 | Acc: 85.264% (30886/36224)\n",
      "Train: Loss: 0.424 | Acc: 85.269% (30997/36352)\n",
      "Train: Loss: 0.424 | Acc: 85.271% (31107/36480)\n",
      "Train: Loss: 0.424 | Acc: 85.257% (31211/36608)\n",
      "Train: Loss: 0.424 | Acc: 85.257% (31320/36736)\n",
      "Train: Loss: 0.424 | Acc: 85.259% (31430/36864)\n",
      "Train: Loss: 0.424 | Acc: 85.267% (31542/36992)\n",
      "Train: Loss: 0.424 | Acc: 85.261% (31649/37120)\n",
      "Train: Loss: 0.424 | Acc: 85.264% (31759/37248)\n",
      "Train: Loss: 0.424 | Acc: 85.269% (31870/37376)\n",
      "Train: Loss: 0.424 | Acc: 85.255% (31974/37504)\n",
      "Train: Loss: 0.424 | Acc: 85.241% (32078/37632)\n",
      "Train: Loss: 0.424 | Acc: 85.241% (32187/37760)\n",
      "Train: Loss: 0.425 | Acc: 85.230% (32292/37888)\n",
      "Train: Loss: 0.424 | Acc: 85.248% (32408/38016)\n",
      "Train: Loss: 0.424 | Acc: 85.245% (32516/38144)\n",
      "Train: Loss: 0.424 | Acc: 85.248% (32626/38272)\n",
      "Train: Loss: 0.424 | Acc: 85.245% (32734/38400)\n",
      "Train: Loss: 0.424 | Acc: 85.234% (32839/38528)\n",
      "Train: Loss: 0.424 | Acc: 85.229% (32946/38656)\n",
      "Train: Loss: 0.425 | Acc: 85.216% (33050/38784)\n",
      "Train: Loss: 0.425 | Acc: 85.228% (33164/38912)\n",
      "Train: Loss: 0.425 | Acc: 85.233% (33275/39040)\n",
      "Train: Loss: 0.425 | Acc: 85.223% (33380/39168)\n",
      "Train: Loss: 0.425 | Acc: 85.230% (33492/39296)\n",
      "Train: Loss: 0.424 | Acc: 85.242% (33606/39424)\n",
      "Train: Loss: 0.425 | Acc: 85.225% (33708/39552)\n",
      "Train: Loss: 0.424 | Acc: 85.234% (33821/39680)\n",
      "Train: Loss: 0.425 | Acc: 85.217% (33923/39808)\n",
      "Train: Loss: 0.425 | Acc: 85.211% (34030/39936)\n",
      "Train: Loss: 0.426 | Acc: 85.201% (34135/40064)\n",
      "Train: Loss: 0.426 | Acc: 85.211% (34248/40192)\n",
      "Train: Loss: 0.426 | Acc: 85.206% (34355/40320)\n",
      "Train: Loss: 0.426 | Acc: 85.206% (34464/40448)\n",
      "Train: Loss: 0.426 | Acc: 85.198% (34570/40576)\n",
      "Train: Loss: 0.426 | Acc: 85.193% (34677/40704)\n",
      "Train: Loss: 0.427 | Acc: 85.178% (34780/40832)\n",
      "Train: Loss: 0.427 | Acc: 85.176% (34888/40960)\n",
      "Train: Loss: 0.427 | Acc: 85.181% (34999/41088)\n",
      "Train: Loss: 0.427 | Acc: 85.185% (35110/41216)\n",
      "Train: Loss: 0.427 | Acc: 85.164% (35210/41344)\n",
      "Train: Loss: 0.428 | Acc: 85.161% (35318/41472)\n",
      "Train: Loss: 0.428 | Acc: 85.161% (35427/41600)\n",
      "Train: Loss: 0.428 | Acc: 85.161% (35536/41728)\n",
      "Train: Loss: 0.428 | Acc: 85.171% (35649/41856)\n",
      "Train: Loss: 0.427 | Acc: 85.185% (35764/41984)\n",
      "Train: Loss: 0.427 | Acc: 85.187% (35874/42112)\n",
      "Train: Loss: 0.427 | Acc: 85.196% (35987/42240)\n",
      "Train: Loss: 0.427 | Acc: 85.192% (36094/42368)\n",
      "Train: Loss: 0.427 | Acc: 85.192% (36203/42496)\n",
      "Train: Loss: 0.427 | Acc: 85.184% (36309/42624)\n",
      "Train: Loss: 0.426 | Acc: 85.198% (36424/42752)\n",
      "Train: Loss: 0.426 | Acc: 85.194% (36531/42880)\n",
      "Train: Loss: 0.427 | Acc: 85.196% (36641/43008)\n",
      "Train: Loss: 0.426 | Acc: 85.205% (36754/43136)\n",
      "Train: Loss: 0.426 | Acc: 85.209% (36865/43264)\n",
      "Train: Loss: 0.426 | Acc: 85.207% (36973/43392)\n",
      "Train: Loss: 0.426 | Acc: 85.218% (37087/43520)\n",
      "Train: Loss: 0.426 | Acc: 85.216% (37195/43648)\n",
      "Train: Loss: 0.426 | Acc: 85.216% (37304/43776)\n",
      "Train: Loss: 0.426 | Acc: 85.222% (37416/43904)\n",
      "Train: Loss: 0.426 | Acc: 85.227% (37527/44032)\n",
      "Train: Loss: 0.426 | Acc: 85.211% (37629/44160)\n",
      "Train: Loss: 0.426 | Acc: 85.210% (37738/44288)\n",
      "Train: Loss: 0.427 | Acc: 85.210% (37847/44416)\n",
      "Train: Loss: 0.427 | Acc: 85.208% (37955/44544)\n",
      "Train: Loss: 0.427 | Acc: 85.199% (38060/44672)\n",
      "Train: Loss: 0.427 | Acc: 85.172% (38157/44800)\n",
      "Train: Loss: 0.427 | Acc: 85.176% (38268/44928)\n",
      "Train: Loss: 0.428 | Acc: 85.167% (38373/45056)\n",
      "Train: Loss: 0.428 | Acc: 85.152% (38475/45184)\n",
      "Train: Loss: 0.428 | Acc: 85.150% (38583/45312)\n",
      "Train: Loss: 0.428 | Acc: 85.150% (38692/45440)\n",
      "Train: Loss: 0.428 | Acc: 85.147% (38800/45568)\n",
      "Train: Loss: 0.428 | Acc: 85.143% (38907/45696)\n",
      "Train: Loss: 0.428 | Acc: 85.150% (39019/45824)\n",
      "Train: Loss: 0.428 | Acc: 85.148% (39127/45952)\n",
      "Train: Loss: 0.429 | Acc: 85.143% (39234/46080)\n",
      "Train: Loss: 0.429 | Acc: 85.154% (39348/46208)\n",
      "Train: Loss: 0.429 | Acc: 85.141% (39451/46336)\n",
      "Train: Loss: 0.429 | Acc: 85.128% (39554/46464)\n",
      "Train: Loss: 0.429 | Acc: 85.126% (39662/46592)\n",
      "Train: Loss: 0.430 | Acc: 85.122% (39769/46720)\n",
      "Train: Loss: 0.429 | Acc: 85.133% (39883/46848)\n",
      "Train: Loss: 0.429 | Acc: 85.122% (39987/46976)\n",
      "Train: Loss: 0.429 | Acc: 85.129% (40099/47104)\n",
      "Train: Loss: 0.430 | Acc: 85.124% (40206/47232)\n",
      "Train: Loss: 0.430 | Acc: 85.110% (40308/47360)\n",
      "Train: Loss: 0.430 | Acc: 85.104% (40414/47488)\n",
      "Train: Loss: 0.430 | Acc: 85.100% (40521/47616)\n",
      "Train: Loss: 0.431 | Acc: 85.096% (40628/47744)\n",
      "Train: Loss: 0.430 | Acc: 85.104% (40741/47872)\n",
      "Train: Loss: 0.430 | Acc: 85.100% (40848/48000)\n",
      "Train: Loss: 0.430 | Acc: 85.104% (40959/48128)\n",
      "Train: Loss: 0.431 | Acc: 85.088% (41060/48256)\n",
      "Train: Loss: 0.431 | Acc: 85.086% (41168/48384)\n",
      "Train: Loss: 0.431 | Acc: 85.096% (41282/48512)\n",
      "Train: Loss: 0.431 | Acc: 85.090% (41388/48640)\n",
      "Train: Loss: 0.431 | Acc: 85.087% (41495/48768)\n",
      "Train: Loss: 0.430 | Acc: 85.093% (41607/48896)\n",
      "Train: Loss: 0.431 | Acc: 85.087% (41713/49024)\n",
      "Train: Loss: 0.430 | Acc: 85.089% (41823/49152)\n",
      "Train: Loss: 0.431 | Acc: 85.079% (41927/49280)\n",
      "Train: Loss: 0.431 | Acc: 85.079% (42036/49408)\n",
      "Train: Loss: 0.431 | Acc: 85.071% (42141/49536)\n",
      "Train: Loss: 0.431 | Acc: 85.062% (42245/49664)\n",
      "Train: Loss: 0.431 | Acc: 85.064% (42355/49792)\n",
      "Train: Loss: 0.431 | Acc: 85.066% (42465/49920)\n",
      "Train: Loss: 0.430 | Acc: 85.074% (42537/50000)\n",
      "Test: Loss: 0.585 | Acc: 78.000% (78/100)\n",
      "Test: Loss: 0.653 | Acc: 76.000% (152/200)\n",
      "Test: Loss: 0.659 | Acc: 75.667% (227/300)\n",
      "Test: Loss: 0.676 | Acc: 75.500% (302/400)\n",
      "Test: Loss: 0.677 | Acc: 75.600% (378/500)\n",
      "Test: Loss: 0.656 | Acc: 77.000% (462/600)\n",
      "Test: Loss: 0.667 | Acc: 77.000% (539/700)\n",
      "Test: Loss: 0.671 | Acc: 77.000% (616/800)\n",
      "Test: Loss: 0.677 | Acc: 76.556% (689/900)\n",
      "Test: Loss: 0.688 | Acc: 76.800% (768/1000)\n",
      "Test: Loss: 0.683 | Acc: 76.818% (845/1100)\n",
      "Test: Loss: 0.692 | Acc: 76.417% (917/1200)\n",
      "Test: Loss: 0.693 | Acc: 76.538% (995/1300)\n",
      "Test: Loss: 0.689 | Acc: 76.643% (1073/1400)\n",
      "Test: Loss: 0.680 | Acc: 76.600% (1149/1500)\n",
      "Test: Loss: 0.677 | Acc: 76.750% (1228/1600)\n",
      "Test: Loss: 0.671 | Acc: 77.176% (1312/1700)\n",
      "Test: Loss: 0.681 | Acc: 76.944% (1385/1800)\n",
      "Test: Loss: 0.685 | Acc: 76.737% (1458/1900)\n",
      "Test: Loss: 0.704 | Acc: 76.450% (1529/2000)\n",
      "Test: Loss: 0.707 | Acc: 76.286% (1602/2100)\n",
      "Test: Loss: 0.707 | Acc: 76.273% (1678/2200)\n",
      "Test: Loss: 0.712 | Acc: 76.217% (1753/2300)\n",
      "Test: Loss: 0.709 | Acc: 76.250% (1830/2400)\n",
      "Test: Loss: 0.717 | Acc: 76.240% (1906/2500)\n",
      "Test: Loss: 0.732 | Acc: 75.885% (1973/2600)\n",
      "Test: Loss: 0.731 | Acc: 75.926% (2050/2700)\n",
      "Test: Loss: 0.732 | Acc: 75.964% (2127/2800)\n",
      "Test: Loss: 0.727 | Acc: 76.207% (2210/2900)\n",
      "Test: Loss: 0.727 | Acc: 76.367% (2291/3000)\n",
      "Test: Loss: 0.729 | Acc: 76.226% (2363/3100)\n",
      "Test: Loss: 0.725 | Acc: 76.344% (2443/3200)\n",
      "Test: Loss: 0.721 | Acc: 76.333% (2519/3300)\n",
      "Test: Loss: 0.724 | Acc: 76.147% (2589/3400)\n",
      "Test: Loss: 0.731 | Acc: 76.000% (2660/3500)\n",
      "Test: Loss: 0.730 | Acc: 76.111% (2740/3600)\n",
      "Test: Loss: 0.729 | Acc: 76.081% (2815/3700)\n",
      "Test: Loss: 0.727 | Acc: 76.132% (2893/3800)\n",
      "Test: Loss: 0.728 | Acc: 76.128% (2969/3900)\n",
      "Test: Loss: 0.721 | Acc: 76.375% (3055/4000)\n",
      "Test: Loss: 0.723 | Acc: 76.317% (3129/4100)\n",
      "Test: Loss: 0.726 | Acc: 76.190% (3200/4200)\n",
      "Test: Loss: 0.722 | Acc: 76.186% (3276/4300)\n",
      "Test: Loss: 0.724 | Acc: 76.068% (3347/4400)\n",
      "Test: Loss: 0.729 | Acc: 75.978% (3419/4500)\n",
      "Test: Loss: 0.733 | Acc: 75.891% (3491/4600)\n",
      "Test: Loss: 0.730 | Acc: 76.021% (3573/4700)\n",
      "Test: Loss: 0.731 | Acc: 76.021% (3649/4800)\n",
      "Test: Loss: 0.728 | Acc: 76.143% (3731/4900)\n",
      "Test: Loss: 0.730 | Acc: 76.020% (3801/5000)\n",
      "Test: Loss: 0.726 | Acc: 76.118% (3882/5100)\n",
      "Test: Loss: 0.728 | Acc: 76.077% (3956/5200)\n",
      "Test: Loss: 0.728 | Acc: 76.075% (4032/5300)\n",
      "Test: Loss: 0.726 | Acc: 76.074% (4108/5400)\n",
      "Test: Loss: 0.726 | Acc: 76.127% (4187/5500)\n",
      "Test: Loss: 0.727 | Acc: 76.143% (4264/5600)\n",
      "Test: Loss: 0.727 | Acc: 76.140% (4340/5700)\n",
      "Test: Loss: 0.723 | Acc: 76.190% (4419/5800)\n",
      "Test: Loss: 0.727 | Acc: 76.068% (4488/5900)\n",
      "Test: Loss: 0.726 | Acc: 76.200% (4572/6000)\n",
      "Test: Loss: 0.729 | Acc: 76.098% (4642/6100)\n",
      "Test: Loss: 0.730 | Acc: 76.097% (4718/6200)\n",
      "Test: Loss: 0.729 | Acc: 76.111% (4795/6300)\n",
      "Test: Loss: 0.727 | Acc: 76.156% (4874/6400)\n",
      "Test: Loss: 0.730 | Acc: 76.046% (4943/6500)\n",
      "Test: Loss: 0.732 | Acc: 76.030% (5018/6600)\n",
      "Test: Loss: 0.732 | Acc: 75.970% (5090/6700)\n",
      "Test: Loss: 0.734 | Acc: 75.838% (5157/6800)\n",
      "Test: Loss: 0.735 | Acc: 75.855% (5234/6900)\n",
      "Test: Loss: 0.737 | Acc: 75.771% (5304/7000)\n",
      "Test: Loss: 0.737 | Acc: 75.746% (5378/7100)\n",
      "Test: Loss: 0.736 | Acc: 75.792% (5457/7200)\n",
      "Test: Loss: 0.734 | Acc: 75.863% (5538/7300)\n",
      "Test: Loss: 0.735 | Acc: 75.851% (5613/7400)\n",
      "Test: Loss: 0.737 | Acc: 75.760% (5682/7500)\n",
      "Test: Loss: 0.741 | Acc: 75.632% (5748/7600)\n",
      "Test: Loss: 0.742 | Acc: 75.662% (5826/7700)\n",
      "Test: Loss: 0.741 | Acc: 75.718% (5906/7800)\n",
      "Test: Loss: 0.741 | Acc: 75.696% (5980/7900)\n",
      "Test: Loss: 0.741 | Acc: 75.650% (6052/8000)\n",
      "Test: Loss: 0.741 | Acc: 75.580% (6122/8100)\n",
      "Test: Loss: 0.741 | Acc: 75.573% (6197/8200)\n",
      "Test: Loss: 0.742 | Acc: 75.518% (6268/8300)\n",
      "Test: Loss: 0.743 | Acc: 75.464% (6339/8400)\n",
      "Test: Loss: 0.744 | Acc: 75.447% (6413/8500)\n",
      "Test: Loss: 0.744 | Acc: 75.477% (6491/8600)\n",
      "Test: Loss: 0.744 | Acc: 75.506% (6569/8700)\n",
      "Test: Loss: 0.742 | Acc: 75.545% (6648/8800)\n",
      "Test: Loss: 0.744 | Acc: 75.517% (6721/8900)\n",
      "Test: Loss: 0.745 | Acc: 75.500% (6795/9000)\n",
      "Test: Loss: 0.745 | Acc: 75.538% (6874/9100)\n",
      "Test: Loss: 0.745 | Acc: 75.543% (6950/9200)\n",
      "Test: Loss: 0.745 | Acc: 75.527% (7024/9300)\n",
      "Test: Loss: 0.744 | Acc: 75.574% (7104/9400)\n",
      "Test: Loss: 0.745 | Acc: 75.537% (7176/9500)\n",
      "Test: Loss: 0.744 | Acc: 75.583% (7256/9600)\n",
      "Test: Loss: 0.742 | Acc: 75.629% (7336/9700)\n",
      "Test: Loss: 0.744 | Acc: 75.643% (7413/9800)\n",
      "Test: Loss: 0.743 | Acc: 75.687% (7493/9900)\n",
      "Test: Loss: 0.743 | Acc: 75.660% (7566/10000)\n",
      "Train: Loss: 0.299 | Acc: 87.500% (112/128)\n",
      "Train: Loss: 0.295 | Acc: 89.453% (229/256)\n",
      "Train: Loss: 0.288 | Acc: 89.062% (342/384)\n",
      "Train: Loss: 0.310 | Acc: 88.867% (455/512)\n",
      "Train: Loss: 0.332 | Acc: 88.125% (564/640)\n",
      "Train: Loss: 0.341 | Acc: 87.760% (674/768)\n",
      "Train: Loss: 0.370 | Acc: 87.277% (782/896)\n",
      "Train: Loss: 0.366 | Acc: 87.207% (893/1024)\n",
      "Train: Loss: 0.355 | Acc: 87.500% (1008/1152)\n",
      "Train: Loss: 0.339 | Acc: 88.125% (1128/1280)\n",
      "Train: Loss: 0.336 | Acc: 88.352% (1244/1408)\n",
      "Train: Loss: 0.339 | Acc: 88.346% (1357/1536)\n",
      "Train: Loss: 0.340 | Acc: 88.281% (1469/1664)\n",
      "Train: Loss: 0.337 | Acc: 88.393% (1584/1792)\n",
      "Train: Loss: 0.332 | Acc: 88.594% (1701/1920)\n",
      "Train: Loss: 0.329 | Acc: 88.525% (1813/2048)\n",
      "Train: Loss: 0.329 | Acc: 88.327% (1922/2176)\n",
      "Train: Loss: 0.326 | Acc: 88.411% (2037/2304)\n",
      "Train: Loss: 0.326 | Acc: 88.240% (2146/2432)\n",
      "Train: Loss: 0.322 | Acc: 88.359% (2262/2560)\n",
      "Train: Loss: 0.319 | Acc: 88.504% (2379/2688)\n",
      "Train: Loss: 0.324 | Acc: 88.388% (2489/2816)\n",
      "Train: Loss: 0.319 | Acc: 88.621% (2609/2944)\n",
      "Train: Loss: 0.313 | Acc: 88.867% (2730/3072)\n",
      "Train: Loss: 0.314 | Acc: 88.750% (2840/3200)\n",
      "Train: Loss: 0.319 | Acc: 88.642% (2950/3328)\n",
      "Train: Loss: 0.321 | Acc: 88.600% (3062/3456)\n",
      "Train: Loss: 0.322 | Acc: 88.560% (3174/3584)\n",
      "Train: Loss: 0.321 | Acc: 88.605% (3289/3712)\n",
      "Train: Loss: 0.318 | Acc: 88.750% (3408/3840)\n",
      "Train: Loss: 0.321 | Acc: 88.735% (3521/3968)\n",
      "Train: Loss: 0.323 | Acc: 88.672% (3632/4096)\n",
      "Train: Loss: 0.322 | Acc: 88.684% (3746/4224)\n",
      "Train: Loss: 0.319 | Acc: 88.810% (3865/4352)\n",
      "Train: Loss: 0.322 | Acc: 88.638% (3971/4480)\n",
      "Train: Loss: 0.321 | Acc: 88.650% (4085/4608)\n",
      "Train: Loss: 0.323 | Acc: 88.661% (4199/4736)\n",
      "Train: Loss: 0.320 | Acc: 88.713% (4315/4864)\n",
      "Train: Loss: 0.319 | Acc: 88.762% (4431/4992)\n",
      "Train: Loss: 0.319 | Acc: 88.809% (4547/5120)\n",
      "Train: Loss: 0.319 | Acc: 88.815% (4661/5248)\n",
      "Train: Loss: 0.317 | Acc: 88.839% (4776/5376)\n",
      "Train: Loss: 0.316 | Acc: 88.863% (4891/5504)\n",
      "Train: Loss: 0.316 | Acc: 88.885% (5006/5632)\n",
      "Train: Loss: 0.317 | Acc: 88.837% (5117/5760)\n",
      "Train: Loss: 0.316 | Acc: 88.876% (5233/5888)\n",
      "Train: Loss: 0.319 | Acc: 88.830% (5344/6016)\n",
      "Train: Loss: 0.319 | Acc: 88.802% (5456/6144)\n",
      "Train: Loss: 0.317 | Acc: 88.871% (5574/6272)\n",
      "Train: Loss: 0.317 | Acc: 88.812% (5684/6400)\n",
      "Train: Loss: 0.317 | Acc: 88.802% (5797/6528)\n",
      "Train: Loss: 0.317 | Acc: 88.747% (5907/6656)\n",
      "Train: Loss: 0.317 | Acc: 88.723% (6019/6784)\n",
      "Train: Loss: 0.316 | Acc: 88.773% (6136/6912)\n",
      "Train: Loss: 0.316 | Acc: 88.750% (6248/7040)\n",
      "Train: Loss: 0.318 | Acc: 88.714% (6359/7168)\n",
      "Train: Loss: 0.318 | Acc: 88.665% (6469/7296)\n",
      "Train: Loss: 0.319 | Acc: 88.658% (6582/7424)\n",
      "Train: Loss: 0.322 | Acc: 88.506% (6684/7552)\n",
      "Train: Loss: 0.321 | Acc: 88.568% (6802/7680)\n",
      "Train: Loss: 0.319 | Acc: 88.627% (6920/7808)\n",
      "Train: Loss: 0.318 | Acc: 88.659% (7036/7936)\n",
      "Train: Loss: 0.317 | Acc: 88.690% (7152/8064)\n",
      "Train: Loss: 0.316 | Acc: 88.708% (7267/8192)\n",
      "Train: Loss: 0.316 | Acc: 88.738% (7383/8320)\n",
      "Train: Loss: 0.316 | Acc: 88.743% (7497/8448)\n",
      "Train: Loss: 0.317 | Acc: 88.724% (7609/8576)\n",
      "Train: Loss: 0.318 | Acc: 88.695% (7720/8704)\n",
      "Train: Loss: 0.319 | Acc: 88.632% (7828/8832)\n",
      "Train: Loss: 0.318 | Acc: 88.650% (7943/8960)\n",
      "Train: Loss: 0.318 | Acc: 88.677% (8059/9088)\n",
      "Train: Loss: 0.319 | Acc: 88.639% (8169/9216)\n",
      "Train: Loss: 0.320 | Acc: 88.592% (8278/9344)\n",
      "Train: Loss: 0.320 | Acc: 88.598% (8392/9472)\n",
      "Train: Loss: 0.321 | Acc: 88.604% (8506/9600)\n",
      "Train: Loss: 0.320 | Acc: 88.600% (8619/9728)\n",
      "Train: Loss: 0.321 | Acc: 88.545% (8727/9856)\n",
      "Train: Loss: 0.321 | Acc: 88.532% (8839/9984)\n",
      "Train: Loss: 0.323 | Acc: 88.499% (8949/10112)\n",
      "Train: Loss: 0.322 | Acc: 88.535% (9066/10240)\n",
      "Train: Loss: 0.324 | Acc: 88.445% (9170/10368)\n",
      "Train: Loss: 0.325 | Acc: 88.462% (9285/10496)\n",
      "Train: Loss: 0.325 | Acc: 88.441% (9396/10624)\n",
      "Train: Loss: 0.325 | Acc: 88.458% (9511/10752)\n",
      "Train: Loss: 0.324 | Acc: 88.520% (9631/10880)\n",
      "Train: Loss: 0.323 | Acc: 88.554% (9748/11008)\n",
      "Train: Loss: 0.323 | Acc: 88.560% (9862/11136)\n",
      "Train: Loss: 0.323 | Acc: 88.530% (9972/11264)\n",
      "Train: Loss: 0.323 | Acc: 88.527% (10085/11392)\n",
      "Train: Loss: 0.323 | Acc: 88.533% (10199/11520)\n",
      "Train: Loss: 0.325 | Acc: 88.513% (10310/11648)\n",
      "Train: Loss: 0.324 | Acc: 88.561% (10429/11776)\n",
      "Train: Loss: 0.324 | Acc: 88.567% (10543/11904)\n",
      "Train: Loss: 0.324 | Acc: 88.572% (10657/12032)\n",
      "Train: Loss: 0.324 | Acc: 88.553% (10768/12160)\n",
      "Train: Loss: 0.325 | Acc: 88.509% (10876/12288)\n",
      "Train: Loss: 0.326 | Acc: 88.434% (10980/12416)\n",
      "Train: Loss: 0.325 | Acc: 88.449% (11095/12544)\n",
      "Train: Loss: 0.325 | Acc: 88.479% (11212/12672)\n",
      "Train: Loss: 0.326 | Acc: 88.438% (11320/12800)\n",
      "Train: Loss: 0.326 | Acc: 88.420% (11431/12928)\n",
      "Train: Loss: 0.327 | Acc: 88.381% (11539/13056)\n",
      "Train: Loss: 0.328 | Acc: 88.365% (11650/13184)\n",
      "Train: Loss: 0.329 | Acc: 88.334% (11759/13312)\n",
      "Train: Loss: 0.329 | Acc: 88.311% (11869/13440)\n",
      "Train: Loss: 0.328 | Acc: 88.325% (11984/13568)\n",
      "Train: Loss: 0.330 | Acc: 88.274% (12090/13696)\n",
      "Train: Loss: 0.329 | Acc: 88.274% (12203/13824)\n",
      "Train: Loss: 0.330 | Acc: 88.260% (12314/13952)\n",
      "Train: Loss: 0.330 | Acc: 88.246% (12425/14080)\n",
      "Train: Loss: 0.331 | Acc: 88.218% (12534/14208)\n",
      "Train: Loss: 0.333 | Acc: 88.184% (12642/14336)\n",
      "Train: Loss: 0.334 | Acc: 88.136% (12748/14464)\n",
      "Train: Loss: 0.334 | Acc: 88.151% (12863/14592)\n",
      "Train: Loss: 0.334 | Acc: 88.139% (12974/14720)\n",
      "Train: Loss: 0.335 | Acc: 88.140% (13087/14848)\n",
      "Train: Loss: 0.336 | Acc: 88.101% (13194/14976)\n",
      "Train: Loss: 0.336 | Acc: 88.102% (13307/15104)\n",
      "Train: Loss: 0.336 | Acc: 88.097% (13419/15232)\n",
      "Train: Loss: 0.336 | Acc: 88.079% (13529/15360)\n",
      "Train: Loss: 0.337 | Acc: 88.062% (13639/15488)\n",
      "Train: Loss: 0.337 | Acc: 88.083% (13755/15616)\n",
      "Train: Loss: 0.337 | Acc: 88.078% (13867/15744)\n",
      "Train: Loss: 0.337 | Acc: 88.054% (13976/15872)\n",
      "Train: Loss: 0.337 | Acc: 88.050% (14088/16000)\n",
      "Train: Loss: 0.338 | Acc: 88.033% (14198/16128)\n",
      "Train: Loss: 0.339 | Acc: 87.992% (14304/16256)\n",
      "Train: Loss: 0.339 | Acc: 87.958% (14411/16384)\n",
      "Train: Loss: 0.339 | Acc: 87.991% (14529/16512)\n",
      "Train: Loss: 0.339 | Acc: 87.969% (14638/16640)\n",
      "Train: Loss: 0.340 | Acc: 87.929% (14744/16768)\n",
      "Train: Loss: 0.340 | Acc: 87.950% (14860/16896)\n",
      "Train: Loss: 0.340 | Acc: 87.952% (14973/17024)\n",
      "Train: Loss: 0.340 | Acc: 87.961% (15087/17152)\n",
      "Train: Loss: 0.340 | Acc: 87.940% (15196/17280)\n",
      "Train: Loss: 0.340 | Acc: 87.908% (15303/17408)\n",
      "Train: Loss: 0.340 | Acc: 87.905% (15415/17536)\n",
      "Train: Loss: 0.340 | Acc: 87.913% (15529/17664)\n",
      "Train: Loss: 0.340 | Acc: 87.933% (15645/17792)\n",
      "Train: Loss: 0.340 | Acc: 87.952% (15761/17920)\n",
      "Train: Loss: 0.341 | Acc: 87.932% (15870/18048)\n",
      "Train: Loss: 0.342 | Acc: 87.885% (15974/18176)\n",
      "Train: Loss: 0.342 | Acc: 87.893% (16088/18304)\n",
      "Train: Loss: 0.342 | Acc: 87.918% (16205/18432)\n",
      "Train: Loss: 0.342 | Acc: 87.899% (16314/18560)\n",
      "Train: Loss: 0.343 | Acc: 87.891% (16425/18688)\n",
      "Train: Loss: 0.343 | Acc: 87.883% (16536/18816)\n",
      "Train: Loss: 0.344 | Acc: 87.859% (16644/18944)\n",
      "Train: Loss: 0.345 | Acc: 87.830% (16751/19072)\n",
      "Train: Loss: 0.345 | Acc: 87.818% (16861/19200)\n",
      "Train: Loss: 0.346 | Acc: 87.810% (16972/19328)\n",
      "Train: Loss: 0.347 | Acc: 87.783% (17079/19456)\n",
      "Train: Loss: 0.347 | Acc: 87.781% (17191/19584)\n",
      "Train: Loss: 0.347 | Acc: 87.774% (17302/19712)\n",
      "Train: Loss: 0.347 | Acc: 87.757% (17411/19840)\n",
      "Train: Loss: 0.347 | Acc: 87.780% (17528/19968)\n",
      "Train: Loss: 0.347 | Acc: 87.779% (17640/20096)\n",
      "Train: Loss: 0.347 | Acc: 87.782% (17753/20224)\n",
      "Train: Loss: 0.346 | Acc: 87.795% (17868/20352)\n",
      "Train: Loss: 0.346 | Acc: 87.793% (17980/20480)\n",
      "Train: Loss: 0.347 | Acc: 87.767% (18087/20608)\n",
      "Train: Loss: 0.347 | Acc: 87.765% (18199/20736)\n",
      "Train: Loss: 0.347 | Acc: 87.754% (18309/20864)\n",
      "Train: Loss: 0.348 | Acc: 87.729% (18416/20992)\n",
      "Train: Loss: 0.347 | Acc: 87.741% (18531/21120)\n",
      "Train: Loss: 0.348 | Acc: 87.698% (18634/21248)\n",
      "Train: Loss: 0.349 | Acc: 87.678% (18742/21376)\n",
      "Train: Loss: 0.350 | Acc: 87.649% (18848/21504)\n",
      "Train: Loss: 0.349 | Acc: 87.643% (18959/21632)\n",
      "Train: Loss: 0.349 | Acc: 87.656% (19074/21760)\n",
      "Train: Loss: 0.349 | Acc: 87.651% (19185/21888)\n",
      "Train: Loss: 0.349 | Acc: 87.668% (19301/22016)\n",
      "Train: Loss: 0.349 | Acc: 87.676% (19415/22144)\n",
      "Train: Loss: 0.350 | Acc: 87.653% (19522/22272)\n",
      "Train: Loss: 0.350 | Acc: 87.665% (19637/22400)\n",
      "Train: Loss: 0.351 | Acc: 87.651% (19746/22528)\n",
      "Train: Loss: 0.351 | Acc: 87.628% (19853/22656)\n",
      "Train: Loss: 0.352 | Acc: 87.605% (19960/22784)\n",
      "Train: Loss: 0.352 | Acc: 87.565% (20063/22912)\n",
      "Train: Loss: 0.352 | Acc: 87.565% (20175/23040)\n",
      "Train: Loss: 0.352 | Acc: 87.560% (20286/23168)\n",
      "Train: Loss: 0.352 | Acc: 87.577% (20402/23296)\n",
      "Train: Loss: 0.352 | Acc: 87.577% (20514/23424)\n",
      "Train: Loss: 0.351 | Acc: 87.602% (20632/23552)\n",
      "Train: Loss: 0.352 | Acc: 87.589% (20741/23680)\n",
      "Train: Loss: 0.352 | Acc: 87.571% (20849/23808)\n",
      "Train: Loss: 0.353 | Acc: 87.563% (20959/23936)\n",
      "Train: Loss: 0.353 | Acc: 87.575% (21074/24064)\n",
      "Train: Loss: 0.353 | Acc: 87.574% (21186/24192)\n",
      "Train: Loss: 0.353 | Acc: 87.570% (21297/24320)\n",
      "Train: Loss: 0.354 | Acc: 87.537% (21401/24448)\n",
      "Train: Loss: 0.355 | Acc: 87.512% (21507/24576)\n",
      "Train: Loss: 0.355 | Acc: 87.504% (21617/24704)\n",
      "Train: Loss: 0.354 | Acc: 87.516% (21732/24832)\n",
      "Train: Loss: 0.355 | Acc: 87.508% (21842/24960)\n",
      "Train: Loss: 0.354 | Acc: 87.524% (21958/25088)\n",
      "Train: Loss: 0.354 | Acc: 87.552% (22077/25216)\n",
      "Train: Loss: 0.354 | Acc: 87.559% (22191/25344)\n",
      "Train: Loss: 0.354 | Acc: 87.547% (22300/25472)\n",
      "Train: Loss: 0.354 | Acc: 87.539% (22410/25600)\n",
      "Train: Loss: 0.354 | Acc: 87.554% (22526/25728)\n",
      "Train: Loss: 0.354 | Acc: 87.570% (22642/25856)\n",
      "Train: Loss: 0.353 | Acc: 87.573% (22755/25984)\n",
      "Train: Loss: 0.354 | Acc: 87.542% (22859/26112)\n",
      "Train: Loss: 0.354 | Acc: 87.515% (22964/26240)\n",
      "Train: Loss: 0.354 | Acc: 87.519% (23077/26368)\n",
      "Train: Loss: 0.354 | Acc: 87.508% (23186/26496)\n",
      "Train: Loss: 0.355 | Acc: 87.492% (23294/26624)\n",
      "Train: Loss: 0.355 | Acc: 87.500% (23408/26752)\n",
      "Train: Loss: 0.355 | Acc: 87.478% (23514/26880)\n",
      "Train: Loss: 0.355 | Acc: 87.485% (23628/27008)\n",
      "Train: Loss: 0.355 | Acc: 87.485% (23740/27136)\n",
      "Train: Loss: 0.355 | Acc: 87.482% (23851/27264)\n",
      "Train: Loss: 0.356 | Acc: 87.474% (23961/27392)\n",
      "Train: Loss: 0.356 | Acc: 87.471% (24072/27520)\n",
      "Train: Loss: 0.356 | Acc: 87.471% (24184/27648)\n",
      "Train: Loss: 0.356 | Acc: 87.471% (24296/27776)\n",
      "Train: Loss: 0.356 | Acc: 87.464% (24406/27904)\n",
      "Train: Loss: 0.357 | Acc: 87.443% (24512/28032)\n",
      "Train: Loss: 0.357 | Acc: 87.443% (24624/28160)\n",
      "Train: Loss: 0.357 | Acc: 87.465% (24742/28288)\n",
      "Train: Loss: 0.357 | Acc: 87.475% (24857/28416)\n",
      "Train: Loss: 0.357 | Acc: 87.482% (24971/28544)\n",
      "Train: Loss: 0.358 | Acc: 87.465% (25078/28672)\n",
      "Train: Loss: 0.358 | Acc: 87.448% (25185/28800)\n",
      "Train: Loss: 0.358 | Acc: 87.434% (25293/28928)\n",
      "Train: Loss: 0.359 | Acc: 87.411% (25398/29056)\n",
      "Train: Loss: 0.359 | Acc: 87.425% (25514/29184)\n",
      "Train: Loss: 0.359 | Acc: 87.432% (25628/29312)\n",
      "Train: Loss: 0.359 | Acc: 87.442% (25743/29440)\n",
      "Train: Loss: 0.359 | Acc: 87.446% (25856/29568)\n",
      "Train: Loss: 0.359 | Acc: 87.443% (25967/29696)\n",
      "Train: Loss: 0.359 | Acc: 87.443% (26079/29824)\n",
      "Train: Loss: 0.358 | Acc: 87.447% (26192/29952)\n",
      "Train: Loss: 0.359 | Acc: 87.434% (26300/30080)\n",
      "Train: Loss: 0.359 | Acc: 87.430% (26411/30208)\n",
      "Train: Loss: 0.359 | Acc: 87.431% (26523/30336)\n",
      "Train: Loss: 0.359 | Acc: 87.431% (26635/30464)\n",
      "Train: Loss: 0.359 | Acc: 87.425% (26745/30592)\n",
      "Train: Loss: 0.358 | Acc: 87.448% (26864/30720)\n",
      "Train: Loss: 0.359 | Acc: 87.429% (26970/30848)\n",
      "Train: Loss: 0.359 | Acc: 87.439% (27085/30976)\n",
      "Train: Loss: 0.359 | Acc: 87.442% (27198/31104)\n",
      "Train: Loss: 0.359 | Acc: 87.417% (27302/31232)\n",
      "Train: Loss: 0.360 | Acc: 87.395% (27407/31360)\n",
      "Train: Loss: 0.360 | Acc: 87.402% (27521/31488)\n",
      "Train: Loss: 0.359 | Acc: 87.415% (27637/31616)\n",
      "Train: Loss: 0.360 | Acc: 87.405% (27746/31744)\n",
      "Train: Loss: 0.360 | Acc: 87.406% (27858/31872)\n",
      "Train: Loss: 0.360 | Acc: 87.406% (27970/32000)\n",
      "Train: Loss: 0.359 | Acc: 87.416% (28085/32128)\n",
      "Train: Loss: 0.360 | Acc: 87.398% (28191/32256)\n",
      "Train: Loss: 0.360 | Acc: 87.410% (28307/32384)\n",
      "Train: Loss: 0.360 | Acc: 87.423% (28423/32512)\n",
      "Train: Loss: 0.360 | Acc: 87.414% (28532/32640)\n",
      "Train: Loss: 0.360 | Acc: 87.402% (28640/32768)\n",
      "Train: Loss: 0.360 | Acc: 87.406% (28753/32896)\n",
      "Train: Loss: 0.360 | Acc: 87.409% (28866/33024)\n",
      "Train: Loss: 0.360 | Acc: 87.410% (28978/33152)\n",
      "Train: Loss: 0.361 | Acc: 87.401% (29087/33280)\n",
      "Train: Loss: 0.361 | Acc: 87.389% (29195/33408)\n",
      "Train: Loss: 0.360 | Acc: 87.402% (29311/33536)\n",
      "Train: Loss: 0.361 | Acc: 87.384% (29417/33664)\n",
      "Train: Loss: 0.361 | Acc: 87.396% (29533/33792)\n",
      "Train: Loss: 0.360 | Acc: 87.409% (29649/33920)\n",
      "Train: Loss: 0.361 | Acc: 87.397% (29757/34048)\n",
      "Train: Loss: 0.361 | Acc: 87.395% (29868/34176)\n",
      "Train: Loss: 0.361 | Acc: 87.389% (29978/34304)\n",
      "Train: Loss: 0.361 | Acc: 87.375% (30085/34432)\n",
      "Train: Loss: 0.362 | Acc: 87.355% (30190/34560)\n",
      "Train: Loss: 0.361 | Acc: 87.376% (30309/34688)\n",
      "Train: Loss: 0.362 | Acc: 87.368% (30418/34816)\n",
      "Train: Loss: 0.362 | Acc: 87.354% (30525/34944)\n",
      "Train: Loss: 0.362 | Acc: 87.360% (30639/35072)\n",
      "Train: Loss: 0.363 | Acc: 87.332% (30741/35200)\n",
      "Train: Loss: 0.363 | Acc: 87.327% (30851/35328)\n",
      "Train: Loss: 0.364 | Acc: 87.294% (30951/35456)\n",
      "Train: Loss: 0.364 | Acc: 87.289% (31061/35584)\n",
      "Train: Loss: 0.364 | Acc: 87.270% (31166/35712)\n",
      "Train: Loss: 0.364 | Acc: 87.271% (31278/35840)\n",
      "Train: Loss: 0.365 | Acc: 87.258% (31385/35968)\n",
      "Train: Loss: 0.365 | Acc: 87.253% (31495/36096)\n",
      "Train: Loss: 0.365 | Acc: 87.229% (31598/36224)\n",
      "Train: Loss: 0.365 | Acc: 87.222% (31707/36352)\n",
      "Train: Loss: 0.365 | Acc: 87.223% (31819/36480)\n",
      "Train: Loss: 0.365 | Acc: 87.230% (31933/36608)\n",
      "Train: Loss: 0.366 | Acc: 87.217% (32040/36736)\n",
      "Train: Loss: 0.366 | Acc: 87.221% (32153/36864)\n",
      "Train: Loss: 0.366 | Acc: 87.227% (32267/36992)\n",
      "Train: Loss: 0.366 | Acc: 87.212% (32373/37120)\n",
      "Train: Loss: 0.367 | Acc: 87.197% (32479/37248)\n",
      "Train: Loss: 0.367 | Acc: 87.200% (32592/37376)\n",
      "Train: Loss: 0.367 | Acc: 87.196% (32702/37504)\n",
      "Train: Loss: 0.367 | Acc: 87.208% (32818/37632)\n",
      "Train: Loss: 0.367 | Acc: 87.206% (32929/37760)\n",
      "Train: Loss: 0.367 | Acc: 87.215% (33044/37888)\n",
      "Train: Loss: 0.367 | Acc: 87.200% (33150/38016)\n",
      "Train: Loss: 0.367 | Acc: 87.204% (33263/38144)\n",
      "Train: Loss: 0.368 | Acc: 87.200% (33373/38272)\n",
      "Train: Loss: 0.368 | Acc: 87.198% (33484/38400)\n",
      "Train: Loss: 0.368 | Acc: 87.173% (33586/38528)\n",
      "Train: Loss: 0.368 | Acc: 87.182% (33701/38656)\n",
      "Train: Loss: 0.368 | Acc: 87.175% (33810/38784)\n",
      "Train: Loss: 0.368 | Acc: 87.168% (33919/38912)\n",
      "Train: Loss: 0.369 | Acc: 87.167% (34030/39040)\n",
      "Train: Loss: 0.369 | Acc: 87.183% (34148/39168)\n",
      "Train: Loss: 0.369 | Acc: 87.190% (34262/39296)\n",
      "Train: Loss: 0.369 | Acc: 87.191% (34374/39424)\n",
      "Train: Loss: 0.369 | Acc: 87.166% (34476/39552)\n",
      "Train: Loss: 0.369 | Acc: 87.165% (34587/39680)\n",
      "Train: Loss: 0.369 | Acc: 87.171% (34701/39808)\n",
      "Train: Loss: 0.369 | Acc: 87.169% (34812/39936)\n",
      "Train: Loss: 0.369 | Acc: 87.173% (34925/40064)\n",
      "Train: Loss: 0.369 | Acc: 87.174% (35037/40192)\n",
      "Train: Loss: 0.369 | Acc: 87.173% (35148/40320)\n",
      "Train: Loss: 0.369 | Acc: 87.154% (35252/40448)\n",
      "Train: Loss: 0.369 | Acc: 87.150% (35362/40576)\n",
      "Train: Loss: 0.369 | Acc: 87.151% (35474/40704)\n",
      "Train: Loss: 0.370 | Acc: 87.145% (35583/40832)\n",
      "Train: Loss: 0.370 | Acc: 87.148% (35696/40960)\n",
      "Train: Loss: 0.370 | Acc: 87.157% (35811/41088)\n",
      "Train: Loss: 0.370 | Acc: 87.148% (35919/41216)\n",
      "Train: Loss: 0.370 | Acc: 87.140% (36027/41344)\n",
      "Train: Loss: 0.370 | Acc: 87.138% (36138/41472)\n",
      "Train: Loss: 0.370 | Acc: 87.135% (36248/41600)\n",
      "Train: Loss: 0.371 | Acc: 87.124% (36355/41728)\n",
      "Train: Loss: 0.371 | Acc: 87.113% (36462/41856)\n",
      "Train: Loss: 0.371 | Acc: 87.107% (36571/41984)\n",
      "Train: Loss: 0.371 | Acc: 87.101% (36680/42112)\n",
      "Train: Loss: 0.371 | Acc: 87.098% (36790/42240)\n",
      "Train: Loss: 0.371 | Acc: 87.096% (36901/42368)\n",
      "Train: Loss: 0.371 | Acc: 87.093% (37011/42496)\n",
      "Train: Loss: 0.371 | Acc: 87.087% (37120/42624)\n",
      "Train: Loss: 0.372 | Acc: 87.063% (37221/42752)\n",
      "Train: Loss: 0.372 | Acc: 87.059% (37331/42880)\n",
      "Train: Loss: 0.372 | Acc: 87.061% (37443/43008)\n",
      "Train: Loss: 0.372 | Acc: 87.055% (37552/43136)\n",
      "Train: Loss: 0.372 | Acc: 87.061% (37666/43264)\n",
      "Train: Loss: 0.372 | Acc: 87.062% (37778/43392)\n",
      "Train: Loss: 0.372 | Acc: 87.057% (37887/43520)\n",
      "Train: Loss: 0.372 | Acc: 87.037% (37990/43648)\n",
      "Train: Loss: 0.372 | Acc: 87.039% (38102/43776)\n",
      "Train: Loss: 0.372 | Acc: 87.022% (38206/43904)\n",
      "Train: Loss: 0.373 | Acc: 87.019% (38316/44032)\n",
      "Train: Loss: 0.373 | Acc: 87.013% (38425/44160)\n",
      "Train: Loss: 0.373 | Acc: 86.999% (38530/44288)\n",
      "Train: Loss: 0.373 | Acc: 87.002% (38643/44416)\n",
      "Train: Loss: 0.373 | Acc: 87.002% (38754/44544)\n",
      "Train: Loss: 0.373 | Acc: 86.985% (38858/44672)\n",
      "Train: Loss: 0.373 | Acc: 86.982% (38968/44800)\n",
      "Train: Loss: 0.373 | Acc: 86.968% (39073/44928)\n",
      "Train: Loss: 0.374 | Acc: 86.961% (39181/45056)\n",
      "Train: Loss: 0.374 | Acc: 86.949% (39287/45184)\n",
      "Train: Loss: 0.374 | Acc: 86.950% (39399/45312)\n",
      "Train: Loss: 0.374 | Acc: 86.948% (39509/45440)\n",
      "Train: Loss: 0.374 | Acc: 86.934% (39614/45568)\n",
      "Train: Loss: 0.374 | Acc: 86.924% (39721/45696)\n",
      "Train: Loss: 0.374 | Acc: 86.935% (39837/45824)\n",
      "Train: Loss: 0.374 | Acc: 86.928% (39945/45952)\n",
      "Train: Loss: 0.374 | Acc: 86.927% (40056/46080)\n",
      "Train: Loss: 0.374 | Acc: 86.927% (40167/46208)\n",
      "Train: Loss: 0.375 | Acc: 86.919% (40275/46336)\n",
      "Train: Loss: 0.375 | Acc: 86.906% (40380/46464)\n",
      "Train: Loss: 0.375 | Acc: 86.893% (40485/46592)\n",
      "Train: Loss: 0.376 | Acc: 86.888% (40594/46720)\n",
      "Train: Loss: 0.376 | Acc: 86.885% (40704/46848)\n",
      "Train: Loss: 0.376 | Acc: 86.874% (40810/46976)\n",
      "Train: Loss: 0.376 | Acc: 86.867% (40918/47104)\n",
      "Train: Loss: 0.376 | Acc: 86.858% (41025/47232)\n",
      "Train: Loss: 0.377 | Acc: 86.833% (41124/47360)\n",
      "Train: Loss: 0.377 | Acc: 86.826% (41232/47488)\n",
      "Train: Loss: 0.377 | Acc: 86.811% (41336/47616)\n",
      "Train: Loss: 0.378 | Acc: 86.803% (41443/47744)\n",
      "Train: Loss: 0.378 | Acc: 86.811% (41558/47872)\n",
      "Train: Loss: 0.378 | Acc: 86.812% (41670/48000)\n",
      "Train: Loss: 0.377 | Acc: 86.821% (41785/48128)\n",
      "Train: Loss: 0.377 | Acc: 86.824% (41898/48256)\n",
      "Train: Loss: 0.377 | Acc: 86.826% (42010/48384)\n",
      "Train: Loss: 0.377 | Acc: 86.838% (42127/48512)\n",
      "Train: Loss: 0.377 | Acc: 86.840% (42239/48640)\n",
      "Train: Loss: 0.377 | Acc: 86.836% (42348/48768)\n",
      "Train: Loss: 0.377 | Acc: 86.833% (42458/48896)\n",
      "Train: Loss: 0.377 | Acc: 86.829% (42567/49024)\n",
      "Train: Loss: 0.377 | Acc: 86.825% (42676/49152)\n",
      "Train: Loss: 0.378 | Acc: 86.808% (42779/49280)\n",
      "Train: Loss: 0.377 | Acc: 86.818% (42895/49408)\n",
      "Train: Loss: 0.377 | Acc: 86.822% (43008/49536)\n",
      "Train: Loss: 0.378 | Acc: 86.809% (43113/49664)\n",
      "Train: Loss: 0.378 | Acc: 86.803% (43221/49792)\n",
      "Train: Loss: 0.378 | Acc: 86.795% (43328/49920)\n",
      "Train: Loss: 0.378 | Acc: 86.802% (43401/50000)\n",
      "Test: Loss: 0.694 | Acc: 80.000% (80/100)\n",
      "Test: Loss: 0.573 | Acc: 81.000% (162/200)\n",
      "Test: Loss: 0.525 | Acc: 82.333% (247/300)\n",
      "Test: Loss: 0.543 | Acc: 81.500% (326/400)\n",
      "Test: Loss: 0.547 | Acc: 81.200% (406/500)\n",
      "Test: Loss: 0.520 | Acc: 82.167% (493/600)\n",
      "Test: Loss: 0.532 | Acc: 81.714% (572/700)\n",
      "Test: Loss: 0.539 | Acc: 81.375% (651/800)\n",
      "Test: Loss: 0.567 | Acc: 80.889% (728/900)\n",
      "Test: Loss: 0.560 | Acc: 80.600% (806/1000)\n",
      "Test: Loss: 0.560 | Acc: 80.636% (887/1100)\n",
      "Test: Loss: 0.555 | Acc: 80.833% (970/1200)\n",
      "Test: Loss: 0.537 | Acc: 81.462% (1059/1300)\n",
      "Test: Loss: 0.541 | Acc: 81.214% (1137/1400)\n",
      "Test: Loss: 0.534 | Acc: 81.400% (1221/1500)\n",
      "Test: Loss: 0.546 | Acc: 81.062% (1297/1600)\n",
      "Test: Loss: 0.551 | Acc: 81.176% (1380/1700)\n",
      "Test: Loss: 0.556 | Acc: 80.833% (1455/1800)\n",
      "Test: Loss: 0.565 | Acc: 80.895% (1537/1900)\n",
      "Test: Loss: 0.577 | Acc: 80.650% (1613/2000)\n",
      "Test: Loss: 0.583 | Acc: 80.381% (1688/2100)\n",
      "Test: Loss: 0.585 | Acc: 80.500% (1771/2200)\n",
      "Test: Loss: 0.586 | Acc: 80.348% (1848/2300)\n",
      "Test: Loss: 0.590 | Acc: 80.042% (1921/2400)\n",
      "Test: Loss: 0.590 | Acc: 80.120% (2003/2500)\n",
      "Test: Loss: 0.601 | Acc: 80.000% (2080/2600)\n",
      "Test: Loss: 0.597 | Acc: 79.963% (2159/2700)\n",
      "Test: Loss: 0.598 | Acc: 79.964% (2239/2800)\n",
      "Test: Loss: 0.598 | Acc: 79.966% (2319/2900)\n",
      "Test: Loss: 0.595 | Acc: 79.967% (2399/3000)\n",
      "Test: Loss: 0.595 | Acc: 80.032% (2481/3100)\n",
      "Test: Loss: 0.594 | Acc: 80.000% (2560/3200)\n",
      "Test: Loss: 0.590 | Acc: 80.121% (2644/3300)\n",
      "Test: Loss: 0.593 | Acc: 80.029% (2721/3400)\n",
      "Test: Loss: 0.594 | Acc: 80.171% (2806/3500)\n",
      "Test: Loss: 0.596 | Acc: 80.167% (2886/3600)\n",
      "Test: Loss: 0.597 | Acc: 80.216% (2968/3700)\n",
      "Test: Loss: 0.599 | Acc: 80.105% (3044/3800)\n",
      "Test: Loss: 0.595 | Acc: 80.179% (3127/3900)\n",
      "Test: Loss: 0.592 | Acc: 80.200% (3208/4000)\n",
      "Test: Loss: 0.593 | Acc: 80.146% (3286/4100)\n",
      "Test: Loss: 0.598 | Acc: 80.119% (3365/4200)\n",
      "Test: Loss: 0.593 | Acc: 80.326% (3454/4300)\n",
      "Test: Loss: 0.596 | Acc: 80.250% (3531/4400)\n",
      "Test: Loss: 0.595 | Acc: 80.289% (3613/4500)\n",
      "Test: Loss: 0.595 | Acc: 80.174% (3688/4600)\n",
      "Test: Loss: 0.595 | Acc: 80.170% (3768/4700)\n",
      "Test: Loss: 0.596 | Acc: 80.146% (3847/4800)\n",
      "Test: Loss: 0.594 | Acc: 80.224% (3931/4900)\n",
      "Test: Loss: 0.596 | Acc: 80.200% (4010/5000)\n",
      "Test: Loss: 0.596 | Acc: 80.255% (4093/5100)\n",
      "Test: Loss: 0.597 | Acc: 80.212% (4171/5200)\n",
      "Test: Loss: 0.595 | Acc: 80.132% (4247/5300)\n",
      "Test: Loss: 0.599 | Acc: 80.093% (4325/5400)\n",
      "Test: Loss: 0.597 | Acc: 80.127% (4407/5500)\n",
      "Test: Loss: 0.596 | Acc: 80.214% (4492/5600)\n",
      "Test: Loss: 0.599 | Acc: 80.123% (4567/5700)\n",
      "Test: Loss: 0.596 | Acc: 80.224% (4653/5800)\n",
      "Test: Loss: 0.600 | Acc: 80.068% (4724/5900)\n",
      "Test: Loss: 0.601 | Acc: 80.050% (4803/6000)\n",
      "Test: Loss: 0.602 | Acc: 80.016% (4881/6100)\n",
      "Test: Loss: 0.602 | Acc: 80.000% (4960/6200)\n",
      "Test: Loss: 0.602 | Acc: 80.000% (5040/6300)\n",
      "Test: Loss: 0.601 | Acc: 79.953% (5117/6400)\n",
      "Test: Loss: 0.603 | Acc: 79.877% (5192/6500)\n",
      "Test: Loss: 0.603 | Acc: 79.773% (5265/6600)\n",
      "Test: Loss: 0.602 | Acc: 79.806% (5347/6700)\n",
      "Test: Loss: 0.603 | Acc: 79.735% (5422/6800)\n",
      "Test: Loss: 0.603 | Acc: 79.696% (5499/6900)\n",
      "Test: Loss: 0.605 | Acc: 79.614% (5573/7000)\n",
      "Test: Loss: 0.607 | Acc: 79.549% (5648/7100)\n",
      "Test: Loss: 0.607 | Acc: 79.528% (5726/7200)\n",
      "Test: Loss: 0.609 | Acc: 79.493% (5803/7300)\n",
      "Test: Loss: 0.608 | Acc: 79.486% (5882/7400)\n",
      "Test: Loss: 0.607 | Acc: 79.507% (5963/7500)\n",
      "Test: Loss: 0.607 | Acc: 79.421% (6036/7600)\n",
      "Test: Loss: 0.609 | Acc: 79.377% (6112/7700)\n",
      "Test: Loss: 0.609 | Acc: 79.385% (6192/7800)\n",
      "Test: Loss: 0.609 | Acc: 79.405% (6273/7900)\n",
      "Test: Loss: 0.609 | Acc: 79.425% (6354/8000)\n",
      "Test: Loss: 0.607 | Acc: 79.494% (6439/8100)\n",
      "Test: Loss: 0.607 | Acc: 79.476% (6517/8200)\n",
      "Test: Loss: 0.609 | Acc: 79.386% (6589/8300)\n",
      "Test: Loss: 0.611 | Acc: 79.310% (6662/8400)\n",
      "Test: Loss: 0.612 | Acc: 79.235% (6735/8500)\n",
      "Test: Loss: 0.612 | Acc: 79.233% (6814/8600)\n",
      "Test: Loss: 0.614 | Acc: 79.195% (6890/8700)\n",
      "Test: Loss: 0.614 | Acc: 79.193% (6969/8800)\n",
      "Test: Loss: 0.614 | Acc: 79.180% (7047/8900)\n",
      "Test: Loss: 0.616 | Acc: 79.178% (7126/9000)\n",
      "Test: Loss: 0.614 | Acc: 79.242% (7211/9100)\n",
      "Test: Loss: 0.613 | Acc: 79.283% (7294/9200)\n",
      "Test: Loss: 0.614 | Acc: 79.226% (7368/9300)\n",
      "Test: Loss: 0.615 | Acc: 79.202% (7445/9400)\n",
      "Test: Loss: 0.616 | Acc: 79.200% (7524/9500)\n",
      "Test: Loss: 0.615 | Acc: 79.219% (7605/9600)\n",
      "Test: Loss: 0.613 | Acc: 79.299% (7692/9700)\n",
      "Test: Loss: 0.614 | Acc: 79.306% (7772/9800)\n",
      "Test: Loss: 0.615 | Acc: 79.263% (7847/9900)\n",
      "Test: Loss: 0.615 | Acc: 79.230% (7923/10000)\n",
      "Train: Loss: 0.287 | Acc: 91.406% (117/128)\n",
      "Train: Loss: 0.259 | Acc: 91.797% (235/256)\n",
      "Train: Loss: 0.273 | Acc: 90.885% (349/384)\n",
      "Train: Loss: 0.270 | Acc: 91.211% (467/512)\n",
      "Train: Loss: 0.287 | Acc: 90.000% (576/640)\n",
      "Train: Loss: 0.275 | Acc: 90.495% (695/768)\n",
      "Train: Loss: 0.275 | Acc: 90.513% (811/896)\n",
      "Train: Loss: 0.269 | Acc: 90.820% (930/1024)\n",
      "Train: Loss: 0.277 | Acc: 90.191% (1039/1152)\n",
      "Train: Loss: 0.285 | Acc: 89.766% (1149/1280)\n",
      "Train: Loss: 0.283 | Acc: 89.844% (1265/1408)\n",
      "Train: Loss: 0.281 | Acc: 90.234% (1386/1536)\n",
      "Train: Loss: 0.288 | Acc: 89.904% (1496/1664)\n",
      "Train: Loss: 0.285 | Acc: 90.067% (1614/1792)\n",
      "Train: Loss: 0.277 | Acc: 90.312% (1734/1920)\n",
      "Train: Loss: 0.278 | Acc: 90.332% (1850/2048)\n",
      "Train: Loss: 0.275 | Acc: 90.349% (1966/2176)\n",
      "Train: Loss: 0.274 | Acc: 90.365% (2082/2304)\n",
      "Train: Loss: 0.271 | Acc: 90.296% (2196/2432)\n",
      "Train: Loss: 0.268 | Acc: 90.391% (2314/2560)\n",
      "Train: Loss: 0.269 | Acc: 90.290% (2427/2688)\n",
      "Train: Loss: 0.268 | Acc: 90.447% (2547/2816)\n",
      "Train: Loss: 0.274 | Acc: 90.285% (2658/2944)\n",
      "Train: Loss: 0.272 | Acc: 90.397% (2777/3072)\n",
      "Train: Loss: 0.269 | Acc: 90.531% (2897/3200)\n",
      "Train: Loss: 0.268 | Acc: 90.565% (3014/3328)\n",
      "Train: Loss: 0.271 | Acc: 90.451% (3126/3456)\n",
      "Train: Loss: 0.268 | Acc: 90.625% (3248/3584)\n",
      "Train: Loss: 0.269 | Acc: 90.598% (3363/3712)\n",
      "Train: Loss: 0.268 | Acc: 90.573% (3478/3840)\n",
      "Train: Loss: 0.268 | Acc: 90.575% (3594/3968)\n",
      "Train: Loss: 0.267 | Acc: 90.625% (3712/4096)\n",
      "Train: Loss: 0.267 | Acc: 90.672% (3830/4224)\n",
      "Train: Loss: 0.263 | Acc: 90.878% (3955/4352)\n",
      "Train: Loss: 0.263 | Acc: 90.871% (4071/4480)\n",
      "Train: Loss: 0.262 | Acc: 90.820% (4185/4608)\n",
      "Train: Loss: 0.262 | Acc: 90.815% (4301/4736)\n",
      "Train: Loss: 0.261 | Acc: 90.892% (4421/4864)\n",
      "Train: Loss: 0.260 | Acc: 90.905% (4538/4992)\n",
      "Train: Loss: 0.264 | Acc: 90.801% (4649/5120)\n",
      "Train: Loss: 0.265 | Acc: 90.720% (4761/5248)\n",
      "Train: Loss: 0.266 | Acc: 90.681% (4875/5376)\n",
      "Train: Loss: 0.267 | Acc: 90.625% (4988/5504)\n",
      "Train: Loss: 0.266 | Acc: 90.678% (5107/5632)\n",
      "Train: Loss: 0.266 | Acc: 90.712% (5225/5760)\n",
      "Train: Loss: 0.267 | Acc: 90.642% (5337/5888)\n",
      "Train: Loss: 0.266 | Acc: 90.675% (5455/6016)\n",
      "Train: Loss: 0.265 | Acc: 90.755% (5576/6144)\n",
      "Train: Loss: 0.267 | Acc: 90.673% (5687/6272)\n",
      "Train: Loss: 0.269 | Acc: 90.641% (5801/6400)\n",
      "Train: Loss: 0.270 | Acc: 90.640% (5917/6528)\n",
      "Train: Loss: 0.272 | Acc: 90.535% (6026/6656)\n",
      "Train: Loss: 0.272 | Acc: 90.537% (6142/6784)\n",
      "Train: Loss: 0.272 | Acc: 90.495% (6255/6912)\n",
      "Train: Loss: 0.276 | Acc: 90.384% (6363/7040)\n",
      "Train: Loss: 0.276 | Acc: 90.346% (6476/7168)\n",
      "Train: Loss: 0.274 | Acc: 90.406% (6596/7296)\n",
      "Train: Loss: 0.273 | Acc: 90.396% (6711/7424)\n",
      "Train: Loss: 0.274 | Acc: 90.387% (6826/7552)\n",
      "Train: Loss: 0.274 | Acc: 90.430% (6945/7680)\n",
      "Train: Loss: 0.275 | Acc: 90.407% (7059/7808)\n",
      "Train: Loss: 0.274 | Acc: 90.411% (7175/7936)\n",
      "Train: Loss: 0.276 | Acc: 90.278% (7280/8064)\n",
      "Train: Loss: 0.276 | Acc: 90.295% (7397/8192)\n",
      "Train: Loss: 0.279 | Acc: 90.240% (7508/8320)\n",
      "Train: Loss: 0.280 | Acc: 90.211% (7621/8448)\n",
      "Train: Loss: 0.280 | Acc: 90.194% (7735/8576)\n",
      "Train: Loss: 0.280 | Acc: 90.188% (7850/8704)\n",
      "Train: Loss: 0.279 | Acc: 90.195% (7966/8832)\n",
      "Train: Loss: 0.280 | Acc: 90.190% (8081/8960)\n",
      "Train: Loss: 0.281 | Acc: 90.108% (8189/9088)\n",
      "Train: Loss: 0.283 | Acc: 90.061% (8300/9216)\n",
      "Train: Loss: 0.282 | Acc: 90.101% (8419/9344)\n",
      "Train: Loss: 0.281 | Acc: 90.129% (8537/9472)\n",
      "Train: Loss: 0.281 | Acc: 90.125% (8652/9600)\n",
      "Train: Loss: 0.282 | Acc: 90.111% (8766/9728)\n",
      "Train: Loss: 0.283 | Acc: 90.067% (8877/9856)\n",
      "Train: Loss: 0.284 | Acc: 90.014% (8987/9984)\n",
      "Train: Loss: 0.285 | Acc: 90.012% (9102/10112)\n",
      "Train: Loss: 0.285 | Acc: 89.961% (9212/10240)\n",
      "Train: Loss: 0.287 | Acc: 89.863% (9317/10368)\n",
      "Train: Loss: 0.287 | Acc: 89.863% (9432/10496)\n",
      "Train: Loss: 0.287 | Acc: 89.863% (9547/10624)\n",
      "Train: Loss: 0.287 | Acc: 89.881% (9664/10752)\n",
      "Train: Loss: 0.287 | Acc: 89.835% (9774/10880)\n",
      "Train: Loss: 0.288 | Acc: 89.798% (9885/11008)\n",
      "Train: Loss: 0.288 | Acc: 89.790% (9999/11136)\n",
      "Train: Loss: 0.288 | Acc: 89.773% (10112/11264)\n",
      "Train: Loss: 0.287 | Acc: 89.800% (10230/11392)\n",
      "Train: Loss: 0.288 | Acc: 89.757% (10340/11520)\n",
      "Train: Loss: 0.289 | Acc: 89.741% (10453/11648)\n",
      "Train: Loss: 0.289 | Acc: 89.691% (10562/11776)\n",
      "Train: Loss: 0.289 | Acc: 89.701% (10678/11904)\n",
      "Train: Loss: 0.290 | Acc: 89.653% (10787/12032)\n",
      "Train: Loss: 0.290 | Acc: 89.646% (10901/12160)\n",
      "Train: Loss: 0.291 | Acc: 89.632% (11014/12288)\n",
      "Train: Loss: 0.290 | Acc: 89.642% (11130/12416)\n",
      "Train: Loss: 0.290 | Acc: 89.684% (11250/12544)\n",
      "Train: Loss: 0.292 | Acc: 89.623% (11357/12672)\n",
      "Train: Loss: 0.293 | Acc: 89.562% (11464/12800)\n",
      "Train: Loss: 0.294 | Acc: 89.527% (11574/12928)\n",
      "Train: Loss: 0.294 | Acc: 89.537% (11690/13056)\n",
      "Train: Loss: 0.294 | Acc: 89.533% (11804/13184)\n",
      "Train: Loss: 0.295 | Acc: 89.491% (11913/13312)\n",
      "Train: Loss: 0.295 | Acc: 89.516% (12031/13440)\n",
      "Train: Loss: 0.295 | Acc: 89.505% (12144/13568)\n",
      "Train: Loss: 0.294 | Acc: 89.537% (12263/13696)\n",
      "Train: Loss: 0.294 | Acc: 89.540% (12378/13824)\n",
      "Train: Loss: 0.294 | Acc: 89.550% (12494/13952)\n",
      "Train: Loss: 0.294 | Acc: 89.553% (12609/14080)\n",
      "Train: Loss: 0.295 | Acc: 89.520% (12719/14208)\n",
      "Train: Loss: 0.297 | Acc: 89.495% (12830/14336)\n",
      "Train: Loss: 0.297 | Acc: 89.498% (12945/14464)\n",
      "Train: Loss: 0.297 | Acc: 89.474% (13056/14592)\n",
      "Train: Loss: 0.296 | Acc: 89.477% (13171/14720)\n",
      "Train: Loss: 0.296 | Acc: 89.494% (13288/14848)\n",
      "Train: Loss: 0.297 | Acc: 89.456% (13397/14976)\n",
      "Train: Loss: 0.298 | Acc: 89.440% (13509/15104)\n",
      "Train: Loss: 0.298 | Acc: 89.450% (13625/15232)\n",
      "Train: Loss: 0.298 | Acc: 89.427% (13736/15360)\n",
      "Train: Loss: 0.298 | Acc: 89.411% (13848/15488)\n",
      "Train: Loss: 0.298 | Acc: 89.434% (13966/15616)\n",
      "Train: Loss: 0.298 | Acc: 89.444% (14082/15744)\n",
      "Train: Loss: 0.298 | Acc: 89.415% (14192/15872)\n",
      "Train: Loss: 0.298 | Acc: 89.419% (14307/16000)\n",
      "Train: Loss: 0.300 | Acc: 89.360% (14412/16128)\n",
      "Train: Loss: 0.299 | Acc: 89.389% (14531/16256)\n",
      "Train: Loss: 0.299 | Acc: 89.374% (14643/16384)\n",
      "Train: Loss: 0.299 | Acc: 89.402% (14762/16512)\n",
      "Train: Loss: 0.300 | Acc: 89.387% (14874/16640)\n",
      "Train: Loss: 0.299 | Acc: 89.402% (14991/16768)\n",
      "Train: Loss: 0.300 | Acc: 89.406% (15106/16896)\n",
      "Train: Loss: 0.300 | Acc: 89.409% (15221/17024)\n",
      "Train: Loss: 0.301 | Acc: 89.372% (15329/17152)\n",
      "Train: Loss: 0.301 | Acc: 89.346% (15439/17280)\n",
      "Train: Loss: 0.301 | Acc: 89.332% (15551/17408)\n",
      "Train: Loss: 0.302 | Acc: 89.285% (15657/17536)\n",
      "Train: Loss: 0.302 | Acc: 89.255% (15766/17664)\n",
      "Train: Loss: 0.302 | Acc: 89.265% (15882/17792)\n",
      "Train: Loss: 0.303 | Acc: 89.252% (15994/17920)\n",
      "Train: Loss: 0.304 | Acc: 89.251% (16108/18048)\n",
      "Train: Loss: 0.304 | Acc: 89.228% (16218/18176)\n",
      "Train: Loss: 0.305 | Acc: 89.232% (16333/18304)\n",
      "Train: Loss: 0.305 | Acc: 89.220% (16445/18432)\n",
      "Train: Loss: 0.305 | Acc: 89.240% (16563/18560)\n",
      "Train: Loss: 0.306 | Acc: 89.218% (16673/18688)\n",
      "Train: Loss: 0.306 | Acc: 89.243% (16792/18816)\n",
      "Train: Loss: 0.306 | Acc: 89.247% (16907/18944)\n",
      "Train: Loss: 0.306 | Acc: 89.230% (17018/19072)\n",
      "Train: Loss: 0.306 | Acc: 89.214% (17129/19200)\n",
      "Train: Loss: 0.306 | Acc: 89.207% (17242/19328)\n",
      "Train: Loss: 0.307 | Acc: 89.196% (17354/19456)\n",
      "Train: Loss: 0.308 | Acc: 89.160% (17461/19584)\n",
      "Train: Loss: 0.308 | Acc: 89.174% (17578/19712)\n",
      "Train: Loss: 0.308 | Acc: 89.143% (17686/19840)\n",
      "Train: Loss: 0.308 | Acc: 89.133% (17798/19968)\n",
      "Train: Loss: 0.309 | Acc: 89.117% (17909/20096)\n",
      "Train: Loss: 0.308 | Acc: 89.132% (18026/20224)\n",
      "Train: Loss: 0.309 | Acc: 89.117% (18137/20352)\n",
      "Train: Loss: 0.310 | Acc: 89.102% (18248/20480)\n",
      "Train: Loss: 0.311 | Acc: 89.082% (18358/20608)\n",
      "Train: Loss: 0.312 | Acc: 89.058% (18467/20736)\n",
      "Train: Loss: 0.312 | Acc: 89.053% (18580/20864)\n",
      "Train: Loss: 0.312 | Acc: 89.053% (18694/20992)\n",
      "Train: Loss: 0.312 | Acc: 89.048% (18807/21120)\n",
      "Train: Loss: 0.313 | Acc: 89.034% (18918/21248)\n",
      "Train: Loss: 0.313 | Acc: 89.016% (19028/21376)\n",
      "Train: Loss: 0.313 | Acc: 89.021% (19143/21504)\n",
      "Train: Loss: 0.313 | Acc: 88.993% (19251/21632)\n",
      "Train: Loss: 0.314 | Acc: 88.938% (19353/21760)\n",
      "Train: Loss: 0.314 | Acc: 88.916% (19462/21888)\n",
      "Train: Loss: 0.314 | Acc: 88.931% (19579/22016)\n",
      "Train: Loss: 0.315 | Acc: 88.895% (19685/22144)\n",
      "Train: Loss: 0.315 | Acc: 88.896% (19799/22272)\n",
      "Train: Loss: 0.315 | Acc: 88.893% (19912/22400)\n",
      "Train: Loss: 0.316 | Acc: 88.881% (20023/22528)\n",
      "Train: Loss: 0.316 | Acc: 88.877% (20136/22656)\n",
      "Train: Loss: 0.316 | Acc: 88.891% (20253/22784)\n",
      "Train: Loss: 0.316 | Acc: 88.888% (20366/22912)\n",
      "Train: Loss: 0.316 | Acc: 88.872% (20476/23040)\n",
      "Train: Loss: 0.316 | Acc: 88.868% (20589/23168)\n",
      "Train: Loss: 0.317 | Acc: 88.861% (20701/23296)\n",
      "Train: Loss: 0.317 | Acc: 88.853% (20813/23424)\n",
      "Train: Loss: 0.317 | Acc: 88.846% (20925/23552)\n",
      "Train: Loss: 0.317 | Acc: 88.834% (21036/23680)\n",
      "Train: Loss: 0.317 | Acc: 88.836% (21150/23808)\n",
      "Train: Loss: 0.317 | Acc: 88.820% (21260/23936)\n",
      "Train: Loss: 0.318 | Acc: 88.805% (21370/24064)\n",
      "Train: Loss: 0.317 | Acc: 88.827% (21489/24192)\n",
      "Train: Loss: 0.317 | Acc: 88.828% (21603/24320)\n",
      "Train: Loss: 0.317 | Acc: 88.821% (21715/24448)\n",
      "Train: Loss: 0.317 | Acc: 88.839% (21833/24576)\n",
      "Train: Loss: 0.318 | Acc: 88.840% (21947/24704)\n",
      "Train: Loss: 0.318 | Acc: 88.837% (22060/24832)\n",
      "Train: Loss: 0.318 | Acc: 88.838% (22174/24960)\n",
      "Train: Loss: 0.317 | Acc: 88.835% (22287/25088)\n",
      "Train: Loss: 0.318 | Acc: 88.801% (22392/25216)\n",
      "Train: Loss: 0.318 | Acc: 88.790% (22503/25344)\n",
      "Train: Loss: 0.318 | Acc: 88.792% (22617/25472)\n",
      "Train: Loss: 0.318 | Acc: 88.785% (22729/25600)\n",
      "Train: Loss: 0.319 | Acc: 88.790% (22844/25728)\n",
      "Train: Loss: 0.318 | Acc: 88.792% (22958/25856)\n",
      "Train: Loss: 0.318 | Acc: 88.797% (23073/25984)\n",
      "Train: Loss: 0.318 | Acc: 88.817% (23192/26112)\n",
      "Train: Loss: 0.318 | Acc: 88.811% (23304/26240)\n",
      "Train: Loss: 0.318 | Acc: 88.820% (23420/26368)\n",
      "Train: Loss: 0.318 | Acc: 88.813% (23532/26496)\n",
      "Train: Loss: 0.319 | Acc: 88.811% (23645/26624)\n",
      "Train: Loss: 0.319 | Acc: 88.805% (23757/26752)\n",
      "Train: Loss: 0.319 | Acc: 88.802% (23870/26880)\n",
      "Train: Loss: 0.319 | Acc: 88.800% (23983/27008)\n",
      "Train: Loss: 0.319 | Acc: 88.786% (24093/27136)\n",
      "Train: Loss: 0.319 | Acc: 88.795% (24209/27264)\n",
      "Train: Loss: 0.320 | Acc: 88.774% (24317/27392)\n",
      "Train: Loss: 0.320 | Acc: 88.754% (24425/27520)\n",
      "Train: Loss: 0.320 | Acc: 88.741% (24535/27648)\n",
      "Train: Loss: 0.320 | Acc: 88.735% (24647/27776)\n",
      "Train: Loss: 0.320 | Acc: 88.736% (24761/27904)\n",
      "Train: Loss: 0.320 | Acc: 88.738% (24875/28032)\n",
      "Train: Loss: 0.320 | Acc: 88.736% (24988/28160)\n",
      "Train: Loss: 0.320 | Acc: 88.737% (25102/28288)\n",
      "Train: Loss: 0.321 | Acc: 88.711% (25208/28416)\n",
      "Train: Loss: 0.321 | Acc: 88.709% (25321/28544)\n",
      "Train: Loss: 0.321 | Acc: 88.724% (25439/28672)\n",
      "Train: Loss: 0.321 | Acc: 88.701% (25546/28800)\n",
      "Train: Loss: 0.321 | Acc: 88.700% (25659/28928)\n",
      "Train: Loss: 0.321 | Acc: 88.698% (25772/29056)\n",
      "Train: Loss: 0.321 | Acc: 88.696% (25885/29184)\n",
      "Train: Loss: 0.321 | Acc: 88.691% (25997/29312)\n",
      "Train: Loss: 0.321 | Acc: 88.699% (26113/29440)\n",
      "Train: Loss: 0.322 | Acc: 88.680% (26221/29568)\n",
      "Train: Loss: 0.323 | Acc: 88.642% (26323/29696)\n",
      "Train: Loss: 0.323 | Acc: 88.620% (26430/29824)\n",
      "Train: Loss: 0.324 | Acc: 88.602% (26538/29952)\n",
      "Train: Loss: 0.324 | Acc: 88.594% (26649/30080)\n",
      "Train: Loss: 0.324 | Acc: 88.599% (26764/30208)\n",
      "Train: Loss: 0.324 | Acc: 88.594% (26876/30336)\n",
      "Train: Loss: 0.325 | Acc: 88.573% (26983/30464)\n",
      "Train: Loss: 0.325 | Acc: 88.572% (27096/30592)\n",
      "Train: Loss: 0.325 | Acc: 88.577% (27211/30720)\n",
      "Train: Loss: 0.325 | Acc: 88.592% (27329/30848)\n",
      "Train: Loss: 0.326 | Acc: 88.565% (27434/30976)\n",
      "Train: Loss: 0.326 | Acc: 88.551% (27543/31104)\n",
      "Train: Loss: 0.326 | Acc: 88.541% (27653/31232)\n",
      "Train: Loss: 0.327 | Acc: 88.524% (27761/31360)\n",
      "Train: Loss: 0.327 | Acc: 88.532% (27877/31488)\n",
      "Train: Loss: 0.327 | Acc: 88.515% (27985/31616)\n",
      "Train: Loss: 0.327 | Acc: 88.505% (28095/31744)\n",
      "Train: Loss: 0.328 | Acc: 88.498% (28206/31872)\n",
      "Train: Loss: 0.328 | Acc: 88.484% (28315/32000)\n",
      "Train: Loss: 0.329 | Acc: 88.480% (28427/32128)\n",
      "Train: Loss: 0.329 | Acc: 88.483% (28541/32256)\n",
      "Train: Loss: 0.329 | Acc: 88.473% (28651/32384)\n",
      "Train: Loss: 0.329 | Acc: 88.447% (28756/32512)\n",
      "Train: Loss: 0.330 | Acc: 88.441% (28867/32640)\n",
      "Train: Loss: 0.330 | Acc: 88.431% (28977/32768)\n",
      "Train: Loss: 0.330 | Acc: 88.430% (29090/32896)\n",
      "Train: Loss: 0.330 | Acc: 88.408% (29196/33024)\n",
      "Train: Loss: 0.331 | Acc: 88.378% (29299/33152)\n",
      "Train: Loss: 0.331 | Acc: 88.359% (29406/33280)\n",
      "Train: Loss: 0.331 | Acc: 88.359% (29519/33408)\n",
      "Train: Loss: 0.330 | Acc: 88.386% (29641/33536)\n",
      "Train: Loss: 0.330 | Acc: 88.379% (29752/33664)\n",
      "Train: Loss: 0.330 | Acc: 88.385% (29867/33792)\n",
      "Train: Loss: 0.330 | Acc: 88.381% (29979/33920)\n",
      "Train: Loss: 0.331 | Acc: 88.372% (30089/34048)\n",
      "Train: Loss: 0.331 | Acc: 88.372% (30202/34176)\n",
      "Train: Loss: 0.331 | Acc: 88.375% (30316/34304)\n",
      "Train: Loss: 0.330 | Acc: 88.380% (30431/34432)\n",
      "Train: Loss: 0.331 | Acc: 88.368% (30540/34560)\n",
      "Train: Loss: 0.331 | Acc: 88.353% (30648/34688)\n",
      "Train: Loss: 0.331 | Acc: 88.353% (30761/34816)\n",
      "Train: Loss: 0.331 | Acc: 88.361% (30877/34944)\n",
      "Train: Loss: 0.331 | Acc: 88.372% (30994/35072)\n",
      "Train: Loss: 0.331 | Acc: 88.358% (31102/35200)\n",
      "Train: Loss: 0.331 | Acc: 88.352% (31213/35328)\n",
      "Train: Loss: 0.331 | Acc: 88.352% (31326/35456)\n",
      "Train: Loss: 0.332 | Acc: 88.352% (31439/35584)\n",
      "Train: Loss: 0.331 | Acc: 88.354% (31553/35712)\n",
      "Train: Loss: 0.331 | Acc: 88.354% (31666/35840)\n",
      "Train: Loss: 0.332 | Acc: 88.356% (31780/35968)\n",
      "Train: Loss: 0.332 | Acc: 88.348% (31890/36096)\n",
      "Train: Loss: 0.333 | Acc: 88.331% (31997/36224)\n",
      "Train: Loss: 0.332 | Acc: 88.336% (32112/36352)\n",
      "Train: Loss: 0.332 | Acc: 88.339% (32226/36480)\n",
      "Train: Loss: 0.332 | Acc: 88.347% (32342/36608)\n",
      "Train: Loss: 0.333 | Acc: 88.328% (32448/36736)\n",
      "Train: Loss: 0.333 | Acc: 88.333% (32563/36864)\n",
      "Train: Loss: 0.333 | Acc: 88.341% (32679/36992)\n",
      "Train: Loss: 0.333 | Acc: 88.335% (32790/37120)\n",
      "Train: Loss: 0.333 | Acc: 88.335% (32903/37248)\n",
      "Train: Loss: 0.333 | Acc: 88.348% (33021/37376)\n",
      "Train: Loss: 0.333 | Acc: 88.359% (33138/37504)\n",
      "Train: Loss: 0.333 | Acc: 88.361% (33252/37632)\n",
      "Train: Loss: 0.333 | Acc: 88.345% (33359/37760)\n",
      "Train: Loss: 0.333 | Acc: 88.345% (33472/37888)\n",
      "Train: Loss: 0.334 | Acc: 88.329% (33579/38016)\n",
      "Train: Loss: 0.334 | Acc: 88.323% (33690/38144)\n",
      "Train: Loss: 0.334 | Acc: 88.331% (33806/38272)\n",
      "Train: Loss: 0.335 | Acc: 88.318% (33914/38400)\n",
      "Train: Loss: 0.335 | Acc: 88.305% (34022/38528)\n",
      "Train: Loss: 0.335 | Acc: 88.292% (34130/38656)\n",
      "Train: Loss: 0.336 | Acc: 88.276% (34237/38784)\n",
      "Train: Loss: 0.336 | Acc: 88.268% (34347/38912)\n",
      "Train: Loss: 0.336 | Acc: 88.281% (34465/39040)\n",
      "Train: Loss: 0.336 | Acc: 88.281% (34578/39168)\n",
      "Train: Loss: 0.336 | Acc: 88.261% (34683/39296)\n",
      "Train: Loss: 0.336 | Acc: 88.271% (34800/39424)\n",
      "Train: Loss: 0.336 | Acc: 88.264% (34910/39552)\n",
      "Train: Loss: 0.336 | Acc: 88.271% (35026/39680)\n",
      "Train: Loss: 0.336 | Acc: 88.251% (35131/39808)\n",
      "Train: Loss: 0.336 | Acc: 88.251% (35244/39936)\n",
      "Train: Loss: 0.336 | Acc: 88.246% (35355/40064)\n",
      "Train: Loss: 0.336 | Acc: 88.246% (35468/40192)\n",
      "Train: Loss: 0.336 | Acc: 88.244% (35580/40320)\n",
      "Train: Loss: 0.336 | Acc: 88.259% (35699/40448)\n",
      "Train: Loss: 0.337 | Acc: 88.237% (35803/40576)\n",
      "Train: Loss: 0.337 | Acc: 88.227% (35912/40704)\n",
      "Train: Loss: 0.338 | Acc: 88.215% (36020/40832)\n",
      "Train: Loss: 0.338 | Acc: 88.193% (36124/40960)\n",
      "Train: Loss: 0.339 | Acc: 88.179% (36231/41088)\n",
      "Train: Loss: 0.339 | Acc: 88.174% (36342/41216)\n",
      "Train: Loss: 0.339 | Acc: 88.168% (36452/41344)\n",
      "Train: Loss: 0.339 | Acc: 88.173% (36567/41472)\n",
      "Train: Loss: 0.339 | Acc: 88.173% (36680/41600)\n",
      "Train: Loss: 0.340 | Acc: 88.161% (36788/41728)\n",
      "Train: Loss: 0.340 | Acc: 88.150% (36896/41856)\n",
      "Train: Loss: 0.340 | Acc: 88.141% (37005/41984)\n",
      "Train: Loss: 0.340 | Acc: 88.139% (37117/42112)\n",
      "Train: Loss: 0.340 | Acc: 88.142% (37231/42240)\n",
      "Train: Loss: 0.340 | Acc: 88.133% (37340/42368)\n",
      "Train: Loss: 0.341 | Acc: 88.124% (37449/42496)\n",
      "Train: Loss: 0.341 | Acc: 88.110% (37556/42624)\n",
      "Train: Loss: 0.341 | Acc: 88.106% (37667/42752)\n",
      "Train: Loss: 0.341 | Acc: 88.097% (37776/42880)\n",
      "Train: Loss: 0.341 | Acc: 88.095% (37888/43008)\n",
      "Train: Loss: 0.341 | Acc: 88.103% (38004/43136)\n",
      "Train: Loss: 0.341 | Acc: 88.099% (38115/43264)\n",
      "Train: Loss: 0.341 | Acc: 88.097% (38227/43392)\n",
      "Train: Loss: 0.341 | Acc: 88.097% (38340/43520)\n",
      "Train: Loss: 0.341 | Acc: 88.105% (38456/43648)\n",
      "Train: Loss: 0.341 | Acc: 88.092% (38563/43776)\n",
      "Train: Loss: 0.341 | Acc: 88.090% (38675/43904)\n",
      "Train: Loss: 0.341 | Acc: 88.090% (38788/44032)\n",
      "Train: Loss: 0.341 | Acc: 88.084% (38898/44160)\n",
      "Train: Loss: 0.341 | Acc: 88.087% (39012/44288)\n",
      "Train: Loss: 0.341 | Acc: 88.099% (39130/44416)\n",
      "Train: Loss: 0.341 | Acc: 88.090% (39239/44544)\n",
      "Train: Loss: 0.341 | Acc: 88.095% (39354/44672)\n",
      "Train: Loss: 0.340 | Acc: 88.107% (39472/44800)\n",
      "Train: Loss: 0.341 | Acc: 88.099% (39581/44928)\n",
      "Train: Loss: 0.340 | Acc: 88.095% (39692/45056)\n",
      "Train: Loss: 0.341 | Acc: 88.095% (39805/45184)\n",
      "Train: Loss: 0.340 | Acc: 88.109% (39924/45312)\n",
      "Train: Loss: 0.340 | Acc: 88.116% (40040/45440)\n",
      "Train: Loss: 0.340 | Acc: 88.125% (40157/45568)\n",
      "Train: Loss: 0.340 | Acc: 88.119% (40267/45696)\n",
      "Train: Loss: 0.340 | Acc: 88.118% (40379/45824)\n",
      "Train: Loss: 0.340 | Acc: 88.120% (40493/45952)\n",
      "Train: Loss: 0.340 | Acc: 88.112% (40602/46080)\n",
      "Train: Loss: 0.340 | Acc: 88.119% (40718/46208)\n",
      "Train: Loss: 0.339 | Acc: 88.137% (40839/46336)\n",
      "Train: Loss: 0.340 | Acc: 88.120% (40944/46464)\n",
      "Train: Loss: 0.340 | Acc: 88.125% (41059/46592)\n",
      "Train: Loss: 0.340 | Acc: 88.134% (41176/46720)\n",
      "Train: Loss: 0.339 | Acc: 88.143% (41293/46848)\n",
      "Train: Loss: 0.339 | Acc: 88.147% (41408/46976)\n",
      "Train: Loss: 0.339 | Acc: 88.143% (41519/47104)\n",
      "Train: Loss: 0.339 | Acc: 88.148% (41634/47232)\n",
      "Train: Loss: 0.339 | Acc: 88.138% (41742/47360)\n",
      "Train: Loss: 0.340 | Acc: 88.128% (41850/47488)\n",
      "Train: Loss: 0.340 | Acc: 88.126% (41962/47616)\n",
      "Train: Loss: 0.340 | Acc: 88.114% (42069/47744)\n",
      "Train: Loss: 0.340 | Acc: 88.106% (42178/47872)\n",
      "Train: Loss: 0.340 | Acc: 88.100% (42288/48000)\n",
      "Train: Loss: 0.340 | Acc: 88.100% (42401/48128)\n",
      "Train: Loss: 0.340 | Acc: 88.097% (42512/48256)\n",
      "Train: Loss: 0.340 | Acc: 88.091% (42622/48384)\n",
      "Train: Loss: 0.340 | Acc: 88.102% (42740/48512)\n",
      "Train: Loss: 0.340 | Acc: 88.096% (42850/48640)\n",
      "Train: Loss: 0.340 | Acc: 88.099% (42964/48768)\n",
      "Train: Loss: 0.340 | Acc: 88.097% (43076/48896)\n",
      "Train: Loss: 0.340 | Acc: 88.096% (43188/49024)\n",
      "Train: Loss: 0.340 | Acc: 88.094% (43300/49152)\n",
      "Train: Loss: 0.340 | Acc: 88.097% (43414/49280)\n",
      "Train: Loss: 0.340 | Acc: 88.083% (43520/49408)\n",
      "Train: Loss: 0.341 | Acc: 88.079% (43631/49536)\n",
      "Train: Loss: 0.341 | Acc: 88.068% (43738/49664)\n",
      "Train: Loss: 0.341 | Acc: 88.058% (43846/49792)\n",
      "Train: Loss: 0.341 | Acc: 88.051% (43955/49920)\n",
      "Train: Loss: 0.341 | Acc: 88.050% (44025/50000)\n",
      "Test: Loss: 0.661 | Acc: 75.000% (75/100)\n",
      "Test: Loss: 0.630 | Acc: 75.500% (151/200)\n",
      "Test: Loss: 0.644 | Acc: 76.667% (230/300)\n",
      "Test: Loss: 0.653 | Acc: 77.500% (310/400)\n",
      "Test: Loss: 0.645 | Acc: 77.200% (386/500)\n",
      "Test: Loss: 0.610 | Acc: 79.000% (474/600)\n",
      "Test: Loss: 0.597 | Acc: 79.286% (555/700)\n",
      "Test: Loss: 0.614 | Acc: 78.875% (631/800)\n",
      "Test: Loss: 0.632 | Acc: 78.444% (706/900)\n",
      "Test: Loss: 0.630 | Acc: 78.600% (786/1000)\n",
      "Test: Loss: 0.641 | Acc: 78.545% (864/1100)\n",
      "Test: Loss: 0.639 | Acc: 78.750% (945/1200)\n",
      "Test: Loss: 0.652 | Acc: 78.231% (1017/1300)\n",
      "Test: Loss: 0.645 | Acc: 78.214% (1095/1400)\n",
      "Test: Loss: 0.647 | Acc: 78.333% (1175/1500)\n",
      "Test: Loss: 0.642 | Acc: 78.625% (1258/1600)\n",
      "Test: Loss: 0.647 | Acc: 78.706% (1338/1700)\n",
      "Test: Loss: 0.654 | Acc: 78.500% (1413/1800)\n",
      "Test: Loss: 0.658 | Acc: 78.368% (1489/1900)\n",
      "Test: Loss: 0.661 | Acc: 78.400% (1568/2000)\n",
      "Test: Loss: 0.662 | Acc: 78.333% (1645/2100)\n",
      "Test: Loss: 0.662 | Acc: 78.409% (1725/2200)\n",
      "Test: Loss: 0.663 | Acc: 78.348% (1802/2300)\n",
      "Test: Loss: 0.665 | Acc: 78.375% (1881/2400)\n",
      "Test: Loss: 0.665 | Acc: 78.280% (1957/2500)\n",
      "Test: Loss: 0.671 | Acc: 78.231% (2034/2600)\n",
      "Test: Loss: 0.671 | Acc: 78.148% (2110/2700)\n",
      "Test: Loss: 0.673 | Acc: 78.000% (2184/2800)\n",
      "Test: Loss: 0.675 | Acc: 78.069% (2264/2900)\n",
      "Test: Loss: 0.671 | Acc: 78.300% (2349/3000)\n",
      "Test: Loss: 0.675 | Acc: 78.129% (2422/3100)\n",
      "Test: Loss: 0.675 | Acc: 78.250% (2504/3200)\n",
      "Test: Loss: 0.674 | Acc: 78.303% (2584/3300)\n",
      "Test: Loss: 0.679 | Acc: 78.147% (2657/3400)\n",
      "Test: Loss: 0.680 | Acc: 78.086% (2733/3500)\n",
      "Test: Loss: 0.680 | Acc: 78.167% (2814/3600)\n",
      "Test: Loss: 0.680 | Acc: 78.189% (2893/3700)\n",
      "Test: Loss: 0.680 | Acc: 78.132% (2969/3800)\n",
      "Test: Loss: 0.678 | Acc: 78.077% (3045/3900)\n",
      "Test: Loss: 0.672 | Acc: 78.250% (3130/4000)\n",
      "Test: Loss: 0.674 | Acc: 78.195% (3206/4100)\n",
      "Test: Loss: 0.670 | Acc: 78.286% (3288/4200)\n",
      "Test: Loss: 0.667 | Acc: 78.395% (3371/4300)\n",
      "Test: Loss: 0.667 | Acc: 78.409% (3450/4400)\n",
      "Test: Loss: 0.669 | Acc: 78.311% (3524/4500)\n",
      "Test: Loss: 0.671 | Acc: 78.174% (3596/4600)\n",
      "Test: Loss: 0.671 | Acc: 78.128% (3672/4700)\n",
      "Test: Loss: 0.671 | Acc: 78.208% (3754/4800)\n",
      "Test: Loss: 0.666 | Acc: 78.347% (3839/4900)\n",
      "Test: Loss: 0.670 | Acc: 78.220% (3911/5000)\n",
      "Test: Loss: 0.667 | Acc: 78.314% (3994/5100)\n",
      "Test: Loss: 0.670 | Acc: 78.288% (4071/5200)\n",
      "Test: Loss: 0.670 | Acc: 78.358% (4153/5300)\n",
      "Test: Loss: 0.669 | Acc: 78.444% (4236/5400)\n",
      "Test: Loss: 0.669 | Acc: 78.436% (4314/5500)\n",
      "Test: Loss: 0.670 | Acc: 78.411% (4391/5600)\n",
      "Test: Loss: 0.670 | Acc: 78.333% (4465/5700)\n",
      "Test: Loss: 0.665 | Acc: 78.483% (4552/5800)\n",
      "Test: Loss: 0.667 | Acc: 78.390% (4625/5900)\n",
      "Test: Loss: 0.670 | Acc: 78.267% (4696/6000)\n",
      "Test: Loss: 0.667 | Acc: 78.344% (4779/6100)\n",
      "Test: Loss: 0.667 | Acc: 78.339% (4857/6200)\n",
      "Test: Loss: 0.666 | Acc: 78.365% (4937/6300)\n",
      "Test: Loss: 0.665 | Acc: 78.406% (5018/6400)\n",
      "Test: Loss: 0.664 | Acc: 78.400% (5096/6500)\n",
      "Test: Loss: 0.665 | Acc: 78.394% (5174/6600)\n",
      "Test: Loss: 0.663 | Acc: 78.478% (5258/6700)\n",
      "Test: Loss: 0.663 | Acc: 78.426% (5333/6800)\n",
      "Test: Loss: 0.665 | Acc: 78.420% (5411/6900)\n",
      "Test: Loss: 0.671 | Acc: 78.257% (5478/7000)\n",
      "Test: Loss: 0.671 | Acc: 78.282% (5558/7100)\n",
      "Test: Loss: 0.673 | Acc: 78.222% (5632/7200)\n",
      "Test: Loss: 0.672 | Acc: 78.192% (5708/7300)\n",
      "Test: Loss: 0.672 | Acc: 78.189% (5786/7400)\n",
      "Test: Loss: 0.675 | Acc: 78.067% (5855/7500)\n",
      "Test: Loss: 0.674 | Acc: 78.092% (5935/7600)\n",
      "Test: Loss: 0.677 | Acc: 78.052% (6010/7700)\n",
      "Test: Loss: 0.678 | Acc: 78.000% (6084/7800)\n",
      "Test: Loss: 0.678 | Acc: 78.000% (6162/7900)\n",
      "Test: Loss: 0.677 | Acc: 78.050% (6244/8000)\n",
      "Test: Loss: 0.676 | Acc: 78.049% (6322/8100)\n",
      "Test: Loss: 0.676 | Acc: 78.024% (6398/8200)\n",
      "Test: Loss: 0.677 | Acc: 77.952% (6470/8300)\n",
      "Test: Loss: 0.678 | Acc: 77.893% (6543/8400)\n",
      "Test: Loss: 0.680 | Acc: 77.824% (6615/8500)\n",
      "Test: Loss: 0.681 | Acc: 77.802% (6691/8600)\n",
      "Test: Loss: 0.682 | Acc: 77.816% (6770/8700)\n",
      "Test: Loss: 0.682 | Acc: 77.841% (6850/8800)\n",
      "Test: Loss: 0.682 | Acc: 77.910% (6934/8900)\n",
      "Test: Loss: 0.683 | Acc: 77.900% (7011/9000)\n",
      "Test: Loss: 0.682 | Acc: 77.868% (7086/9100)\n",
      "Test: Loss: 0.680 | Acc: 77.891% (7166/9200)\n",
      "Test: Loss: 0.682 | Acc: 77.849% (7240/9300)\n",
      "Test: Loss: 0.683 | Acc: 77.830% (7316/9400)\n",
      "Test: Loss: 0.681 | Acc: 77.821% (7393/9500)\n",
      "Test: Loss: 0.679 | Acc: 77.875% (7476/9600)\n",
      "Test: Loss: 0.679 | Acc: 77.887% (7555/9700)\n",
      "Test: Loss: 0.682 | Acc: 77.837% (7628/9800)\n",
      "Test: Loss: 0.682 | Acc: 77.808% (7703/9900)\n",
      "Test: Loss: 0.681 | Acc: 77.830% (7783/10000)\n",
      "Train: Loss: 0.228 | Acc: 91.406% (117/128)\n",
      "Train: Loss: 0.227 | Acc: 92.188% (236/256)\n",
      "Train: Loss: 0.205 | Acc: 92.708% (356/384)\n",
      "Train: Loss: 0.217 | Acc: 92.188% (472/512)\n",
      "Train: Loss: 0.216 | Acc: 92.344% (591/640)\n",
      "Train: Loss: 0.220 | Acc: 92.318% (709/768)\n",
      "Train: Loss: 0.226 | Acc: 92.076% (825/896)\n",
      "Train: Loss: 0.245 | Acc: 91.406% (936/1024)\n",
      "Train: Loss: 0.250 | Acc: 90.885% (1047/1152)\n",
      "Train: Loss: 0.251 | Acc: 90.703% (1161/1280)\n",
      "Train: Loss: 0.248 | Acc: 91.051% (1282/1408)\n",
      "Train: Loss: 0.253 | Acc: 91.081% (1399/1536)\n",
      "Train: Loss: 0.250 | Acc: 91.226% (1518/1664)\n",
      "Train: Loss: 0.247 | Acc: 91.406% (1638/1792)\n",
      "Train: Loss: 0.246 | Acc: 91.615% (1759/1920)\n",
      "Train: Loss: 0.241 | Acc: 91.846% (1881/2048)\n",
      "Train: Loss: 0.240 | Acc: 91.866% (1999/2176)\n",
      "Train: Loss: 0.235 | Acc: 92.057% (2121/2304)\n",
      "Train: Loss: 0.232 | Acc: 92.270% (2244/2432)\n",
      "Train: Loss: 0.230 | Acc: 92.383% (2365/2560)\n",
      "Train: Loss: 0.229 | Acc: 92.485% (2486/2688)\n",
      "Train: Loss: 0.229 | Acc: 92.472% (2604/2816)\n",
      "Train: Loss: 0.227 | Acc: 92.493% (2723/2944)\n",
      "Train: Loss: 0.227 | Acc: 92.480% (2841/3072)\n",
      "Train: Loss: 0.226 | Acc: 92.594% (2963/3200)\n",
      "Train: Loss: 0.228 | Acc: 92.608% (3082/3328)\n",
      "Train: Loss: 0.228 | Acc: 92.593% (3200/3456)\n",
      "Train: Loss: 0.225 | Acc: 92.773% (3325/3584)\n",
      "Train: Loss: 0.229 | Acc: 92.726% (3442/3712)\n",
      "Train: Loss: 0.229 | Acc: 92.656% (3558/3840)\n",
      "Train: Loss: 0.229 | Acc: 92.540% (3672/3968)\n",
      "Train: Loss: 0.228 | Acc: 92.505% (3789/4096)\n",
      "Train: Loss: 0.228 | Acc: 92.448% (3905/4224)\n",
      "Train: Loss: 0.229 | Acc: 92.371% (4020/4352)\n",
      "Train: Loss: 0.230 | Acc: 92.388% (4139/4480)\n",
      "Train: Loss: 0.229 | Acc: 92.361% (4256/4608)\n",
      "Train: Loss: 0.232 | Acc: 92.314% (4372/4736)\n",
      "Train: Loss: 0.230 | Acc: 92.331% (4491/4864)\n",
      "Train: Loss: 0.229 | Acc: 92.368% (4611/4992)\n",
      "Train: Loss: 0.229 | Acc: 92.324% (4727/5120)\n",
      "Train: Loss: 0.229 | Acc: 92.340% (4846/5248)\n",
      "Train: Loss: 0.228 | Acc: 92.318% (4963/5376)\n",
      "Train: Loss: 0.227 | Acc: 92.333% (5082/5504)\n",
      "Train: Loss: 0.227 | Acc: 92.365% (5202/5632)\n",
      "Train: Loss: 0.228 | Acc: 92.309% (5317/5760)\n",
      "Train: Loss: 0.228 | Acc: 92.272% (5433/5888)\n",
      "Train: Loss: 0.228 | Acc: 92.320% (5554/6016)\n",
      "Train: Loss: 0.227 | Acc: 92.350% (5674/6144)\n",
      "Train: Loss: 0.226 | Acc: 92.395% (5795/6272)\n",
      "Train: Loss: 0.224 | Acc: 92.484% (5919/6400)\n",
      "Train: Loss: 0.226 | Acc: 92.448% (6035/6528)\n",
      "Train: Loss: 0.225 | Acc: 92.428% (6152/6656)\n",
      "Train: Loss: 0.224 | Acc: 92.468% (6273/6784)\n",
      "Train: Loss: 0.227 | Acc: 92.347% (6383/6912)\n",
      "Train: Loss: 0.226 | Acc: 92.358% (6502/7040)\n",
      "Train: Loss: 0.225 | Acc: 92.369% (6621/7168)\n",
      "Train: Loss: 0.226 | Acc: 92.311% (6735/7296)\n",
      "Train: Loss: 0.226 | Acc: 92.309% (6853/7424)\n",
      "Train: Loss: 0.228 | Acc: 92.227% (6965/7552)\n",
      "Train: Loss: 0.229 | Acc: 92.188% (7080/7680)\n",
      "Train: Loss: 0.229 | Acc: 92.188% (7198/7808)\n",
      "Train: Loss: 0.230 | Acc: 92.188% (7316/7936)\n",
      "Train: Loss: 0.230 | Acc: 92.188% (7434/8064)\n",
      "Train: Loss: 0.230 | Acc: 92.200% (7553/8192)\n",
      "Train: Loss: 0.230 | Acc: 92.236% (7674/8320)\n",
      "Train: Loss: 0.231 | Acc: 92.211% (7790/8448)\n",
      "Train: Loss: 0.231 | Acc: 92.211% (7908/8576)\n",
      "Train: Loss: 0.231 | Acc: 92.210% (8026/8704)\n",
      "Train: Loss: 0.230 | Acc: 92.188% (8142/8832)\n",
      "Train: Loss: 0.231 | Acc: 92.165% (8258/8960)\n",
      "Train: Loss: 0.230 | Acc: 92.165% (8376/9088)\n",
      "Train: Loss: 0.231 | Acc: 92.144% (8492/9216)\n",
      "Train: Loss: 0.232 | Acc: 92.102% (8606/9344)\n",
      "Train: Loss: 0.232 | Acc: 92.082% (8722/9472)\n",
      "Train: Loss: 0.231 | Acc: 92.104% (8842/9600)\n",
      "Train: Loss: 0.231 | Acc: 92.146% (8964/9728)\n",
      "Train: Loss: 0.231 | Acc: 92.127% (9080/9856)\n",
      "Train: Loss: 0.232 | Acc: 92.117% (9197/9984)\n",
      "Train: Loss: 0.233 | Acc: 92.108% (9314/10112)\n",
      "Train: Loss: 0.233 | Acc: 92.080% (9429/10240)\n",
      "Train: Loss: 0.234 | Acc: 92.062% (9545/10368)\n",
      "Train: Loss: 0.235 | Acc: 92.035% (9660/10496)\n",
      "Train: Loss: 0.235 | Acc: 92.065% (9781/10624)\n",
      "Train: Loss: 0.235 | Acc: 92.048% (9897/10752)\n",
      "Train: Loss: 0.234 | Acc: 92.086% (10019/10880)\n",
      "Train: Loss: 0.236 | Acc: 92.042% (10132/11008)\n",
      "Train: Loss: 0.236 | Acc: 92.026% (10248/11136)\n",
      "Train: Loss: 0.235 | Acc: 92.019% (10365/11264)\n",
      "Train: Loss: 0.235 | Acc: 92.029% (10484/11392)\n",
      "Train: Loss: 0.235 | Acc: 92.040% (10603/11520)\n",
      "Train: Loss: 0.234 | Acc: 92.076% (10725/11648)\n",
      "Train: Loss: 0.235 | Acc: 92.052% (10840/11776)\n",
      "Train: Loss: 0.235 | Acc: 92.053% (10958/11904)\n",
      "Train: Loss: 0.237 | Acc: 91.988% (11068/12032)\n",
      "Train: Loss: 0.238 | Acc: 91.998% (11187/12160)\n",
      "Train: Loss: 0.238 | Acc: 91.984% (11303/12288)\n",
      "Train: Loss: 0.238 | Acc: 91.970% (11419/12416)\n",
      "Train: Loss: 0.238 | Acc: 91.964% (11536/12544)\n",
      "Train: Loss: 0.237 | Acc: 91.982% (11656/12672)\n",
      "Train: Loss: 0.237 | Acc: 91.984% (11774/12800)\n",
      "Train: Loss: 0.237 | Acc: 91.986% (11892/12928)\n",
      "Train: Loss: 0.237 | Acc: 91.958% (12006/13056)\n",
      "Train: Loss: 0.237 | Acc: 91.990% (12128/13184)\n",
      "Train: Loss: 0.237 | Acc: 91.962% (12242/13312)\n",
      "Train: Loss: 0.237 | Acc: 91.957% (12359/13440)\n",
      "Train: Loss: 0.237 | Acc: 91.966% (12478/13568)\n",
      "Train: Loss: 0.238 | Acc: 91.961% (12595/13696)\n",
      "Train: Loss: 0.239 | Acc: 91.934% (12709/13824)\n",
      "Train: Loss: 0.239 | Acc: 91.951% (12829/13952)\n",
      "Train: Loss: 0.239 | Acc: 91.932% (12944/14080)\n",
      "Train: Loss: 0.239 | Acc: 91.913% (13059/14208)\n",
      "Train: Loss: 0.240 | Acc: 91.881% (13172/14336)\n",
      "Train: Loss: 0.241 | Acc: 91.835% (13283/14464)\n",
      "Train: Loss: 0.241 | Acc: 91.831% (13400/14592)\n",
      "Train: Loss: 0.242 | Acc: 91.807% (13514/14720)\n",
      "Train: Loss: 0.242 | Acc: 91.797% (13630/14848)\n",
      "Train: Loss: 0.243 | Acc: 91.794% (13747/14976)\n",
      "Train: Loss: 0.242 | Acc: 91.797% (13865/15104)\n",
      "Train: Loss: 0.243 | Acc: 91.754% (13976/15232)\n",
      "Train: Loss: 0.244 | Acc: 91.706% (14086/15360)\n",
      "Train: Loss: 0.245 | Acc: 91.703% (14203/15488)\n",
      "Train: Loss: 0.245 | Acc: 91.707% (14321/15616)\n",
      "Train: Loss: 0.245 | Acc: 91.673% (14433/15744)\n",
      "Train: Loss: 0.246 | Acc: 91.620% (14542/15872)\n",
      "Train: Loss: 0.246 | Acc: 91.612% (14658/16000)\n",
      "Train: Loss: 0.247 | Acc: 91.580% (14770/16128)\n",
      "Train: Loss: 0.248 | Acc: 91.560% (14884/16256)\n",
      "Train: Loss: 0.248 | Acc: 91.559% (15001/16384)\n",
      "Train: Loss: 0.248 | Acc: 91.552% (15117/16512)\n",
      "Train: Loss: 0.249 | Acc: 91.526% (15230/16640)\n",
      "Train: Loss: 0.248 | Acc: 91.549% (15351/16768)\n",
      "Train: Loss: 0.249 | Acc: 91.525% (15464/16896)\n",
      "Train: Loss: 0.249 | Acc: 91.518% (15580/17024)\n",
      "Train: Loss: 0.250 | Acc: 91.476% (15690/17152)\n",
      "Train: Loss: 0.250 | Acc: 91.470% (15806/17280)\n",
      "Train: Loss: 0.251 | Acc: 91.446% (15919/17408)\n",
      "Train: Loss: 0.251 | Acc: 91.423% (16032/17536)\n",
      "Train: Loss: 0.252 | Acc: 91.418% (16148/17664)\n",
      "Train: Loss: 0.251 | Acc: 91.434% (16268/17792)\n",
      "Train: Loss: 0.251 | Acc: 91.423% (16383/17920)\n",
      "Train: Loss: 0.253 | Acc: 91.373% (16491/18048)\n",
      "Train: Loss: 0.253 | Acc: 91.368% (16607/18176)\n",
      "Train: Loss: 0.253 | Acc: 91.357% (16722/18304)\n",
      "Train: Loss: 0.253 | Acc: 91.352% (16838/18432)\n",
      "Train: Loss: 0.255 | Acc: 91.320% (16949/18560)\n",
      "Train: Loss: 0.255 | Acc: 91.326% (17067/18688)\n",
      "Train: Loss: 0.255 | Acc: 91.268% (17173/18816)\n",
      "Train: Loss: 0.255 | Acc: 91.285% (17293/18944)\n",
      "Train: Loss: 0.255 | Acc: 91.280% (17409/19072)\n",
      "Train: Loss: 0.255 | Acc: 91.266% (17523/19200)\n",
      "Train: Loss: 0.255 | Acc: 91.267% (17640/19328)\n",
      "Train: Loss: 0.255 | Acc: 91.257% (17755/19456)\n",
      "Train: Loss: 0.256 | Acc: 91.248% (17870/19584)\n",
      "Train: Loss: 0.256 | Acc: 91.234% (17984/19712)\n",
      "Train: Loss: 0.256 | Acc: 91.240% (18102/19840)\n",
      "Train: Loss: 0.257 | Acc: 91.226% (18216/19968)\n",
      "Train: Loss: 0.257 | Acc: 91.237% (18335/20096)\n",
      "Train: Loss: 0.257 | Acc: 91.218% (18448/20224)\n",
      "Train: Loss: 0.257 | Acc: 91.229% (18567/20352)\n",
      "Train: Loss: 0.257 | Acc: 91.196% (18677/20480)\n",
      "Train: Loss: 0.258 | Acc: 91.154% (18785/20608)\n",
      "Train: Loss: 0.258 | Acc: 91.151% (18901/20736)\n",
      "Train: Loss: 0.258 | Acc: 91.128% (19013/20864)\n",
      "Train: Loss: 0.259 | Acc: 91.120% (19128/20992)\n",
      "Train: Loss: 0.259 | Acc: 91.122% (19245/21120)\n",
      "Train: Loss: 0.259 | Acc: 91.114% (19360/21248)\n",
      "Train: Loss: 0.259 | Acc: 91.083% (19470/21376)\n",
      "Train: Loss: 0.260 | Acc: 91.062% (19582/21504)\n",
      "Train: Loss: 0.260 | Acc: 91.097% (19706/21632)\n",
      "Train: Loss: 0.260 | Acc: 91.089% (19821/21760)\n",
      "Train: Loss: 0.260 | Acc: 91.068% (19933/21888)\n",
      "Train: Loss: 0.260 | Acc: 91.079% (20052/22016)\n",
      "Train: Loss: 0.260 | Acc: 91.072% (20167/22144)\n",
      "Train: Loss: 0.261 | Acc: 91.056% (20280/22272)\n",
      "Train: Loss: 0.261 | Acc: 91.022% (20389/22400)\n",
      "Train: Loss: 0.261 | Acc: 91.011% (20503/22528)\n",
      "Train: Loss: 0.261 | Acc: 91.000% (20617/22656)\n",
      "Train: Loss: 0.262 | Acc: 90.972% (20727/22784)\n",
      "Train: Loss: 0.262 | Acc: 90.965% (20842/22912)\n",
      "Train: Loss: 0.262 | Acc: 90.959% (20957/23040)\n",
      "Train: Loss: 0.263 | Acc: 90.936% (21068/23168)\n",
      "Train: Loss: 0.263 | Acc: 90.925% (21182/23296)\n",
      "Train: Loss: 0.265 | Acc: 90.902% (21293/23424)\n",
      "Train: Loss: 0.266 | Acc: 90.867% (21401/23552)\n",
      "Train: Loss: 0.266 | Acc: 90.836% (21510/23680)\n",
      "Train: Loss: 0.266 | Acc: 90.822% (21623/23808)\n",
      "Train: Loss: 0.266 | Acc: 90.830% (21741/23936)\n",
      "Train: Loss: 0.266 | Acc: 90.820% (21855/24064)\n",
      "Train: Loss: 0.267 | Acc: 90.782% (21962/24192)\n",
      "Train: Loss: 0.268 | Acc: 90.757% (22072/24320)\n",
      "Train: Loss: 0.268 | Acc: 90.735% (22183/24448)\n",
      "Train: Loss: 0.268 | Acc: 90.731% (22298/24576)\n",
      "Train: Loss: 0.268 | Acc: 90.722% (22412/24704)\n",
      "Train: Loss: 0.268 | Acc: 90.726% (22529/24832)\n",
      "Train: Loss: 0.269 | Acc: 90.729% (22646/24960)\n",
      "Train: Loss: 0.269 | Acc: 90.721% (22760/25088)\n",
      "Train: Loss: 0.269 | Acc: 90.716% (22875/25216)\n",
      "Train: Loss: 0.269 | Acc: 90.700% (22987/25344)\n",
      "Train: Loss: 0.269 | Acc: 90.704% (23104/25472)\n",
      "Train: Loss: 0.270 | Acc: 90.664% (23210/25600)\n",
      "Train: Loss: 0.270 | Acc: 90.683% (23331/25728)\n",
      "Train: Loss: 0.270 | Acc: 90.691% (23449/25856)\n",
      "Train: Loss: 0.269 | Acc: 90.702% (23568/25984)\n",
      "Train: Loss: 0.269 | Acc: 90.713% (23687/26112)\n",
      "Train: Loss: 0.269 | Acc: 90.713% (23803/26240)\n",
      "Train: Loss: 0.270 | Acc: 90.678% (23910/26368)\n",
      "Train: Loss: 0.270 | Acc: 90.678% (24026/26496)\n",
      "Train: Loss: 0.270 | Acc: 90.651% (24135/26624)\n",
      "Train: Loss: 0.271 | Acc: 90.636% (24247/26752)\n",
      "Train: Loss: 0.271 | Acc: 90.636% (24363/26880)\n",
      "Train: Loss: 0.271 | Acc: 90.625% (24476/27008)\n",
      "Train: Loss: 0.271 | Acc: 90.640% (24596/27136)\n",
      "Train: Loss: 0.272 | Acc: 90.621% (24707/27264)\n",
      "Train: Loss: 0.272 | Acc: 90.629% (24825/27392)\n",
      "Train: Loss: 0.272 | Acc: 90.607% (24935/27520)\n",
      "Train: Loss: 0.272 | Acc: 90.585% (25045/27648)\n",
      "Train: Loss: 0.271 | Acc: 90.593% (25163/27776)\n",
      "Train: Loss: 0.272 | Acc: 90.568% (25272/27904)\n",
      "Train: Loss: 0.272 | Acc: 90.561% (25386/28032)\n",
      "Train: Loss: 0.272 | Acc: 90.554% (25500/28160)\n",
      "Train: Loss: 0.273 | Acc: 90.533% (25610/28288)\n",
      "Train: Loss: 0.274 | Acc: 90.495% (25715/28416)\n",
      "Train: Loss: 0.274 | Acc: 90.474% (25825/28544)\n",
      "Train: Loss: 0.274 | Acc: 90.482% (25943/28672)\n",
      "Train: Loss: 0.274 | Acc: 90.469% (26055/28800)\n",
      "Train: Loss: 0.274 | Acc: 90.469% (26171/28928)\n",
      "Train: Loss: 0.274 | Acc: 90.474% (26288/29056)\n",
      "Train: Loss: 0.275 | Acc: 90.467% (26402/29184)\n",
      "Train: Loss: 0.275 | Acc: 90.458% (26515/29312)\n",
      "Train: Loss: 0.276 | Acc: 90.442% (26626/29440)\n",
      "Train: Loss: 0.276 | Acc: 90.429% (26738/29568)\n",
      "Train: Loss: 0.277 | Acc: 90.406% (26847/29696)\n",
      "Train: Loss: 0.277 | Acc: 90.397% (26960/29824)\n",
      "Train: Loss: 0.278 | Acc: 90.388% (27073/29952)\n",
      "Train: Loss: 0.278 | Acc: 90.386% (27188/30080)\n",
      "Train: Loss: 0.278 | Acc: 90.380% (27302/30208)\n",
      "Train: Loss: 0.278 | Acc: 90.384% (27419/30336)\n",
      "Train: Loss: 0.278 | Acc: 90.385% (27535/30464)\n",
      "Train: Loss: 0.278 | Acc: 90.383% (27650/30592)\n",
      "Train: Loss: 0.279 | Acc: 90.368% (27761/30720)\n",
      "Train: Loss: 0.279 | Acc: 90.362% (27875/30848)\n",
      "Train: Loss: 0.280 | Acc: 90.354% (27988/30976)\n",
      "Train: Loss: 0.280 | Acc: 90.336% (28098/31104)\n",
      "Train: Loss: 0.280 | Acc: 90.343% (28216/31232)\n",
      "Train: Loss: 0.280 | Acc: 90.335% (28329/31360)\n",
      "Train: Loss: 0.280 | Acc: 90.336% (28445/31488)\n",
      "Train: Loss: 0.280 | Acc: 90.334% (28560/31616)\n",
      "Train: Loss: 0.281 | Acc: 90.319% (28671/31744)\n",
      "Train: Loss: 0.281 | Acc: 90.299% (28780/31872)\n",
      "Train: Loss: 0.281 | Acc: 90.300% (28896/32000)\n",
      "Train: Loss: 0.281 | Acc: 90.289% (29008/32128)\n",
      "Train: Loss: 0.282 | Acc: 90.275% (29119/32256)\n",
      "Train: Loss: 0.282 | Acc: 90.270% (29233/32384)\n",
      "Train: Loss: 0.283 | Acc: 90.253% (29343/32512)\n",
      "Train: Loss: 0.283 | Acc: 90.248% (29457/32640)\n",
      "Train: Loss: 0.283 | Acc: 90.244% (29571/32768)\n",
      "Train: Loss: 0.283 | Acc: 90.230% (29682/32896)\n",
      "Train: Loss: 0.283 | Acc: 90.225% (29796/33024)\n",
      "Train: Loss: 0.283 | Acc: 90.242% (29917/33152)\n",
      "Train: Loss: 0.283 | Acc: 90.222% (30026/33280)\n",
      "Train: Loss: 0.283 | Acc: 90.215% (30139/33408)\n",
      "Train: Loss: 0.284 | Acc: 90.219% (30256/33536)\n",
      "Train: Loss: 0.284 | Acc: 90.227% (30374/33664)\n",
      "Train: Loss: 0.283 | Acc: 90.243% (30495/33792)\n",
      "Train: Loss: 0.284 | Acc: 90.236% (30608/33920)\n",
      "Train: Loss: 0.284 | Acc: 90.237% (30724/34048)\n",
      "Train: Loss: 0.284 | Acc: 90.230% (30837/34176)\n",
      "Train: Loss: 0.284 | Acc: 90.240% (30956/34304)\n",
      "Train: Loss: 0.284 | Acc: 90.224% (31066/34432)\n",
      "Train: Loss: 0.285 | Acc: 90.197% (31172/34560)\n",
      "Train: Loss: 0.284 | Acc: 90.201% (31289/34688)\n",
      "Train: Loss: 0.284 | Acc: 90.211% (31408/34816)\n",
      "Train: Loss: 0.284 | Acc: 90.216% (31525/34944)\n",
      "Train: Loss: 0.284 | Acc: 90.214% (31640/35072)\n",
      "Train: Loss: 0.284 | Acc: 90.207% (31753/35200)\n",
      "Train: Loss: 0.285 | Acc: 90.212% (31870/35328)\n",
      "Train: Loss: 0.285 | Acc: 90.185% (31976/35456)\n",
      "Train: Loss: 0.285 | Acc: 90.175% (32088/35584)\n",
      "Train: Loss: 0.285 | Acc: 90.169% (32201/35712)\n",
      "Train: Loss: 0.286 | Acc: 90.156% (32312/35840)\n",
      "Train: Loss: 0.286 | Acc: 90.141% (32422/35968)\n",
      "Train: Loss: 0.286 | Acc: 90.121% (32530/36096)\n",
      "Train: Loss: 0.287 | Acc: 90.109% (32641/36224)\n",
      "Train: Loss: 0.287 | Acc: 90.105% (32755/36352)\n",
      "Train: Loss: 0.287 | Acc: 90.085% (32863/36480)\n",
      "Train: Loss: 0.287 | Acc: 90.070% (32973/36608)\n",
      "Train: Loss: 0.288 | Acc: 90.048% (33080/36736)\n",
      "Train: Loss: 0.288 | Acc: 90.042% (33193/36864)\n",
      "Train: Loss: 0.288 | Acc: 90.025% (33302/36992)\n",
      "Train: Loss: 0.288 | Acc: 90.038% (33422/37120)\n",
      "Train: Loss: 0.288 | Acc: 90.024% (33532/37248)\n",
      "Train: Loss: 0.288 | Acc: 90.031% (33650/37376)\n",
      "Train: Loss: 0.288 | Acc: 90.020% (33761/37504)\n",
      "Train: Loss: 0.289 | Acc: 90.001% (33869/37632)\n",
      "Train: Loss: 0.289 | Acc: 89.984% (33978/37760)\n",
      "Train: Loss: 0.290 | Acc: 89.968% (34087/37888)\n",
      "Train: Loss: 0.290 | Acc: 89.962% (34200/38016)\n",
      "Train: Loss: 0.291 | Acc: 89.949% (34310/38144)\n",
      "Train: Loss: 0.291 | Acc: 89.943% (34423/38272)\n",
      "Train: Loss: 0.291 | Acc: 89.945% (34539/38400)\n",
      "Train: Loss: 0.291 | Acc: 89.932% (34649/38528)\n",
      "Train: Loss: 0.291 | Acc: 89.921% (34760/38656)\n",
      "Train: Loss: 0.291 | Acc: 89.919% (34874/38784)\n",
      "Train: Loss: 0.291 | Acc: 89.918% (34989/38912)\n",
      "Train: Loss: 0.291 | Acc: 89.918% (35104/39040)\n",
      "Train: Loss: 0.292 | Acc: 89.923% (35221/39168)\n",
      "Train: Loss: 0.292 | Acc: 89.918% (35334/39296)\n",
      "Train: Loss: 0.292 | Acc: 89.912% (35447/39424)\n",
      "Train: Loss: 0.293 | Acc: 89.899% (35557/39552)\n",
      "Train: Loss: 0.293 | Acc: 89.897% (35671/39680)\n",
      "Train: Loss: 0.293 | Acc: 89.881% (35780/39808)\n",
      "Train: Loss: 0.293 | Acc: 89.874% (35892/39936)\n",
      "Train: Loss: 0.293 | Acc: 89.884% (36011/40064)\n",
      "Train: Loss: 0.293 | Acc: 89.891% (36129/40192)\n",
      "Train: Loss: 0.293 | Acc: 89.891% (36244/40320)\n",
      "Train: Loss: 0.293 | Acc: 89.878% (36354/40448)\n",
      "Train: Loss: 0.293 | Acc: 89.868% (36465/40576)\n",
      "Train: Loss: 0.293 | Acc: 89.863% (36578/40704)\n",
      "Train: Loss: 0.294 | Acc: 89.834% (36681/40832)\n",
      "Train: Loss: 0.294 | Acc: 89.814% (36788/40960)\n",
      "Train: Loss: 0.295 | Acc: 89.805% (36899/41088)\n",
      "Train: Loss: 0.295 | Acc: 89.815% (37018/41216)\n",
      "Train: Loss: 0.295 | Acc: 89.810% (37131/41344)\n",
      "Train: Loss: 0.295 | Acc: 89.808% (37245/41472)\n",
      "Train: Loss: 0.295 | Acc: 89.812% (37362/41600)\n",
      "Train: Loss: 0.295 | Acc: 89.801% (37472/41728)\n",
      "Train: Loss: 0.295 | Acc: 89.803% (37588/41856)\n",
      "Train: Loss: 0.296 | Acc: 89.787% (37696/41984)\n",
      "Train: Loss: 0.296 | Acc: 89.784% (37810/42112)\n",
      "Train: Loss: 0.296 | Acc: 89.763% (37916/42240)\n",
      "Train: Loss: 0.296 | Acc: 89.752% (38026/42368)\n",
      "Train: Loss: 0.296 | Acc: 89.752% (38141/42496)\n",
      "Train: Loss: 0.296 | Acc: 89.757% (38258/42624)\n",
      "Train: Loss: 0.296 | Acc: 89.743% (38367/42752)\n",
      "Train: Loss: 0.296 | Acc: 89.757% (38488/42880)\n",
      "Train: Loss: 0.296 | Acc: 89.765% (38606/43008)\n",
      "Train: Loss: 0.296 | Acc: 89.758% (38718/43136)\n",
      "Train: Loss: 0.296 | Acc: 89.763% (38835/43264)\n",
      "Train: Loss: 0.297 | Acc: 89.740% (38940/43392)\n",
      "Train: Loss: 0.297 | Acc: 89.743% (39056/43520)\n",
      "Train: Loss: 0.297 | Acc: 89.738% (39169/43648)\n",
      "Train: Loss: 0.297 | Acc: 89.739% (39284/43776)\n",
      "Train: Loss: 0.297 | Acc: 89.728% (39394/43904)\n",
      "Train: Loss: 0.297 | Acc: 89.735% (39512/44032)\n",
      "Train: Loss: 0.297 | Acc: 89.733% (39626/44160)\n",
      "Train: Loss: 0.297 | Acc: 89.726% (39738/44288)\n",
      "Train: Loss: 0.297 | Acc: 89.720% (39850/44416)\n",
      "Train: Loss: 0.297 | Acc: 89.714% (39962/44544)\n",
      "Train: Loss: 0.297 | Acc: 89.703% (40072/44672)\n",
      "Train: Loss: 0.298 | Acc: 89.683% (40178/44800)\n",
      "Train: Loss: 0.298 | Acc: 89.681% (40292/44928)\n",
      "Train: Loss: 0.298 | Acc: 89.688% (40410/45056)\n",
      "Train: Loss: 0.298 | Acc: 89.676% (40519/45184)\n",
      "Train: Loss: 0.298 | Acc: 89.678% (40635/45312)\n",
      "Train: Loss: 0.298 | Acc: 89.685% (40753/45440)\n",
      "Train: Loss: 0.298 | Acc: 89.686% (40868/45568)\n",
      "Train: Loss: 0.298 | Acc: 89.677% (40979/45696)\n",
      "Train: Loss: 0.299 | Acc: 89.674% (41092/45824)\n",
      "Train: Loss: 0.299 | Acc: 89.661% (41201/45952)\n",
      "Train: Loss: 0.299 | Acc: 89.659% (41315/46080)\n",
      "Train: Loss: 0.299 | Acc: 89.668% (41434/46208)\n",
      "Train: Loss: 0.298 | Acc: 89.675% (41552/46336)\n",
      "Train: Loss: 0.298 | Acc: 89.682% (41670/46464)\n",
      "Train: Loss: 0.298 | Acc: 89.683% (41785/46592)\n",
      "Train: Loss: 0.298 | Acc: 89.681% (41899/46720)\n",
      "Train: Loss: 0.298 | Acc: 89.688% (42017/46848)\n",
      "Train: Loss: 0.298 | Acc: 89.688% (42132/46976)\n",
      "Train: Loss: 0.299 | Acc: 89.672% (42239/47104)\n",
      "Train: Loss: 0.299 | Acc: 89.657% (42347/47232)\n",
      "Train: Loss: 0.300 | Acc: 89.645% (42456/47360)\n",
      "Train: Loss: 0.300 | Acc: 89.648% (42572/47488)\n",
      "Train: Loss: 0.300 | Acc: 89.636% (42681/47616)\n",
      "Train: Loss: 0.300 | Acc: 89.638% (42797/47744)\n",
      "Train: Loss: 0.300 | Acc: 89.643% (42914/47872)\n",
      "Train: Loss: 0.300 | Acc: 89.612% (43014/48000)\n",
      "Train: Loss: 0.300 | Acc: 89.613% (43129/48128)\n",
      "Train: Loss: 0.301 | Acc: 89.603% (43239/48256)\n",
      "Train: Loss: 0.301 | Acc: 89.602% (43353/48384)\n",
      "Train: Loss: 0.301 | Acc: 89.613% (43473/48512)\n",
      "Train: Loss: 0.301 | Acc: 89.581% (43572/48640)\n",
      "Train: Loss: 0.301 | Acc: 89.575% (43684/48768)\n",
      "Train: Loss: 0.301 | Acc: 89.572% (43797/48896)\n",
      "Train: Loss: 0.301 | Acc: 89.579% (43915/49024)\n",
      "Train: Loss: 0.301 | Acc: 89.583% (44032/49152)\n",
      "Train: Loss: 0.301 | Acc: 89.590% (44150/49280)\n",
      "Train: Loss: 0.301 | Acc: 89.585% (44262/49408)\n",
      "Train: Loss: 0.301 | Acc: 89.583% (44376/49536)\n",
      "Train: Loss: 0.301 | Acc: 89.568% (44483/49664)\n",
      "Train: Loss: 0.301 | Acc: 89.561% (44594/49792)\n",
      "Train: Loss: 0.301 | Acc: 89.557% (44707/49920)\n",
      "Train: Loss: 0.302 | Acc: 89.544% (44772/50000)\n",
      "Test: Loss: 0.742 | Acc: 81.000% (81/100)\n",
      "Test: Loss: 0.698 | Acc: 79.000% (158/200)\n",
      "Test: Loss: 0.668 | Acc: 79.667% (239/300)\n",
      "Test: Loss: 0.621 | Acc: 80.250% (321/400)\n",
      "Test: Loss: 0.656 | Acc: 79.200% (396/500)\n",
      "Test: Loss: 0.617 | Acc: 80.000% (480/600)\n",
      "Test: Loss: 0.629 | Acc: 79.571% (557/700)\n",
      "Test: Loss: 0.630 | Acc: 79.375% (635/800)\n",
      "Test: Loss: 0.651 | Acc: 78.889% (710/900)\n",
      "Test: Loss: 0.666 | Acc: 78.900% (789/1000)\n",
      "Test: Loss: 0.657 | Acc: 79.182% (871/1100)\n",
      "Test: Loss: 0.673 | Acc: 79.000% (948/1200)\n",
      "Test: Loss: 0.661 | Acc: 79.077% (1028/1300)\n",
      "Test: Loss: 0.664 | Acc: 78.714% (1102/1400)\n",
      "Test: Loss: 0.661 | Acc: 78.600% (1179/1500)\n",
      "Test: Loss: 0.671 | Acc: 78.562% (1257/1600)\n",
      "Test: Loss: 0.669 | Acc: 78.765% (1339/1700)\n",
      "Test: Loss: 0.680 | Acc: 78.389% (1411/1800)\n",
      "Test: Loss: 0.685 | Acc: 78.263% (1487/1900)\n",
      "Test: Loss: 0.698 | Acc: 78.050% (1561/2000)\n",
      "Test: Loss: 0.701 | Acc: 78.000% (1638/2100)\n",
      "Test: Loss: 0.708 | Acc: 77.955% (1715/2200)\n",
      "Test: Loss: 0.710 | Acc: 77.826% (1790/2300)\n",
      "Test: Loss: 0.709 | Acc: 77.792% (1867/2400)\n",
      "Test: Loss: 0.718 | Acc: 77.400% (1935/2500)\n",
      "Test: Loss: 0.726 | Acc: 77.231% (2008/2600)\n",
      "Test: Loss: 0.725 | Acc: 77.259% (2086/2700)\n",
      "Test: Loss: 0.726 | Acc: 77.107% (2159/2800)\n",
      "Test: Loss: 0.731 | Acc: 76.931% (2231/2900)\n",
      "Test: Loss: 0.727 | Acc: 77.000% (2310/3000)\n",
      "Test: Loss: 0.725 | Acc: 76.935% (2385/3100)\n",
      "Test: Loss: 0.723 | Acc: 77.094% (2467/3200)\n",
      "Test: Loss: 0.722 | Acc: 77.303% (2551/3300)\n",
      "Test: Loss: 0.723 | Acc: 77.294% (2628/3400)\n",
      "Test: Loss: 0.724 | Acc: 77.286% (2705/3500)\n",
      "Test: Loss: 0.724 | Acc: 77.333% (2784/3600)\n",
      "Test: Loss: 0.724 | Acc: 77.486% (2867/3700)\n",
      "Test: Loss: 0.722 | Acc: 77.553% (2947/3800)\n",
      "Test: Loss: 0.718 | Acc: 77.564% (3025/3900)\n",
      "Test: Loss: 0.715 | Acc: 77.675% (3107/4000)\n",
      "Test: Loss: 0.716 | Acc: 77.707% (3186/4100)\n",
      "Test: Loss: 0.716 | Acc: 77.857% (3270/4200)\n",
      "Test: Loss: 0.711 | Acc: 78.023% (3355/4300)\n",
      "Test: Loss: 0.713 | Acc: 77.932% (3429/4400)\n",
      "Test: Loss: 0.709 | Acc: 78.000% (3510/4500)\n",
      "Test: Loss: 0.712 | Acc: 77.978% (3587/4600)\n",
      "Test: Loss: 0.714 | Acc: 77.979% (3665/4700)\n",
      "Test: Loss: 0.714 | Acc: 78.042% (3746/4800)\n",
      "Test: Loss: 0.711 | Acc: 78.041% (3824/4900)\n",
      "Test: Loss: 0.719 | Acc: 77.860% (3893/5000)\n",
      "Test: Loss: 0.717 | Acc: 77.902% (3973/5100)\n",
      "Test: Loss: 0.717 | Acc: 77.885% (4050/5200)\n",
      "Test: Loss: 0.717 | Acc: 77.774% (4122/5300)\n",
      "Test: Loss: 0.714 | Acc: 77.796% (4201/5400)\n",
      "Test: Loss: 0.715 | Acc: 77.745% (4276/5500)\n",
      "Test: Loss: 0.716 | Acc: 77.571% (4344/5600)\n",
      "Test: Loss: 0.721 | Acc: 77.368% (4410/5700)\n",
      "Test: Loss: 0.720 | Acc: 77.397% (4489/5800)\n",
      "Test: Loss: 0.722 | Acc: 77.271% (4559/5900)\n",
      "Test: Loss: 0.725 | Acc: 77.233% (4634/6000)\n",
      "Test: Loss: 0.727 | Acc: 77.180% (4708/6100)\n",
      "Test: Loss: 0.729 | Acc: 77.177% (4785/6200)\n",
      "Test: Loss: 0.727 | Acc: 77.238% (4866/6300)\n",
      "Test: Loss: 0.724 | Acc: 77.328% (4949/6400)\n",
      "Test: Loss: 0.723 | Acc: 77.308% (5025/6500)\n",
      "Test: Loss: 0.726 | Acc: 77.197% (5095/6600)\n",
      "Test: Loss: 0.725 | Acc: 77.179% (5171/6700)\n",
      "Test: Loss: 0.727 | Acc: 77.132% (5245/6800)\n",
      "Test: Loss: 0.727 | Acc: 77.145% (5323/6900)\n",
      "Test: Loss: 0.729 | Acc: 77.071% (5395/7000)\n",
      "Test: Loss: 0.733 | Acc: 77.042% (5470/7100)\n",
      "Test: Loss: 0.733 | Acc: 77.028% (5546/7200)\n",
      "Test: Loss: 0.733 | Acc: 77.041% (5624/7300)\n",
      "Test: Loss: 0.734 | Acc: 77.054% (5702/7400)\n",
      "Test: Loss: 0.735 | Acc: 77.000% (5775/7500)\n",
      "Test: Loss: 0.735 | Acc: 76.921% (5846/7600)\n",
      "Test: Loss: 0.735 | Acc: 76.935% (5924/7700)\n",
      "Test: Loss: 0.734 | Acc: 76.936% (6001/7800)\n",
      "Test: Loss: 0.734 | Acc: 76.937% (6078/7900)\n",
      "Test: Loss: 0.735 | Acc: 76.950% (6156/8000)\n",
      "Test: Loss: 0.733 | Acc: 77.037% (6240/8100)\n",
      "Test: Loss: 0.733 | Acc: 77.037% (6317/8200)\n",
      "Test: Loss: 0.734 | Acc: 77.036% (6394/8300)\n",
      "Test: Loss: 0.737 | Acc: 77.000% (6468/8400)\n",
      "Test: Loss: 0.740 | Acc: 76.953% (6541/8500)\n",
      "Test: Loss: 0.740 | Acc: 76.965% (6619/8600)\n",
      "Test: Loss: 0.741 | Acc: 76.943% (6694/8700)\n",
      "Test: Loss: 0.740 | Acc: 77.000% (6776/8800)\n",
      "Test: Loss: 0.741 | Acc: 77.022% (6855/8900)\n",
      "Test: Loss: 0.741 | Acc: 77.011% (6931/9000)\n",
      "Test: Loss: 0.738 | Acc: 77.121% (7018/9100)\n",
      "Test: Loss: 0.736 | Acc: 77.207% (7103/9200)\n",
      "Test: Loss: 0.738 | Acc: 77.172% (7177/9300)\n",
      "Test: Loss: 0.739 | Acc: 77.138% (7251/9400)\n",
      "Test: Loss: 0.738 | Acc: 77.126% (7327/9500)\n",
      "Test: Loss: 0.736 | Acc: 77.198% (7411/9600)\n",
      "Test: Loss: 0.737 | Acc: 77.134% (7482/9700)\n",
      "Test: Loss: 0.739 | Acc: 77.071% (7553/9800)\n",
      "Test: Loss: 0.739 | Acc: 77.030% (7626/9900)\n",
      "Test: Loss: 0.739 | Acc: 77.030% (7703/10000)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "net = ResNet18().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "def train(epoch):\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        print(f'Train: Loss: {train_loss/(batch_idx+1):.3f} | Acc: {100.*correct/total:.3f}% ({correct}/{total})')\n",
    "\n",
    "def test(epoch):\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            print(f'Test: Loss: {test_loss/(batch_idx+1):.3f} | Acc: {100.*correct/total:.3f}% ({correct}/{total})')\n",
    "\n",
    "    # Save checkpoint.\n",
    "    torch.save(net.state_dict(), './checkpoint.pth')\n",
    "\n",
    "for epoch in range(0, 10):\n",
    "    train(epoch)\n",
    "    test(epoch)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
